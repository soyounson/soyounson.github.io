---
layout: post
title: Feature Engineering & Preprocessing for NLP
---



Before we consider diverse models to predict, pretty clean and well-organized ingredients, we are going to call **feataures**, should be well-prepared. 
Here, preprocessing and **Feature engineering** will be covered but these process are limited in NLP field. (Ofc, these concepts could be applied to fields.)

First things first, we should **Natural Language Process** is. ~~

but it is quite tricky and time consuming process to be read by the computer. 
~~
****** ì‹¤ì œ ì‹¤ë¬´ì—ì„œ í™•ì¸ í•  ê²ƒ ***
ëª¨ë¸ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤ ë°ì´í„°ì—ì„œ ìŠ¹ë¶€ê°€ ë‚˜ëŠ” ê²ƒ 
ì°¨ë¼ë¦¬ í† í¬ë„ˆë‚˜ì´ì €ê°™ì€ ê²ƒì„ ë°”ê¾¸ëŠ” ê²ƒì´ ì¤‘ìš”í•¨ 
ì—­ì‚¼ê°í˜•ì´ì–´ì„œ ëª¨ë¸ì„ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤, ì´ˆê¸°ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ì¢‹ìŒ 
ë²„íŠ¸ë„ ë²„íŠ¸ì˜ ëª¨ë¸ë³´ë‹¤ oovì¤„ì—¬ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒ
í† í¬ë‚˜ì´ì €ì—ì„œ ì›Œë“œ í”¼ìŠ¤ë¥¼ ì¤„ì´ëŠ” ê²ƒ. 
íŒŒì‹±í• ë•Œ (í•œêµ­ì–´ì—ì„œ ì–´ê°„, ì–´ë¯¸ ìª¼ê°¤ë•Œ)ì™€ ê°™ì€ í† í¬ë‚˜ì´ì €ë¥¼ ì¼ì„ë•Œ ì„±ëŠ¥ ì˜¬ë¼ê°
ì—¬ì „íˆ ë„ë©”ì¸ì´ ì¤‘ìš”. ë„ë©”ì¸ì—ì„œ ë§ì´ ì“°ëŠ” ì–¸ì–´ê°€ ë‹¨ì–´ë¡œ í• ë‹¹ë¨?
~~

-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ Preprocessing              
  - tokenization         
  - remove stop words         
  - stemming           
  - POS tagging         
  - Lemmatization             
  - language detection  
          
â˜ºï¸ Feature extraction            
  - weighted words - BOW          
  - countvectorizer          
  - TF-IDF          

â˜ºï¸ Embedding          
  - word2vec -> gensim          
  - Glove          
  - FastText          

â˜ºï¸ Feature Selection                      
â˜ºï¸ What the next step is?          
â˜» Reference
          
-----------------------------------------------------------------------

The pipline of NLP is below

![Fig02](/images/NLP_FE_Fig01.png)

to undestand datasets and also problems, we should spend 60% (even more) time of the process in preprocessing/feature engineering. 


### â˜ºï¸ Preprocessing 
  

â˜» Data cleaning [1,2] 

basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.*

removing urls
removing HTML tags
removing Emojis
removing punctuations
spelling correction


â˜» text preprocessing [2]
+ text pre-processing 
  - tokenization 
  - remove stop words : nltkì— tokenizer ì‚¬ì „ì´ ì¡´ì¬. í•œêµ­ì–´ì˜ ê²½ìš°ëŠ” POS tagging ì´í›„ì— ì§„í–‰
  - stemming : ì–´ê°„ì¶”ì¶œ, ë¶„ì„ê²°ê³¼ê°€ ì•ˆì¢‹ì•˜ì„ë•Œ í•˜ëŠ” ê²½ìš°ê°€ ì¼ë°˜ì  
  - POS tagging : í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒ. ë™ì‚¬/ëª…ì‚¬/... ë¬¸ë§¥ì ìœ¼ë¡œ ì˜ íŒŒì•…í• ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê³¼ì •. í’ˆì‚¬ì— ë”°ë¼ì„œ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ê²½ìš°ê°€ ë°œìƒ. í•œêµ­ì–´ì˜ ê²½ìš° KoNLPy
  - Lemmatization : í‘œì œì–´ ì¶”ì¶œ. ê¸°ë³¸í˜•ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê³¼ì •. ë”°ë¼ì„œ í’ˆì‚¬ ì§€ì •í•˜ëŠ” PoS taggingí•œ í›„ì— ì§„í–‰í•´ì•¼ í•¨. 
  - (optional) language detection 



### â˜ºï¸ Text Feature Extraction 
  + weighted words - BOW
    - countvectorizer
    - TF-IDF
    
> Time to time, TF might deliver better results.     
### â˜ºï¸ word embedding 
  - word2vec -> gensim 
  - Glove
  - FastText
  - Bert : transformer 30ì–µê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ í•™ìŠµì‹œí‚¨í›„, ë³¸ì¸ì˜ í…ŒìŠ¤í¬ì— ë”°ë¼ì„œ fine tunningì„ í•˜ë©´ ì •í™•ë„ê°€ ë†’ìŒ. 

GloVe for Vecttorization [3]
use GloVe pretrained corpus model to represent our words.It is available in 3 varieties

ê¸€ë¡œë¸Œ(Global Vectors for Word Representation, GloVe)ëŠ” ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ 2014ë…„ì— ë¯¸êµ­ ìŠ¤íƒ í¬ë“œëŒ€í•™ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ì•ì„œ í•™ìŠµí•˜ì˜€ë˜ ê¸°ì¡´ì˜ ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ LSA(Latent Semantic Analysis)ì™€ ì˜ˆì¸¡ ê¸°ë°˜ì˜ Word2Vecì˜ ë‹¨ì ì„ ì§€ì í•˜ë©° ì´ë¥¼ ë³´ì™„í•œë‹¤ëŠ” ëª©ì ìœ¼ë¡œ ë‚˜ì™”ê³ , ì‹¤ì œë¡œë„ Word2Vecë§Œí¼ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ë‹¨ì •ì ìœ¼ë¡œ Word2Vecì™€ GloVe ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” ë›°ì–´ë‚˜ë‹¤ê³  ë§í•  ìˆ˜ëŠ” ì—†ê³ , ì´ ë‘ ê°€ì§€ ì „ë¶€ë¥¼ ì‚¬ìš©í•´ë³´ê³  ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•©ë‹ˆë‹¤.


### â˜ºï¸ comparison of feature extraction techinique  

ì˜ˆë¡œ Bertì˜ í•œê³„ê°€ ì¡´ì¬í•œë‹¤ë©´, 
ì´ëŸ° ë¬¸ì œì˜€ì„ë•ŒëŠ” ë‹¤ë¥¸ ê²ƒ ì‚¬ìš© í•  ê²ƒ
ë‹¤ë¥¸ ì ‘ê·¼ë²•: ê° ë°©ë²•ì˜ ì•½ì ì„ í™•ì¸í•˜ë©´ ì¢‹ì„ ë“¯
Bert (Transformer)ë§Œ ì‚¬ìš©í•˜ë‹¤ë³´ë‹ˆ, ê¸°ì¡´ ëª¨ë¸ë“¤ RNN
A : architecture 



### â˜ºï¸ Feature selection 


### â˜ºï¸ Next steps?


**sample**

![\Large \sqrt{\frac{n}{6}}](https://latex.codecogs.com/svg.image?%5Csqrt%7B%5Cfrac%7Bn%7D%7B6%7D%7D)
<!-- https://www.codecogs.com/latex/eqneditor.php -->

![Fig02](/images/DR_Fig02.png)
Ref [x]


### â˜» Reference
1. [Kaggle]  https://www.kaggle.com/competitions/nlp-getting-started/data
2. [Kaggle, NL_ preprocess] https://www.kaggle.com/code/longtng/nlp-preprocessing-feature-extraction-methods-a-z/notebook
3. [wiki doc] https://wikidocs.net/22885
4. [book, ê°œì¸ ì†Œì¥] Hands-on Machine laerning with Scikit-Learn Keras & TensorFlow : í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹ 2íŒ
5. [book, ê°œì¸ ì†Œì¥] Machine Learning: An Algorithmic Perspective by Stephen Marsland
6. 


ğŸŒº **Thanks for reading. Hope to see you again :o)**
