---
layout: post
title: Feature Engineering & Preprocessing for NLP
---



Before we consider diverse models to predict, pretty clean and well-organized ingredients, we are going to call **feataures**, should be well-prepared. 
Here, preprocessing and **Feature engineering** will be covered but these process are limited in NLP field. (Ofc, these concepts could be applied to fields.)

First things first, we should **Natural Language Process** is. ~~

but it is quite tricky and time consuming process to be read by the computer. 
~~
****** ì‹¤ì œ ì‹¤ë¬´ì—ì„œ í™•ì¸ í•  ê²ƒ ***
ëª¨ë¸ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤ ë°ì´í„°ì—ì„œ ìŠ¹ë¶€ê°€ ë‚˜ëŠ” ê²ƒ 
ì°¨ë¼ë¦¬ í† í¬ë„ˆë‚˜ì´ì €ê°™ì€ ê²ƒì„ ë°”ê¾¸ëŠ” ê²ƒì´ ì¤‘ìš”í•¨ 
ì—­ì‚¼ê°í˜•ì´ì–´ì„œ ëª¨ë¸ì„ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤, ì´ˆê¸°ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ì¢‹ìŒ 
ë²„íŠ¸ë„ ë²„íŠ¸ì˜ ëª¨ë¸ë³´ë‹¤ oovì¤„ì—¬ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒ
í† í¬ë‚˜ì´ì €ì—ì„œ ì›Œë“œ í”¼ìŠ¤ë¥¼ ì¤„ì´ëŠ” ê²ƒ. 
íŒŒì‹±í• ë•Œ (í•œêµ­ì–´ì—ì„œ ì–´ê°„, ì–´ë¯¸ ìª¼ê°¤ë•Œ)ì™€ ê°™ì€ í† í¬ë‚˜ì´ì €ë¥¼ ì¼ì„ë•Œ ì„±ëŠ¥ ì˜¬ë¼ê°
ì—¬ì „íˆ ë„ë©”ì¸ì´ ì¤‘ìš”. ë„ë©”ì¸ì—ì„œ ë§ì´ ì“°ëŠ” ì–¸ì–´ê°€ ë‹¨ì–´ë¡œ í• ë‹¹ë¨?
~~

-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ Preprocessing              
  - tokenization         
  - remove stop words         
  - stemming           
  - POS tagging         
  - Lemmatization             
  - language detection  
          
â˜ºï¸ Feature extraction            
  - weighted words - BOW          
  - countvectorizer          
  - TF-IDF          

â˜ºï¸ Embedding          
  - word2vec -> gensim          
  - Glove          
  - FastText          

â˜ºï¸ Feature Selection                      
â˜ºï¸ What the next step is?          
â˜» Reference
          
-----------------------------------------------------------------------

The pipline of NLP is below

![Fig01](/images/NLP_FE_Fig01.png)
Ref [1]
to undestand datasets and also problems, we should spend 60% (even more) time of the process in preprocessing/feature engineering. Simply 


### â˜ºï¸ Preprocessing 
 
#### â˜» Data cleaning [1,2] 

basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.*

removing urls
removing HTML tags
removing Emojis
removing punctuations
spelling correction

+ Capitalization/ Lower case [2]
The most common approach in text cleaning is capitalization or lower case due to the diversity of capitalization to form a sentence. This technique will project all words in text and document into the same feature space. However, it would also cause problems with exceptional cases such as the USA or UK, which could be solved by replacing typos, slang, acronyms or informal abbreviations technique.
The simple example is below : 


Forest fire near La Ronge Sask. Canada -> forest fire near la ronge sask. canada


+ Expand the Contractions
We use the contractions package to expand the contraction in English such as we'll -> we will or we shouldn't've -> we should not have.
```
Y'all can -> you all can 
```

+ Noise Removal
Text data could include various unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters (symbols, emojis, and other graphic characters).
  - Remove URLs
  - Remove HTML tags
  - Remove Non-ASCI
  - Remove special characters
(Remove special characters : The special characters could be symbols, emojis, and other graphic characters. We use the "Toxic Comment Classification Challenge" dataset as the "Real or Not? NLP with Disaster Tweets" dataset do not have any special charaters in their text.) 


+ Remove punctuations

+ Other Manual Text Cleaning Tasks
- Replace the Unicode character with equivalent ASCII character (instead of removing)
- Replace the entity references with their actual symbols  instead of removing as HTML tags
- Replace the Typos, slang, acronyms or informal abbreviations - depend on different situations or main topics of the NLP such as finance or medical topics.
```
sample_typos_slang = {
                                "w/e": "whatever",
                                "usagov": "usa government"}
  # Acronyms
  sample_acronyms =  { 
                            "mh370": "malaysia airlines flight 370",
                            "okwx": "oklahoma city weather",
                            "arwx": "arkansas weather"}
 # Some common abbreviations 
 sample_abbr = {
                        "$" : " dollar ",
                        "â‚¬" : " euro ",
                        "4ao" : "for adults only"}
 ```      
 
- List out all the hashtags/ usernames then replace with equivalent words
- Replace the emoticon/ emoji with equivalant word meaning such as ":)" with "smile" 
- Spelling correction : Spelling correction could also be considered an optional preprocessing task as the social media text data is often are typos or mistyped. However, the spelling correction output should be carefully double-checked with the original text input as it could be a mistake.





### â˜ºï¸ text preprocessing [2]

â˜» tokenization 
Tokenization is a common technique that **split a sentence into tokens**, where a token could be characters, words, phrases, symbols, or other meaningful elements. By breaking sentences into smaller chunks, that would help to investigate the words in a sentence and also the subsequent steps in the NLP pipeline, such as stemming.

<<<
Forest fire near La Ronge Sask. Canada	1	forest fire near la ronge sask canada	[forest, fire, near, la, ronge, sask, canada]


- remove stop words : nltkì— tokenizer ì‚¬ì „ì´ ì¡´ì¬. í•œêµ­ì–´ì˜ ê²½ìš°ëŠ” POS tagging ì´í›„ì— ì§„í–‰
  - stemming : ì–´ê°„ì¶”ì¶œ, ë¶„ì„ê²°ê³¼ê°€ ì•ˆì¢‹ì•˜ì„ë•Œ í•˜ëŠ” ê²½ìš°ê°€ ì¼ë°˜ì  
  - POS tagging : í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒ. ë™ì‚¬/ëª…ì‚¬/... ë¬¸ë§¥ì ìœ¼ë¡œ ì˜ íŒŒì•…í• ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê³¼ì •. í’ˆì‚¬ì— ë”°ë¼ì„œ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ê²½ìš°ê°€ ë°œìƒ. í•œêµ­ì–´ì˜ ê²½ìš° KoNLPy
  - Lemmatization : í‘œì œì–´ ì¶”ì¶œ. ê¸°ë³¸í˜•ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê³¼ì •. ë”°ë¼ì„œ í’ˆì‚¬ ì§€ì •í•˜ëŠ” PoS taggingí•œ í›„ì— ì§„í–‰í•´ì•¼ í•¨. 
  - (optional) language detection 



### â˜ºï¸ Text Feature Extraction 

transfer Natural language into numerical values the computer understand and read 
  + weighted words - BOW
    - countvectorizer
    - TF-IDF
  + one-hot encoding 
  + target encoding : Label of NLP is categorical values, label (text)
> Time to time, TF might deliver better results.     
### â˜ºï¸ word embedding 
  - word2vec -> gensim 
  - Glove
  - FastText
  - Bert : transformer 30ì–µê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ í•™ìŠµì‹œí‚¨í›„, ë³¸ì¸ì˜ í…ŒìŠ¤í¬ì— ë”°ë¼ì„œ fine tunningì„ í•˜ë©´ ì •í™•ë„ê°€ ë†’ìŒ. 

GloVe for Vecttorization [3]
use GloVe pretrained corpus model to represent our words.It is available in 3 varieties

ê¸€ë¡œë¸Œ(Global Vectors for Word Representation, GloVe)ëŠ” ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ 2014ë…„ì— ë¯¸êµ­ ìŠ¤íƒ í¬ë“œëŒ€í•™ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ì•ì„œ í•™ìŠµí•˜ì˜€ë˜ ê¸°ì¡´ì˜ ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ LSA(Latent Semantic Analysis)ì™€ ì˜ˆì¸¡ ê¸°ë°˜ì˜ Word2Vecì˜ ë‹¨ì ì„ ì§€ì í•˜ë©° ì´ë¥¼ ë³´ì™„í•œë‹¤ëŠ” ëª©ì ìœ¼ë¡œ ë‚˜ì™”ê³ , ì‹¤ì œë¡œë„ Word2Vecë§Œí¼ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ë‹¨ì •ì ìœ¼ë¡œ Word2Vecì™€ GloVe ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” ë›°ì–´ë‚˜ë‹¤ê³  ë§í•  ìˆ˜ëŠ” ì—†ê³ , ì´ ë‘ ê°€ì§€ ì „ë¶€ë¥¼ ì‚¬ìš©í•´ë³´ê³  ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•©ë‹ˆë‹¤.


### â˜ºï¸ comparison of feature extraction techinique  

ì˜ˆë¡œ Bertì˜ í•œê³„ê°€ ì¡´ì¬í•œë‹¤ë©´, 
ì´ëŸ° ë¬¸ì œì˜€ì„ë•ŒëŠ” ë‹¤ë¥¸ ê²ƒ ì‚¬ìš© í•  ê²ƒ
ë‹¤ë¥¸ ì ‘ê·¼ë²•: ê° ë°©ë²•ì˜ ì•½ì ì„ í™•ì¸í•˜ë©´ ì¢‹ì„ ë“¯
Bert (Transformer)ë§Œ ì‚¬ìš©í•˜ë‹¤ë³´ë‹ˆ, ê¸°ì¡´ ëª¨ë¸ë“¤ RNN
A : architecture 



### â˜ºï¸ Feature selection 


### â˜ºï¸ Next steps?


**sample**

![\Large \sqrt{\frac{n}{6}}](https://latex.codecogs.com/svg.image?%5Csqrt%7B%5Cfrac%7Bn%7D%7B6%7D%7D)
<!-- https://www.codecogs.com/latex/eqneditor.php -->




### â˜» Reference
1. [Kaggle]  https://www.kaggle.com/competitions/nlp-getting-started/data
2. [Kaggle, NL_ preprocess] https://www.kaggle.com/code/longtng/nlp-preprocessing-feature-extraction-methods-a-z/notebook
3. [wiki doc] https://wikidocs.net/22885
4. [book, ê°œì¸ ì†Œì¥] Hands-on Machine laerning with Scikit-Learn Keras & TensorFlow : í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹ 2íŒ
5. [book, ê°œì¸ ì†Œì¥] Machine Learning: An Algorithmic Perspective by Stephen Marsland
6. 


ğŸŒº **Thanks for reading. Hope to see you again :o)**
