---
layout: post
title: Dimensionality Reduction
---


ì°¨ì› ì¶•ì†ŒëŠ” ë¬´ì—‡ì¼ê¹Œ? ì‰½ê²Œ ìƒê°í•˜ë©´ ê·¸ëƒ¥ ë§ ê·¸ëŒ€ë¡œ ì°¨ì›ì„ ì¶•ì†Œí•œë‹¤ëŠ” ë§ì´ë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ë‚´ê°€ 10ì°¨ì›ì˜ features (or input data)ê°€ ìˆì„ë•Œ, ì´ ì•„ì´ë“¤ì„ ëª¨ë‘ ë°ë¦¬ê³  ê°€ê¸°ì— ë„ˆë¬´ ë¬´ê²ê³ , ë§ë‹¤ë©´ ì´ ì¤‘ ì •ë§ ì¤‘ìš”í•œ íŠ¹ì„±ì„ ê°–ê³  ìˆëŠ” ì•„ì´ë“¤ë§Œì„ ë½‘ì•„ë‚´ì„œ ë°ë¦¬ê³  ê°€ê² ë‹¤ëŠ” ê²ƒì´ë‹¤.
ë¬¼ë¡ , ê·¸ê²ƒì´ 2ì°¨ì›ì´ ë  ìˆ˜ë„ í˜¹ì€ 5ì°¨ì›ì´ ë  ìˆ˜ë„ ìˆë‹¤. 


ë‚´ê°€ ì²˜ìŒì— ì°¨ì› ì¶•ì†Œë¥¼ ì ‘í•œ ì´ìœ ëŠ” geophysicsë¶„ì•¼ì—ì„œ seismic datasetì„ í•´ì„í•  ë•Œ features extractionì˜ ê´€ì ì—ì„œ PCAë¥¼ ì ìš©í•˜ì˜€ë‹¤.
ê·¸ ë‹¹ì‹œì—ëŠ” PCA, ICA, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ manifold learningì„ ê³ ë ¤í•˜ì˜€ì§€ë§Œ, ì—¬ê¸°ì„œ ë‚´ê°€ ê°„ê³¼í–ˆë˜ ì ì€ ì°¨ì› ì¶•ì†Œ ë°©ë²•ì„ ì„ íƒí•˜ê¸°ì— ì•ì„œ ë°ì´í„°ì— ëŒ€í•œ ì •í™•í•œ ì´í•´ ë° ê·¸ì— ë”°ë¥¸ ì ì ˆí•œ ë°©ë²•ë¡  ì„ íƒì´ì—ˆì„ ê±°ë¼ ìƒê°í•œë‹¤.

ì•„ëŠ” ë§Œí¼ ë³´ì¸ë‹¤ëŠ” ë§ì²˜ëŸ¼ ê·¸ë‹¹ì‹œì—ëŠ” ì •í™•í•˜ê²Œ íë¦„ì„ ì´í•´í•˜ê¸°ë³´ë‹¤ëŠ” ì‘ì€ êµ¬ë¦„ë“¤ì´ ë™ë™ ë– ë‹¤ë‹ˆë“¯ â˜ï¸ â˜ï¸ â˜ï¸ ì´í•´í•˜ê³  ì§„í–‰í–ˆë˜ ê²ƒ ê°™ë‹¤. 
ì–´ì¨Œë“  ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸´ í•˜ì˜€ì§€ë§Œ, ê³¼ì—° ê·¸ê²ƒì´ ìµœì ì˜ ë°©ë²•ì´ì—ˆëŠ”ì§€ì— ëŒ€í•œ ì˜êµ¬ì‹¬ì´ í•­ìƒ ë‚´ ë§ˆìŒ í•œì¼ ì— ìë¦¬ì¡ê³  ìˆì—ˆê³ , í•­ìƒ ë§ˆìŒì— ê±¸ë ¸ì—ˆëŠ”ë°, ì´ë²ˆì— ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ë©´ì„œ ì •ë¦¬í•˜ê³  ê·¸ ì§ì„ ëœì–´ ë†“ê³ ì í•œë‹¤.
ê·¸ ì‹ì„ ë‹¤ ì´í•´í•˜ê¸° ì „ì—ëŠ” í˜ì´ì§€ë¥¼ ë„˜ê¸°ì§€ ë§ë¼ëŠ” Robertì˜ ì¡°ì–¸ì²˜ëŸ¼, ì´ë²ˆì—ëŠ” ì¡°ê¸ˆ ëŠë ¤ë„ ì •í™•í•˜ê²Œ ì´í•´í•˜ê³ ì í•œë‹¤.

-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ A definition of Dimensionality Reduction       
â˜ºï¸ Eigenvalue and Eigenvector : ê³ ìœ ê°’, ê³ ìœ ë²¡í„°         
â˜ºï¸ Linear Discriminant Analysis (LDA) : ì„ í˜• íŒë³„ ë¶„ì„        
â˜ºï¸ Principal Components Analysis (PCA) : ì£¼ì„±ë¶„ ë¶„ì„      
â˜ºï¸ Independent Component Analysis (ICA) : ë…ë¦½ ì„±ë¶„ ë¶„ì„          
â˜ºï¸ Factor Analysis : ìš”ì¸ ë¶„ì„         
â˜ºï¸ Manifold learning       
â˜ºï¸ Isomap        
â˜ºï¸ Locally Linear Embedding (LLE)            
â˜ºï¸ t-Distributed Stochastic Neighbor Embedding (t-SNE)          
â˜ºï¸ Multi-dimensional Scaling (MDS)             
â˜» Reference

-----------------------------------------------------------------------


### â˜ºï¸ A definition of Dimensionality Reduction
ì°¨ì›ì¶•ì†Œë¼ëŠ” ê²ƒì€ ë¬´ì—‡ì¼ê¹Œ?

Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics [1].
 
ì¦‰, ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ì €ì°¨ì›ì˜ ë°ì´í„°ë¡œ ë°”ê¾¸ëŠ” ê²ƒì¸ë°, ì´ ì €ì°¨ì›ì´ ê³ ì°¨ì›ì¼ë•Œ ì˜ë¯¸ìˆë˜ ì •ë³´ë¥¼ ëŒ€ë¶€ë¶„ coverí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.          
ê¶ê·¹ì  ëª©ì ì€ ì •ë³´ì˜ ì†ì‹¤ì´ ê±°ì˜ ì¼ì–´ë‚˜ì§€ ì•Šìœ¼ë©´ì„œ ì°¨ì›ì„ ì¤„ì—¬ì„œ ìš°ë¦¬ê°€ ë°ì´í„° ë¶„ì„, ëª¨ë¸ ê°œë°œ, í˜¹ì€ ì—¬ëŸ¬ê°€ì§€ ì‘ìš©ë¶„ì•¼ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ê²ƒì´ë‹¤.       
ì°¨ì› ì¶•ì†Œì˜ ì¥ì ìœ¼ë¡œëŠ” í›ˆë ¨ ì†ë„ ì¦ê°€, í›ˆë ¨ ì‹œê°„ ì¶•ì†Œ, ì°¨ì›ì˜ ì €ì£¼ (Curse of Dimensionality) ë° ê³¼ì í•© (Over fitting) ë°©ì§€ë¥¼ í†µí•œ ì¼ë°˜í™” ë° ëª¨ë¸ì˜ ì •í™•ë„, ì„±ëŠ¥ í–¥ìƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.               
ìš°ì„  ëª‡ê°€ì§€ ì—°ê´€ì„±ì— ëŒ€í•˜ì—¬ ì •ë¦¬í•´ë³´ë©´,               

#### â˜» ì°¨ì›ì˜ ì €ì£¼ì™€ì˜ ì—°ê´€ì„± 
ë¬¸ì œë¥¼ í‘¸ëŠ”ë° ìˆì–´ì„œ, íŠ¹íˆ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ì„ í†µí•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ë•Œ, training setì˜ feature ê°œìˆ˜ê°€ ë„ˆë¬´ ë§ë‹¤ë©´ ëª¨ë¸ì„ í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§€ê²Œ ë˜ê³ , ì¢‹ì€ ëª¨ë¸ ì¦‰ generalized and robust modelì„ ì–»ê¸° í˜ë“¤ë‹¤[2]. 

>The number of instances increases exponentially to achieve the same explanation ability when the number of variables increases. 

#### â˜» ê³¼ì í•©ê³¼ì˜ ì—°ê´€ì„±

<img src="https://github.com/soyounson/soyounson.github.io/blob/master/images/DR_Fig01.jpeg" width="600"/>

ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ë³´ë“¯, 2ê°œì˜ p'tì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ êµ¬í•œë‹¤ê³  í• ë•Œ, ì°¨ì›ì´ 2ì°¨ì›ì´ë©´ 0.52, 3ì°¨ì›ì´ë©´ 0.66..ì¦‰ ì°¨ì›ì´ ì¦ê°€í•  ìˆ˜ë¡ ê±°ë¦¬ê°€ ë” ë©€ì–´ì§„ë‹¤. ì´ëŠ” ë°ì´í„° p't ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ë©€ë‹¤ ([ëœë¤í•œ ë‘ p'tì‚¬ì´ì˜ í‰ê·  ê±°ë¦¬](https://ibmathsresources.com/2021/01/08/find-the-average-distance-between-2-points-on-a-square/) ë¡œ 

![\Large \sqrt{\frac{n}{6}}](https://latex.codecogs.com/svg.image?%5Csqrt%7B%5Cfrac%7Bn%7D%7B6%7D%7D)

ì´ê³ , nì€ nth dimension). ê·¸ë ‡ë‹¤ë©´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ ë°ì´í„°ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€í•œë‹¤ë©´ ë” ë©€ë¦¬ ë–¨ì–´ì ¸ìˆì„ìˆ˜ë„ ìˆê³ , ì´ë¥¼ ìœ„í•˜ì—¬ ì˜ˆì¸¡ì„ ìœ„í•˜ì—¬ ë” ë§ì€ extrapolationì„ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì €ì°¨ì›ë³´ë‹¤ predictionì´ ë¶ˆì•ˆì •í•˜ê²Œ ë˜ë©° ê³¼ì í•© (over-fitting)ì˜ ìœ„í—˜ì„±ì´ ì»¤ì§„ë‹¤. 

**ê·¸ëŸ¼ ì™œ overfittingì„ ì§€ì–‘í•˜ëŠ”ê°€ëŠ” ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡í•´ì§€ê³ , generalizedí•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì´ë‹¤.**

<img src="https://github.com/soyounson/soyounson.github.io/blob/master/images/DR_Fig02.jpeg" width="600"/>
Ref [3]


> If we train for too long, then we will overfit the data, which means that we have learnt about the noise and inaccuracies in the data as well as the actual function. Therefore, the model that we learns will be much too complicated and won't be abble to generalise [3]. 


#### â˜»  3 different ways to do dimensionality reduction [3]


1. Features selection: looking through the features that are available and seeing whether or not they are actually useful, i.e. ***correlated to the output variables.***
Feature derivation: deriving new features from the old ones, generally by applying ***transforms to the dataset that simply change the axes*** (coordinate system) of the graph by moving and rotating them.  The reason this performs dimensionality reduction is that it enables us to combine features and to identify which are useful and which are not. 
Clustering: in order to ***group together similar data points***, and to see whether this allows fewer features to be used. 
ì—¬ê¸°ì„œ ì°¨ì›ì˜ ì¶•ì†Œì—ì„œ ê°€ì¥ ë§ì´ ì–¸ê¸‰ë˜ëŠ” ê²ƒì€  Features selection (ë³€ìˆ˜ ì„ íƒ) ê³¼ Feature derivation (or Features extraction, ë³€ìˆ˜ ì¶”ì¶œ) ì´ë‹¤. Feature selectionì€ ë§ ê·¸ëŒ€ë¡œ ë‹¨ìˆœíˆ ì¤‘ìš”í•œ ë³€ìˆ˜ë§Œì„ ë¹¼ë‚´ëŠ” ê²ƒì´ê³ , Feature derivation (Feature extraction)ì€ ê¸°ì¡´ ë³€ìˆ˜ ì¡°í•©ì—ì„œ ìƒˆë¡œìš´ ë³€ìˆ˜ ë§Œë“œëŠ” ê¸°ë²•ìœ¼ë¡œ (a) ê¸°ì¡´ ë³€ìˆ˜ ê°€ìš´ë° ì¼ë¶€ë§Œ í™œìš©í•˜ëŠ” ë°©ì‹, (b) ëª¨ë“  ë³€ìˆ˜ ì‚¬ìš© (PCA, ê¸°ì¡´ ë³€ìˆ˜ë¥¼ ì„ í˜• ê²°í•©í•´ì„œ linear combinationí†µí•´ ìƒˆë¡œìš´ ë³€ìˆ˜ ë§Œë“¤ì–´ ëƒ„) ì´ ìˆë‹¤. 



### â˜¹ï¸ Features selection (ë³€ìˆ˜ ì„ íƒ)

ì¶”ê°€ì ìœ¼ë¡œ ë³€ìˆ˜ ì„ íƒì€ feature importance testë¥¼ í†µí•´ì„œ the most important featuersì„ ì°¾ëŠ”ë°ë„ ì‚¬ìš©ëœë‹¤. 

ì´ëŠ” ë‹¤ë¥¸ ê¸€ì—ì„œ ì¢€ ë” ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•œë‹¤. 

ìš°ì„ , ë‘ê°€ì§€ ë°©ë²•ì´ ìˆëŠ”ë° í•˜ë‚˜ëŠ” ë³€ìˆ˜ë¥¼ ì¶”ê°€í•´ê°€ëŠ” ë°©ë²•, ë‘ê°€ì§€ëŠ” ì²˜ìŒì— ëª¨ë‘ ê³ ë ¤í•œ í›„ í•˜ë‚˜ì”©  ë¹¼ë‚´ë©´ì„œ performaceê°€ ë³€í•˜ì§€ ì•Šìœ¼ë©´ ì œê±°í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ë‹¤. 

1. Constructive (Forward selection) : decide which feature to add next. that means, it starts with none, and then iteratively and more, testing the error at each stage to see whether or not it changed at all when that features was added.  
2. Destructive (Backward elimination) : to prune the decision tree, lopping off branches and checking whether the error changed at all [3]

#### â˜»  ì°¨ì›  ì¶•ì†Œë¥¼ ìœ„í•œ ì ‘ê·¼ ë°©ë²• 
ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ ë‘ê°€ì§€ ì ‘ê·¼ë²•ìœ¼ë¡œëŠ” 

1. Projection (íˆ¬ì˜) : A projection is the means by which you display the coordinate system and your data on a flat surface, such as a piece of paper or a digital screen. Mathematical calculations are used to convert the coordinate system used on the curved surface of earth to one for a flat surface.
Manifold learning (ë§¤ë‹ˆí´ë“œ í•™ìŠµ) : an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.
2. Projection (Ref [2])	Manifold learning  (Ref [2])


	 
### â˜ºï¸  Eigenvalue and Eigenvector 
LDAì™€ PCAë¥¼ ì´í•´í•˜ëŠ”ë° ìˆì–´ì„œ Eigenvalue ì™€ EigenvectorëŠ” ë°˜ë“œì‹œ ì´í•´í•´ì•¼ë§Œ í•˜ëŠ” ìˆ™ëª…ê°™ì€ ì¡´ì¬ì´ë‹¤.

ìš°ì„ , ì´ë“¤ì˜ ì˜ë¯¸ë¥¼ í™•ì¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤ [4].

> In linear algebra, an eigenvector or characteristic vector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it.
The corresponding eigenvalue, often denoted by, is the factor by which the eigenvector is scaled. Geometrically, an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction in which it is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed. Loosely speaking, in a multidimensional vector space, the eigenvector is not rotated.
ì„ í˜• ë³€í™˜ì˜ eignevector (ê³ ìœ ë²¡í„°)ëŠ” ê·¸ ì„ í˜• ë³€í™˜ì´ ì¼ì–´ë‚œ í›„ì—ë„ ë°©í–¥ì´ ë³€í•˜ì§€ ì•ŠëŠ”, 0ì´ ì•„ë‹Œ ë²¡í„°ì´ê³ , eigenvalue (ê³ ìœ ê°’)ì€ ê³ ìœ  ë²¡í„°ì˜ ê¸¸ì´ê°€ ë³€í•˜ëŠ” ë°°ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ê¸°í•˜í•™ì ìœ¼ë¡œ ë´¤ì„ ë•Œ, í–‰ë ¬(ì„ í˜•ë³€í™˜) Aì˜ ê³ ìœ ë²¡í„°ëŠ” ì„ í˜•ë³€í™˜ Aì— ì˜í•´ ë°©í–¥ì€ ë³´ì¡´ë˜ê³  ìŠ¤ì¼€ì¼(scale)ë§Œ ë³€í™”ë˜ëŠ” ë°©í–¥ ë²¡í„°ë¥¼ ë‚˜íƒ€ë‚´ê³  ê³ ìœ ê°’ì€ ê·¸ ê³ ìœ ë²¡í„°ì˜ ë³€í™”ë˜ëŠ” ìŠ¤ì¼€ì¼ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì´ë‹¤ [4,5].



$$ A\mathbf{v} = \lambda \mathbf{v} $$



***A*** ëŠ” eigenvalue, ***v*** ëŠ” eigenvetor, ê·¸ë¦¬ê³  ***ğº***ëŠ” scalarì´ë‹¤.  


>ì¦‰, ì´ ê³¼ì •ì€ scalingì— í•´ë‹¹í•˜ëŠ” ë³€í™˜ì´ë‹¤. ë”°ë¼ì„œ, ë²¡í„°ë¥¼ ì„ í˜•ë³€í™˜ í• ëŒ€, ì›ì ì—ì„œ ë©€ì–´ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” scalingí•˜ëŠ” ë°©í–¥ì„ ì°¾ëŠ” ê²ƒì´ ê³ ìœ ê°’ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒìœ¼ë¡œ ê·¸ ë°©í–¥ì´ ê³ ìœ ë²¡í„°ì˜ ë°©í–¥ì´ê³ , ê·¸ë•Œ ì´ë™í•˜ëŠ” ê±°ë¦¬ê°€ ê³ ìœ ê°’ì´ ëœë‹¤ [6]. 

ì—¬ì „íˆ, ì´í•´ê°€ 100% ì´í•´ê°€ ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ê·¸ëŸ¼ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì°¨ì›ì¶•ì†Œ (Dimensionality reduction)ì™€ì˜ ì—°ê´€ì„±ì€ ì–´ë–¤ ê²ƒì¼ê¹Œ? 

LDAì™€ PCAì—ì„œ ì—°ê´€ì„±ì„ ê¸°ìˆ í•˜ë„ë¡ í•œë‹¤. 

### â˜ºï¸ Linear Discriminant Analysis (LDA)
ìš°ì„ , LDAì™€ PCAëŠ” ë‘˜ë‹¤ Linear planeì— embeddingì„ ì‹œí‚¨ë‹¤ëŠ” ê³µí†µì ì„ ê°–ê³  ìˆì§€ë§Œ,  ì°¨ë³„ì„±ì„ ê¼½ìœ¼ë¼ë©´ LDAëŠ” class (Labelì— ëŒ€í•œ ì •ë³´)ê°€ ë¨¼ì € ì •í•´ì ¸ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. 

ì¦‰, LDAëŠ” supervisedì´ê³ , PCAëŠ” unsupervisedë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. 


Ref [3]
ìœ„ì˜ ê·¸ë¦¼[3]ì—ì„œ ì²˜ëŸ¼ ë‘ê°œì˜ classesë¡œ êµ¬ë¶„ëœ ë°ì´í„°ê°€ ìˆë‹¤ê³  ë³´ì. ì „ì²´ ë°ì´í„°ì—ì„œì˜ Meanì€ ğœ‡, ê° class (âœšê³¼ â—‹)ì—ì„œì˜ Meanì„ ğœ‡1,ğœ‡2, ê·¸ë¦¬ê³  ê° classì—ì„œì˜ covarianceë¥¼



$$ \sum_{j} (x_j - \mu)(x_j - \mu)^T $$
![\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}](https://latex.codecogs.com/svg.latex?x%3D%5Cfrac%7B-b%5Cpm%5Csqrt%7Bb%5E2-4ac%7D%7D%7B2a%7D)


ë¼ê³  í•˜ì. ìœ„ì˜ ê·¸ë˜í”„ì˜ ë‘ classì— í‘œí˜„í•˜ìë©´ ì•„ë˜ì™€ ê°™ë‹¤. 

Means	Covariance
	


***LDAì˜ í•µì‹¬ì€ within-class scatter, Swì™€ Between-classes scatter, SBì˜ ë¹„ìœ¨ì„ ìµœëŒ€í™” (maximization) í•˜ëŠ” ê²ƒì´ë‹¤. SB/Swë¥¼ ìµœëŒ€í™”í•¨ìœ¼ë¡œì¨ classì‚¬ì´ëŠ” ë©€ì–´ì ¸ì„œ ì˜ separationì´ ì´ë¤„ì§€ê³ , ê° class ë‚´ë¶€ì—ì„œëŠ” ë°ì´í„°ë¼ë¦¬ëŠ” ì˜ ëª¨ì—¬ìˆëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.*** 

ì¢€ ë” ìì„¸í•˜ê²Œ ì‚´í´ë³´ë©´, 


Ref [3]

#### â˜»  Sw, Within-class scatter


$$ S_W = \sum_{classes c}\sum_{j\in c} p_c(x_j - \mu_c)(x_j - \mu_c)^T $$



ì¦‰, ***ê° class ë‚´ë¶€ì—ì„œì˜ scatter ì •ë„ë¥¼ ë‚˜íƒ€ë‚¸ ê°’ì˜ í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì–´ì§„ë‹¤. ì´ ê°’ì€ ì‘ì€ ê°’ì„ ë„ì–´ì•¼ classì—ì„œì˜ ë°ì´í„°ë“¤ì´ êµ°ì§‘ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸***í•˜ê²Œ ëœë‹¤. 



Covariance matrix can tell us about the scatter within a dataset, which is the amount of spread that there is within the data. The way to find this scatter is to multiply the covariance by the Pc, the probability of the class (that is the number of datapoints there are in that class divided by the total number)[3].

#### â˜»  SB, Between-classes scatter
$$ S_B = \sum_{classes c}(\mu_c - \mu)(\mu_c - \mu)^T $$



ì „ì²´ ë°ì´í„°ì˜ mean vectorì™€ ê° classì˜ mean vectorì‚¬ì´ì˜ ê±°ë¦¬ì˜ ì œê³±ìœ¼ë¡œ, ì´ ê°’ì€ í° ê°’ì„ ê°–ì•„ì•¼ classë¼ë¦¬ì˜ separationì´ ì˜ ì´ë¤„ì§„ ê²ƒì„ ì˜ë¯¸í•˜ê²Œ ëœë‹¤. 



To be able to separate the data, we also want the distance between the classes to be large. This is know as the between-classes scatter. 
ì‚¬ì‹¤ ì—¬ê¸°ì„œ ì´ SB/SWë¥¼ ê°€ëŠ¥í•œ í¬ê²Œ í•œë‹¤ëŠ” ê²ƒì´ Dimensionality reductionì— ëŒ€í•˜ì—¬ ì–´ë–¤ê²ƒë„ ì˜ë¯¸í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œ ìš°ë¦¬ëŠ” ì—¬ì „íˆ 100%ì´í•´í•˜ê¸° í˜ë“¤ ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤. í•˜ì§€ë§Œ, ì°¨ì›ì˜ ìˆ˜ë¥¼ ê°ì†Œ ì‹œí‚¬ë•Œ ë¬´ì–¸ê°€ë¥¼ í•œë‹¤ê³  ë§í•  ìˆ˜ëŠ” ìˆì„ ê²ƒì´ë‹¤. ìš°ì„ , projectionì— ëŒ€í•´ì„œ ìƒê°í•´ë³´ì. 

ì•ì—ì„œ ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ ë‘ê°€ì§€ ì ‘ê·¼ë²• ì¤‘ í•˜ë‚˜ê°€ Projection (íˆ¬ì˜)ì´ë¼ê³  í–ˆë‹¤. 


Ref [3]


ìš°ë¦¬ê°€ ì•„ê¹Œë¶€í„° ë´ì™”ë˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ linear lineì— projectionì‹œí‚¨ë‹¤ê³  í•  ë•Œ, ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ Vector, wì— ë”°ë¼ì„œ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚œë‹¤. 



ìš°ì„ , projection of the data can be written as 

$$ z = \mathbf{w}^T \cdot \mathbf{x} $$ 



ì—¬ê¸°ì„œ, x ëŠ” datapoint. ì´ ê²ƒì€ ìš°ë¦¬ì—ê²Œ scalarê°’ì„ ì£¼ëŠ”ë°, ì´ scalarê°€ the distance along the w vector ì´ë‹¤. 



Projection.
To see remember that the equation, we write above, is the sum of the vectors multiplied together element-wise, and is equal to the size of w times the size of x times the cosine of the angle between them. We can make the size of w be 1, so that we don't have to worry about it, and all that is then described is the amount of x that lies along w. 
So we can compute the projection of our data along w for every point, and we will have projected our data onto a straight line. 


ì! ì´ì œ within-class scatterê³¼ between-class scatterì— ê°„ë‹¨í•œ ì‘ì—…ì„ í†µí•´ ì™œ SB/Swì˜ ìµœëŒ€í™” (maximization)ì™€ dimensionality reductionì´ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ë ¤ í•œë‹¤. 

$$ \sum_{classes c}\sum_{j\in c} p_c (\mathbf{w}^T \cdot (x_j - \mu_c))(\mathbf{w}^T \cdot (x_j - \mu_c))^T = \mathbf{w}^T S_w \mathbf{w} $$ $$ \sum_{classes c}\mathbf{w}^T(\mu_c - \mu)(\mu_c - \mu)^T\mathbf{w} = \mathbf{w}^T S_B \mathbf{w} $$



ì´ ë‘ ê°’ì˜ ë¹„ìœ¨ì€ Objective function, J(w)ë¡œ 



$$ J(\mathbf{w}) = \frac{\mathbf{w}^T S_B \mathbf{w}}{\mathbf{w}^T S_w \mathbf{w}} \Uparrow $$



In order to find the maximum value of this with respect to w, we differentiate it and set the derivative equal to 0 :



$$ \frac{S_B\mathbf{w}(\mathbf{w}^T S_W \mathbf{w})-S_W\mathbf{w}(\mathbf{w}^T S_B \mathbf{w})}{(\mathbf{w}^T S_W \mathbf{w})^2} = 0 $$



ì—¬ê¸°ì„œ Projection vector, wë¥¼ í’€ë©´ ëœë‹¤. 

$$ S_W \mathbf{w} = \frac{\mathbf{w}^T S_W \mathbf{w}}{\mathbf{w}^T S_B \mathbf{w}} S_B \mathbf{w} $$



ì—¬ê¸°ì„œ right hand sideë¥¼ ì¡°ê¸ˆë§Œ ìˆ˜ì •í•´ë³´ë©´ [7] ê´„í˜¸ ì•ˆì˜ ê°’ì€ scalarê°€ ë˜ê³ , ì´ê²ƒì„ ë‹¤ì‹œ ì“°ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 

$$ S_W \mathbf{w} = \lambda S_B \mathbf{w} $$

$$ S_B^{-1} S_W \mathbf{w} = \lambda \mathbf{w} $$



ì´ ì‹ì€ Eigenvalue, Eigenvectorì‹ê³¼ ìœ ì‚¬í•˜ë‹¤ (ì•„ë˜ ì‹ ì°¸ê³ )

$$ A\mathbf{v} = \lambda \mathbf{v} $$

A ëŠ” eigenvalue, v ëŠ” eigenvetor, ê·¸ë¦¬ê³  ğºëŠ” scalarì´ë‹¤.  

ë”°ë¼ì„œ, Known valuesì¸ Sw, SB ê·¸ë¦¬ê³  scalarê°’ì¸ ğºì™€ Eigenvalue decompositioní†µí•´ì„œ ìƒˆë¡œìš´ ì¶• wë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. 



### â˜ºï¸ Principal Components Analysis (PCA)
ì´ì œ unsupervised ì¸ PCAë¥¼ ë³´ë„ë¡ í•˜ì. 

ìš°ì„ , PCAëŠ” ì°¨ì›ì¶•ì†Œì—ì„œ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ë°©ë²•ì´ë‹¤. ì´ëŠ” ê¼­ ì°¨ì› ì¶•ì†Œ ë¿ë§Œ ì•„ë‹ˆë¼, ë°ì´í„° ì••ì¶•, feature ì¶”ì¶œ, ë°ì´í„° ì‹œê°í™”ë“±ì—ë„ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆë‹¤ [7]. 

Principal Component Analysis, or PCA, is a techinique that is widely used for applications such as dimensionality reduction, lossy data compression, feature extraction, and data visualization (Jolliffe,200).


ê·¸ëŸ¼, PCAëŠ” ë¬´ì—‡ì„ í•˜ëŠ” ê²ƒì¼ê¹Œ? ê°„ë‹¨íˆ ë§í•´ ë°ì´í„°ì˜ ê³µë¶„ì‚°ì´ í° ë°©í–¥ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. 




ì™œ? ì™œ ê³µë¶„ì‚°ì´ í°ë°©í–¥ì„ ì°¾ëŠ”ê±¸ê¹Œ? 

ê·¸ í•´ë‹µì€ ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë°ì´í„°ê°€ í©ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œë¥¼ í•œë‹¤ë©´ ê°€ì¥ ë§ì€ ì •ë³´ë¥¼ ê°–ê²Œ ë˜ê³  ì—­ìœ¼ë¡œ ê°€ì¥ ì ì€ ì •ë³´ ì†ì‹¤ì´ ì´ë¤„ì§„ë‹¤ëŠ” ë§ì´ë‹¤. ì´ê²ƒì´ ìš°ë¦¬ê°€ ì°¨ì›ì¶•ì†Œì—ì„œ ê¶ê·¹ì ìœ¼ë¡œ ì¶”êµ¬í•˜ëŠ” ë°©í–¥ì´ë¼ í•  ìˆ˜ ìˆë‹¤. 

ì¢€ ë” ìì„¸í•˜ê²Œ ì‚´í´ë³´ì. 




Ref [3]


>***The idea of a principal component is that it is a direction in the data with the largest variation.***
The algorithm first centres the data by subtracting off the mean, and then chooses the direction with the largest variation and places an axis in that direction, and then looks at the variation that remains and finds another axis that is ***orthogonal to the first*** and covers as much of the remaining variation as possible. It then iterates this until it has run out of possible axes. The end result is that all the variation is along the axes of the coordinate set, and so the covariance matrix is diagonalâ€”each new variable is uncorrelated with every variable except itself. Some of the axes that are found last have very little variation, and so they can be removed without affecting the variability in the data [3].

ì¦‰, the lagest variationì„ ê°–ëŠ” ì§êµ ì„±ë¶„ ì¶•ì„ ì°¾ê³ , ë°˜ëŒ€ë¡œ little variation, ì¦‰ data informationì´ ì ì€ ê²ƒì€ ì œê±°í•œë‹¤ëŠ” ê²ƒì´ë‹¤.  

Principal component (ì£¼ì„±ë¶„)ì€ ê°€ì¥ ë¶„ì‚°ì´ í° ë°©í–¥ì´ê¸° ë•Œë¬¸ì— ì£¼ì„±ë¶„ì— íˆ¬ì˜í•˜ì—¬ ë°”ê¾¼ ë°ì´í„°ëŠ” ì›ë³¸ì´ ê°€ì§€ê³  ìˆëŠ” íŠ¹ì„±ì„ ì˜ ë‚˜íƒ€ë‚´ê³  ìˆì„ ê²ƒì´ë‹¤.  

PCAì— ëŒ€í•œ Kevin Murphyì˜ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤ [9]. 

>The basic idea is to find a linear and orthogonal projection of the high dimensional data x âˆˆ R^{D} to a low dimensional subspace z âˆˆ R^{L}, such that the low dimensional representation is a â€œgood approximationâ€ to the original data, in the following sense: if we project or encode x to get z = w^{T}x, and then unproject or decode z to get x' = wz, then we want x' to be close to x in l2 distance.  In particular, we can define the following reconstruction error or distortion. 


ìˆœì°¨ì ì¸ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

1. ë°ì´í„° Xì—ì„œ í‰ê· ì„ êµ¬í•˜ê³ , ê·¸ í‰ê· ì— ì¢Œí‘œì¶•ì„ ê°€ì ¸ê°.
2. ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚° 
3. ì´ì— ëŒ€í•œ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„° êµ¬í•¨
4. ê³ ìœ ë²¡í„° ë°©í–¥ìœ¼ë¡œ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ë©´, ê°€ì¥ í° ë¶„ì‚°ì„ ê°€ì§€ëŠ” ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ

ì¢€ ë” ë””í…Œì¼í•œ ê´€ì ì—ì„œ ë°”ë¼ë³¸ë‹¤ë©´ [7], ë°ì´í„° Xë¥¼ wë¼ëŠ” ìƒˆë¡œìš´ ì¶•ì— projection (íˆ¬ì˜)ì‹œì¼œ, zë¼ëŠ” ìƒˆë¡œìš´ í–‰ë ¬ì„ êµ¬í•©ë‹ˆë‹¤. 



$$ \mathbf{z} = \mathbf{w}^{T} \mathbf{X} $$



ì°¨ì› ì¶•ì†Œì˜ ì›ë˜ ëª©í‘œëŠ” ë°ì´í„°ì˜ ì •ë³´ë¥¼ ìµœëŒ€í•œ ë³´ì¡´ = ë°ì´í„° Xì˜ ë¶„ì‚°ì˜ ìµœëŒ€í™”ì´ë¯€ë¡œ ê²°êµ­ zì˜ ë¶„ì‚°ë„ ìµœëŒ€í™”ê°€ ë˜ì•¼ í•œë‹¤. ì´ë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ 



$$ \begin{matrix} \mathbf{max} \left\{ Var (\mathbf{z}) \right\} &=& \mathbf{max} \left\{ Var(\mathbf{w}^{T} \mathbf{X}) \right\}\\ &=& \mathbf{max} \left\{ \mathbf{w}^{T} Var(\mathbf{X})\mathbf{w}\right\} &=& \mathbf{max} \left\{ \mathbf{w}^{T} \mathsf{\Sigma} \mathbf{w}\right\} \end{matrix}$$



ì—¬ê¸°ì„œ wì˜ í¬ê¸°ë¥¼ í¬ê²Œ í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ zì˜ ë¶„ì‚°ì„ ë†’ì¼ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œì•½ì‹ì„ ì¤€ë‹¤. 



$$ \parallel \mathbf{w} \parallel = \mathbf{w}^{T} \mathbf{w} = 1 $$



ì´ë¥¼ ë¼ê·¸ë‘ì§€ì•ˆ ë¬¸ì œë¡œ ë³€í˜•í•˜ë©´,  


$$ L = \mathbf{w}^{T} \mathsf{\Sigma} \mathbf{w} - \lambda ( \mathbf{w}^{T} \mathbf{w} - 1) $$ ë¯¸ë¶„ì„ í•´ì„œ ìµœëŒ€ê°’ì„ êµ¬í•˜ë©´, $$ \frac{\partial L}{\partial \mathsf{w}} = \mathsf{\Sigma} \mathbf{w} - \lambda \mathbf{w} = 0 $$ $$ (\Sigma} - \lambda) \mathbf{w} = 0 $$



wëŠ” ë°ì´í„° Xì˜ ê³µë¶„ì‚° í–‰ë ¬ì¸ âˆ‘ì— ëŒ€í•œ eigenvector (ê³ ìœ ë²¡í„°)ì´ë©°,  ì´ë¥¼ Principal component (ì£¼ì„±ë¶„)ì´ë¼ê³  í•œë‹¤. 

ì¢€ ë” ë””í…Œì¼í•œ ë¶€ë¶„ì€ ì¶”ê°€ì  ì´í•´ë¥¼ í•œ í›„ ì‘ì„±í•˜ê³ ì í•œë‹¤.


Ref [9]
ê´€ì‹¬ìˆë‹¤ë©´, ìœ ëª…í•œ Youtubeë¥¼ ì¶”ì²œí•œë‹¤.

ê°œì¸ì ìœ¼ë¡œ intro songì€ ë¬´ì²™ ë§˜ì— ë“ ë‹¤ :)


https://www.youtube.com/watch?v=HMOI_lkzW08&t=64s


#### â˜»  Kernel PCA

ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ ë³µì¡í•œ ë¹„ì„ í˜• íˆ¬í˜•ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ê¸°ë²•ìœ¼ë¡œ, ì´ ë˜í•œ unsupervised learningì´ë¯€ë¡œ ì¢‹ì€ ì»¤ë„ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒì„ ìœ„í•œ ëª…í™•í•œ ì„±ëŠ¥ ì¸¡ì • ê¸°ì¤€ì€ ì—†ë‹¤ [1].
** ì¢€ ë” ìì„¸í•œ ë‚´ìš©ì€ ì¶”í›„ ì—…ë°ì´íŠ¸ ì˜ˆì •ì´ë‹¤ **

### â˜» Reference
1. https://en.wikipedia.org/wiki/Dimensionality_reduction
2. [book, ê°œì¸ ì†Œì¥] Hands-on Machine laerning with Scikit-Learn Keras & TensorFlow : í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹ 2íŒ
3. [book, ê°œì¸ ì†Œì¥] Machine Learning: An Algorithmic Perspective by Stephen Marsland
4. https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors
5. [ë¸”ë¡œê·¸] https://darkpgmr.tistory.com/105â€‹
6. [ë¸”ë¡œê·¸] https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=je1206&logNo=220818602286 
7. [Github blog] https://ratsgo.github.io/machine%20learning/2017/03/21/LDA/
8. [book, ê°œì¸ ì†Œì¥] Pattern Recognition and Machine Learning
9. [book, ê°œì¸ ì†Œì¥] Machine Learning: A Probabilistic Perspective

Thanks for reading. Hope to see you again :o)


