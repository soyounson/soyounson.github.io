---
layout: post
title: Feature Engineering & Preprocessing for NLP
---

Before we consider diverse models to predict, pretty clean and well-organized ingredients, we are going to call **feataures**, should be well-prepared. 
Here, preprocessing and **Feature engineering** will be covered but these process are limited in NLP field. (Ofc, these concepts could be applied to fields.)

First things first, we should **Natural Language Process** is. ~~

but it is quite tricky and time consuming process to be read by the computer. 
~~
****** ì‹¤ì œ ì‹¤ë¬´ì—ì„œ í™•ì¸ í•  ê²ƒ ***
ëª¨ë¸ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤ ë°ì´í„°ì—ì„œ ìŠ¹ë¶€ê°€ ë‚˜ëŠ” ê²ƒ 
ì°¨ë¼ë¦¬ í† í¬ë„ˆë‚˜ì´ì €ê°™ì€ ê²ƒì„ ë°”ê¾¸ëŠ” ê²ƒì´ ì¤‘ìš”í•¨ 
ì—­ì‚¼ê°í˜•ì´ì–´ì„œ ëª¨ë¸ì„ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤, ì´ˆê¸°ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ì¢‹ìŒ 
ë²„íŠ¸ë„ ë²„íŠ¸ì˜ ëª¨ë¸ë³´ë‹¤ oovì¤„ì—¬ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒ
í† í¬ë‚˜ì´ì €ì—ì„œ ì›Œë“œ í”¼ìŠ¤ë¥¼ ì¤„ì´ëŠ” ê²ƒ. 
íŒŒì‹±í• ë•Œ (í•œêµ­ì–´ì—ì„œ ì–´ê°„, ì–´ë¯¸ ìª¼ê°¤ë•Œ)ì™€ ê°™ì€ í† í¬ë‚˜ì´ì €ë¥¼ ì¼ì„ë•Œ ì„±ëŠ¥ ì˜¬ë¼ê°
ì—¬ì „íˆ ë„ë©”ì¸ì´ ì¤‘ìš”. ë„ë©”ì¸ì—ì„œ ë§ì´ ì“°ëŠ” ì–¸ì–´ê°€ ë‹¨ì–´ë¡œ í• ë‹¹ë¨?
~~

-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ Preprocessing              
  - tokenization         
  - remove stop words         
  - stemming           
  - POS tagging         
  - Lemmatization             
  - language detection  
          
â˜ºï¸ Feature extraction            
  - weighted words - BOW          
  - countvectorizer          
  - TF-IDF          

â˜ºï¸ Embedding          
  - word2vec -> gensim          
  - Glove          
  - FastText          

â˜ºï¸ Feature Selection                      
â˜ºï¸ What the next step is?          
â˜» Reference
          
-----------------------------------------------------------------------

The pipline of NLP is below

![Fig01](/images/NLP_FE_Fig01.png)
Ref [1]
to undestand datasets and also problems, we should spend 60% (even more) time of the process in preprocessing/feature engineering. Simply 


### â˜ºï¸ Preprocessing 
 
#### â˜» Data cleaning [1,2] 

basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.*

removing urls
removing HTML tags
removing Emojis
removing punctuations
spelling correction

+ Capitalization/ Lower case [2]
The most common approach in text cleaning is capitalization or lower case due to the diversity of capitalization to form a sentence. This technique will project all words in text and document into the same feature space. However, it would also cause problems with exceptional cases such as the USA or UK, which could be solved by replacing typos, slang, acronyms or informal abbreviations technique.
The simple example is below : 


Forest fire near La Ronge Sask. Canada -> forest fire near la ronge sask. canada


+ Expand the Contractions
We use the contractions package to expand the contraction in English such as we'll -> we will or we shouldn't've -> we should not have.
```
Y'all can -> you all can 
```

+ Noise Removal
Text data could include various unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters (symbols, emojis, and other graphic characters).
  - Remove URLs
  - Remove HTML tags
  - Remove Non-ASCI
  - Remove special characters
(Remove special characters : The special characters could be symbols, emojis, and other graphic characters. We use the "Toxic Comment Classification Challenge" dataset as the "Real or Not? NLP with Disaster Tweets" dataset do not have any special charaters in their text.) 


+ Remove punctuations

+ Other Manual Text Cleaning Tasks
- Replace the Unicode character with equivalent ASCII character (instead of removing)
- Replace the entity references with their actual symbols  instead of removing as HTML tags
- Replace the Typos, slang, acronyms or informal abbreviations - depend on different situations or main topics of the NLP such as finance or medical topics.
```
sample_typos_slang = {
                                "w/e": "whatever",
                                "usagov": "usa government"}
  # Acronyms
  sample_acronyms =  { 
                            "mh370": "malaysia airlines flight 370",
                            "okwx": "oklahoma city weather",
                            "arwx": "arkansas weather"}
 # Some common abbreviations 
 sample_abbr = {
                        "$" : " dollar ",
                        "â‚¬" : " euro ",
                        "4ao" : "for adults only"}
 ```      
 
- List out all the hashtags/ usernames then replace with equivalent words
- Replace the emoticon/ emoji with equivalant word meaning such as ":)" with "smile" 
- Spelling correction : Spelling correction could also be considered an optional preprocessing task as the social media text data is often are typos or mistyped. However, the spelling correction output should be carefully double-checked with the original text input as it could be a mistake.


### â˜ºï¸ text preprocessing [2]

#### â˜» tokenization 
Tokenization is a common technique that **split a sentence into tokens**, where a token could be characters, words, phrases, symbols, or other meaningful elements. By breaking sentences into smaller chunks, that would help to investigate the words in a sentence and also the subsequent steps in the NLP pipeline, such as stemming.

| Original | Cleaning | Tokens |
|---|---|---|
|Forest fire near La Ronge Sask. Canada|forest fire near la ronge sask canada|[forest, fire, near, la, ronge, sask, canada]|


#### â˜» Remove Stop Words (or/and Frequent words/ Rare words):
Stop words are common words in any language that occur with a high frequency but do not deliver meaningful information for the whole sentence. For example, {â€œaâ€, â€œaboutâ€, â€œaboveâ€, â€œacrossâ€, â€œafterâ€, â€œafterwardâ€, â€œagainâ€, ...} can be considered as stop words. Traditionally, we could remove all of them in the text preprocessing stage. However, refer to the example from the Natural Language Processing in Action book:

Mark reported to the CEO
Suzanne reported as the CEO to the board
In your NLP pipeline, you might create 4-grams such as reported to the CEO and reported as the CEO. If you remove the stop words from the 4-grams, both examples would be reduced to "reported CEO", and you would lack the information about the professional hierarchy. In the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board. Unfortunately, retaining the stop words within your pipeline creates another problem: it increases the length of the n-grams required to make use of these connections formed by the otherwise meaningless stop words. This issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example. Designing a filter for stop words depends on your particular application.

In short, removing stop words is a common method in NLP text preprocessing, whereas, it needs to be experimented carefully depending on different situations.

-> ì¦‰, sometimes, stopwords are important to understand sentences. Thus, just removing stop words is not the best option. 

nltkì— tokenizer ì‚¬ì „ì´ ì¡´ì¬. í•œêµ­ì–´ì˜ ê²½ìš°ëŠ” POS tagging ì´í›„ì— ì§„í–‰

#### â˜» Stemming : ì–´ê°„ì¶”ì¶œ, ë¶„ì„ê²°ê³¼ê°€ ì•ˆì¢‹ì•˜ì„ë•Œ í•˜ëŠ” ê²½ìš°ê°€ ì¼ë°˜ì  
Stemming is a process of extracting a root word - identifying a common stem among various forms (e.g., singular and plural noun form) of a word, for example, the words "gardening", "gardener" or "gardens" share the same stem, garden. Stemming uproots suffixes from words to merge words with similar meanings under their standard stem.

There are three major stemming algorithms in use nowadays:

- Porter - PorterStemmer()): This stemming algorithm is an older one. Itâ€™s from the 1980s and its main concern is removing the common endings to words so that they can be resolved to a common form. Itâ€™s not too complex and development on it is frozen. Typically, itâ€™s a nice starting basic stemmer, but itâ€™s not really advised to use it for any production/complex application. Instead, it has its place in research as a nice, basic stemming algorithm that can guarantee reproducibility. It also is a very gentle stemming algorithm when compared to others.
- Snowball - LancasterStemmer(): This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer. That being said, it is also more aggressive than the Porter stemmer. A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. There is about a 5% difference in the way that Snowball stems versus Porter.
- Lancaster - SnowballStemmer(): Just for fun, the Lancaster stemming algorithm is another algorithm that you can use. This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, you can add your own custom rules to this algorithm very easily. Itâ€™s a good choice for that. One complaint around this stemming algorithm though is that it sometimes is overly aggressive and can really transform words into strange stems. Just make sure it does what you want it to before you go with this option!

- POS tagging : í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒ. ë™ì‚¬/ëª…ì‚¬/... ë¬¸ë§¥ì ìœ¼ë¡œ ì˜ íŒŒì•…í• ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê³¼ì •. í’ˆì‚¬ì— ë”°ë¼ì„œ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ê²½ìš°ê°€ ë°œìƒ. í•œêµ­ì–´ì˜ ê²½ìš° 

### â˜ºï¸ Text Feature Extraction 

transfer Natural language into numerical values the computer understand and read 
  + weighted words - BOW
    - countvectorizer
    - TF-IDF
  + one-hot encoding 
  + target encoding : Label of NLP is categorical values, label (text)
> Time to time, TF might deliver better results.     
### â˜ºï¸ word embedding 
  - word2vec -> gensim 
  - Glove
  - FastText
  - Bert : transformer 30ì–µê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ í•™ìŠµì‹œí‚¨í›„, ë³¸ì¸ì˜ í…ŒìŠ¤í¬ì— ë”°ë¼ì„œ fine tunningì„ í•˜ë©´ ì •í™•ë„ê°€ ë†’ìŒ. 

GloVe for Vecttorization [3]
use GloVe pretrained corpus model to represent our words.It is available in 3 varieties

ê¸€ë¡œë¸Œ(Global Vectors for Word Representation, GloVe)ëŠ” ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ 2014ë…„ì— ë¯¸êµ­ ìŠ¤íƒ í¬ë“œëŒ€í•™ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ì•ì„œ í•™ìŠµí•˜ì˜€ë˜ ê¸°ì¡´ì˜ ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ LSA(Latent Semantic Analysis)ì™€ ì˜ˆì¸¡ ê¸°ë°˜ì˜ Word2Vecì˜ ë‹¨ì ì„ ì§€ì í•˜ë©° ì´ë¥¼ ë³´ì™„í•œë‹¤ëŠ” ëª©ì ìœ¼ë¡œ ë‚˜ì™”ê³ , ì‹¤ì œë¡œë„ Word2Vecë§Œí¼ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ë‹¨ì •ì ìœ¼ë¡œ Word2Vecì™€ GloVe ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” ë›°ì–´ë‚˜ë‹¤ê³  ë§í•  ìˆ˜ëŠ” ì—†ê³ , ì´ ë‘ ê°€ì§€ ì „ë¶€ë¥¼ ì‚¬ìš©í•´ë³´ê³  ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•©ë‹ˆë‹¤.


### â˜ºï¸ comparison of feature extraction techinique  

ì˜ˆë¡œ Bertì˜ í•œê³„ê°€ ì¡´ì¬í•œë‹¤ë©´, 
ì´ëŸ° ë¬¸ì œì˜€ì„ë•ŒëŠ” ë‹¤ë¥¸ ê²ƒ ì‚¬ìš© í•  ê²ƒ
ë‹¤ë¥¸ ì ‘ê·¼ë²•: ê° ë°©ë²•ì˜ ì•½ì ì„ í™•ì¸í•˜ë©´ ì¢‹ì„ ë“¯
Bert (Transformer)ë§Œ ì‚¬ìš©í•˜ë‹¤ë³´ë‹ˆ, ê¸°ì¡´ ëª¨ë¸ë“¤ RNN
A : architecture 



### â˜ºï¸ Feature selection 


### â˜ºï¸ Next steps?


**sample**

![\Large \sqrt{\frac{n}{6}}](https://latex.codecogs.com/svg.image?%5Csqrt%7B%5Cfrac%7Bn%7D%7B6%7D%7D)
<!-- https://www.codecogs.com/latex/eqneditor.php -->




### â˜» Reference
1. [Kaggle]  https://www.kaggle.com/competitions/nlp-getting-started/data
2. [Kaggle, NL_ preprocess] https://www.kaggle.com/code/longtng/nlp-preprocessing-feature-extraction-methods-a-z/notebook
3. [wiki doc] https://wikidocs.net/22885
4. [book, ê°œì¸ ì†Œì¥] Hands-on Machine laerning with Scikit-Learn Keras & TensorFlow : í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹ 2íŒ
5. [book, ê°œì¸ ì†Œì¥] Machine Learning: An Algorithmic Perspective by Stephen Marsland
6. 


ğŸŒº **Thanks for reading. Hope to see you again :o)**
