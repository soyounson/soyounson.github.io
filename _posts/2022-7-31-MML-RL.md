---
layout: post
title: Linear Regression <img src="/images/B-icon-ver.png" width="30">
---

### [ì½”ë“œë¦¬ë·° | ê°œë…] Linear Regression 

**This article is based on Medium Blog titled by 'Introduction to Linear Regression in Python'[1].**

**ì´ ë¸”ë¡œê·¸ ê¸€ì€ Medium Blog titled by 'Introduction to Linear Regression in Python'[1]ì„ referenceë¡œ í•´ì„œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.**


> ë³¸ ê¸€ì€ ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ ì§„í–‰í•˜ëŠ” 'ğŸŒ± í’€ììŠ¤ì¿¨, Mathematics for Machine Learning S2'ì—ì„œ ê³µë¶€ í•œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. 


<p align="center">
<img src="/images/linear_process.gif" width="450">
</p>



-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ Linear Regression
  - y_hat (predicted value) vs y (ground truth)?

â˜ºï¸ Linear regression w/ the OLS method

â˜ºï¸ Linear regression w/ statsmodels

â˜ºï¸ Linear regression w/ scikit-learn

â˜» Reference

-----------------------------------------------------------------------

### â˜ºï¸ Linear Regression (ì„ í˜•íšŒê·€)

ì„ í˜• íšŒê·€ë¥¼ ë‹¤ë£¨ê¸°ì— ì•ì„œ, íšŒê·€ (regression)ì´ë¼ëŠ” ê°œë…ì„ ì•Œê³  ê°€ëŠ” ê²ƒì´ ê½¤ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•œë‹¤. 

ìš°ì„ , íšŒê·€ì˜ ì‚¬ì „ì  ì˜ë¯¸ë¥¼ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤ (ë‘ë²ˆì§¸ ì˜ë¯¸) [2,3].
~~~~~~~~~~~~~~~~~~~~~
â—¦ Regression 
  1. a return to a former or less developed state.
  2. `STATISTICS` a measure of the relation between the mean value of one variable (e.g. output) and corresponding values of other variables (e.g. time and cost).
â—¦ íšŒê·€
  1. í•œ ë°”í€´ ëŒì•„ ì œìë¦¬ë¡œ ëŒì•„ì˜¤ê±°ë‚˜ ëŒì•„ê°.
  2. í•˜ë‚˜ì˜ ì¢…ì† ë³€ìˆ˜ì™€ ë‘ ê°œ ì´ìƒì˜ ë…ë¦½ ë³€ìˆ˜ ì‚¬ì´ì— ë‚˜íƒ€ë‚˜ëŠ” ê´€ê³„ë¥¼ ìµœì†Œ ì œê³±ë²•ìœ¼ë¡œ ì¶”ì •í•˜ëŠ” ë°©ë²•.
~~~~~~~~~~~~~~~~~~~~~

ì¦‰, ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ì˜ ê´€ê³„ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê° í•  ìˆ˜ ìˆëŠ”ë°, ì´ì— ëŒ€í•´ì„œ ì¢€ ë” casual definitionì„ í™•ì¸í•˜ë©´ [4] 

> In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion.

ì •ë¦¬í•´ë³´ë©´ ë…ë¦½ë³€ìˆ˜ (x) ì™€ ì¢…ì†ë³€ìˆ˜ (y) ì˜ ê´€ê³„ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ íšŒê·€ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ ì¤‘ ê°€ì¥ ì¼ë°˜ì ì´ê³  í”í•œ í˜•íƒœê°€ ì„ í˜•íšŒê·€ (Linear regression)ì¸ë°, ì´ëŠ” íŠ¹ì • ìˆ˜í•™ì ì¸ ê¸°ì¤€ì— ë”°ë¼ì„œ ë°ì´í„°ê°€ ê°€ì¥ ê·¼ì ‘í•˜ê³  ì˜ ë§ëŠ” ì„ ì„ ì°¾ëŠ” ê²ƒì´ë¼ê³  í•œë‹¤ (ì§ì—­ì´ ì´ìƒí•œ ì ì€ ì–‘í•´ë¶€íƒí•©ë‹ˆë‹¤.) 

ì—¬ê¸°ê¹Œì§€ë„ íšŒê·€ì— ëŒ€í•´ì„œ ì˜ ì™€ë‹¿ì§€ ì•Šì„ ìˆ˜ë„ ìˆì–´ì„œ, ê°„ë‹¨í•œ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 

<img src="https://latex.codecogs.com/svg.image?y&space;=&space;\alpha&space;x&space;&plus;&space;\beta&space;&plus;&space;\epsilon" title="y = \alpha x + \beta + \epsilon" />

ì—¬ê¸°ì„œ ë…ë¦½ë³€ìˆ˜ëŠ” xë¡œ input/predictor valueì´ê³ , ì¢…ì†ë³€ìˆ˜ yëŠ” output/label variable (ground truth), ê·¸ë¦¬ê³  ğœ€ ì€ random noiseë¡œ ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê¸° í˜ë“¤ë‹¤. ë”°ë¼ì„œ ì´ ì‹ì„ ì¡°ê¸ˆ ì •ë¦¬í•˜ë©´ 

<img src="https://latex.codecogs.com/svg.image?\hat{y}&space;=&space;\alpha&space;x&space;&plus;&space;\beta" title="\hat{y} = \alpha x + \beta" />

ì´ê³ , ì´ëŠ” yì™€ y_hatì˜ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. 

<img src="https://latex.codecogs.com/svg.image?y&space;=&space;\hat{y}&space;&plus;&space;\epsilon" title="y = \hat{y} + \epsilon" />


ìš°ì„ , ìš°ë¦¬ì˜ ê´€ì‹¬ì‚¬ëŠ” ğœŸy = y - y_hatì„ ì‘ê²Œ ë§Œë“œëŠ” ê²ƒìœ¼ë¡œ, ì´ë¥¼ ìœ„í•´ì„œëŠ” y_hatì„ ì˜ ì •ì˜í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ğœŸyë¥¼ ì‘ê²Œí•˜ëŠ” ìµœì ì˜ ğ›¼ì™€ ğ›½ ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ìš°ë¦¬ê°€ ì•ìœ¼ë¡œ í•˜ê³ ì í•˜ëŠ” ë°”ì´ë‹¤. 

#### â˜» y_hat (predicted value) vs y (ground truth)?

ê·¸ëŸ°ë°, ì—¬ê¸°ì„œ ê°‘ìê¸° ë“±ì¥í•œ y_hatì€ ë¬´ì—‡ì¼ê¹Œ? 

ì´ì „ ê¸€ì¸ [Dataí†ºì•„ë³´ê¸°](https://soyounson.github.io/Data/)ì—ì„œ ì–¸ê¸‰í–ˆë˜ ë¶€ë¶„ì¸ë°, ì‹¤ì œ í˜„ìƒì„ ê°„ë‹¨í•œ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ê¸°ëŠ” ì‰½ì§€ ì•Šë‹¤ [5].

ìš°ë¦¬ê°€ ë§ì´ ë‹¤ë£¨ëŠ” ìˆ˜í•™ì /ë¬¼ë¦¬í•™ì  ì´ë¡ ì´ë‚˜ ì‹ë“¤ì€ ì‹¤ì œ í˜„ìƒì„ constraints, assumption, and conditionsë“±ì„ ê³ ë ¤í•´ì„œ ê°€ì¥ ê·¼ì‚¬í•˜ê²Œ ë‚˜íƒ€ë‚¸ ê²ƒ ì¸ë‹¤. ì¦‰, ì‹¤ì œ í˜„ìƒì€ ìš°ë¦¬ê°€ ìƒê°í•˜ëŠ” ê²ƒë³´ë‹¤ ë¬´ì²™ ë³µì¡í•˜ê¸°ë•Œë¬¸ì— ì‹ ë§Œì´ ì •í™•í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•œë‹¤ (ê·¹ë‹¨ì ìœ¼ë¡œ ë§í•˜ë©´). 

ë”°ë¼ì„œ exact solution (ì°¸ê°’)ì´ ì•„ë‹Œ approximate solution (ê·¼ì‚¬ê°’)ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ë‹¨, exact solution ê³¼ì˜ ì°¨ì´ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” ìµœì í™”ëœ ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ questë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. 

ì´ëŸ° ê´€ì ì—ì„œ ìš°ë¦¬ëŠ” predicted value (ì´í•´ë¥¼ ë•ê¸°ìœ„í•´ approximate solutionê³¼ ê°™ì€ ê°œë…ìœ¼ë¡œ ì´í•´í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤)ì¸ y_hatê³¼ ì‹¤ì œ ë°ì´í„°ì¸ yì˜ ì°¨ì¸ ğœŸy = y - y_hatì„ ê°€ì¥ ì‘ê²Œí•˜ëŠ” parametersì¸ ğ›¼ì™€ ğ›½ ì´ë‹¤. 


ì¢€ ë” ìì„¸í•˜ê²Œ ë°ì´í„° ê´€ì ì—ì„œ Regressionì„ ì´í•´í•œë‹¤ë©´ [6]

> In regression, we aim to find a function f that maps input x âˆˆ â„^{D} to correspnding function value f(x) âˆˆ â„. We assume we are given a set of training input x and correponding noisy observation y = f(x) + ğœ€, where ğœ€ is an i.i.d. random variable that describes measurement/observation noise and potentially unmodeled processes.


ê·¸ë¦¼ì„ í†µí•´ì„œ í™•ì¸í•˜ë©´ [6]

(ì•„ë˜ schematicì€ MMLì˜ Figure 9.1ì— í•œê¸€ë¡œ ë‚´ìš©ì„ ì¶”ê°€í•´ì„œ ì‘ì„±í•œ ê²ƒ ì…ë‹ˆë‹¤.)


<p align="center">
<img src="/images/Linear_regression_MML_SS.jpg" width="800">
</p>

### â˜ºï¸ Linear regression w/ the Ordinary Least Squares (OLS) method

`Scikit learn`ì˜ `LinearRegression` functionì„ ê·¸ëŒ€ë¡œ ì ìš©í•´ì„œ ë¬¸ì œë¥¼ í’€ê¸°ì— ì•ì„œ mechanismì„ ì˜ ì´í•´í•˜ëŠ” ê²ƒì´ ë¬´ì²™ì´ë‚˜ ì¤‘ìš”í•˜ë‹¤. 

ë”°ë¼ì„œ random dataì™€ the Ordinary Least Squares (OLS) methodë¡œ ğ›¼ì™€ ğ›½ ë¥¼ êµ¬í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤. 

(ìì„¸í•œ ë¶€ë¶„ì€ ğŸŒˆ[Code: Jupyter notebook](https://github.com/soyounson/ML_w_code/blob/main/01_Linear_Regression/09_MML_Linear_regression.ipynb)ë¥¼ ì°¸ê³  ë¶€íƒë“œë¦½ë‹ˆë‹¤.)

#### â˜» Dataset 

ë°ì´í„°ëŠ” ì´ 100ê°œë¡œ, mean = 1.5, stddev = 2.5ê°’ì„ ê°–ê³  randomìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œë‹¤. 

```
(base) print(np.mean(x),np.mean(y))
=========================================
 check statistical values
-----------------------------------------
x_mean: 1.6495200388362121
y_mean: 2.5358624970247825
=========================================
```

#### â˜» the OLS method 

Objective of the least squares (OLS) method ì„ í†µí•´ ğ›¼ì™€ ğ›½ë¥¼ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤ [1].

<img src="https://latex.codecogs.com/svg.image?\alpha&space;=&space;\frac{\sum_{i=1}^{n}(X_i&space;-&space;\bar{X})(Y_i&space;-&space;\bar{Y})}{\sum_{i=1}^{n}(X_i&space;-&space;\bar{X})^2}" title="\alpha = \frac{\sum_{i=1}^{n}(X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n}(X_i - \bar{X})^2}" />

<img src="https://latex.codecogs.com/svg.image?\beta&space;=&space;\bar{Y}-\beta\bar{X}" title="\beta = \bar{Y}-\beta\bar{X}" />

where X^bar and Y^bar are the mean values of X and Y.

ë°ì´í„° ë¶„í¬ë¥¼ ì‹œê°í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. black circle ì´ ì‹¤ì œ ground truth data p't ì´ê³ , red solid lineì´ predicted valueë¡œ ì´ë¤„ì§„ functionì´ë‹¤. 

<p align="center">
<img src="/images/Linear_regression_w_OLS.png" width="800">
</p>


```
(base) print('alpha=',alpha,'beta=',beta)
=========================================
alpha = 0.3229396867092763
beta = 2.0031670124623426
=========================================
```
ë”°ë¼ì„œ, ì—¬ê¸°ì„œ êµ¬í•œ ìµœì ì˜ í•¨ìˆ˜ëŠ” f(x) = 0.32 x + 2 ì´ë‹¤. 


ì´ì œëŠ” ë‘ê°œì˜ python modulesì¸ `statsmodels` ê³¼ `scikit-learn`ì„ ì´ìš©í•˜ë„ë¡ í•œë‹¤ [1].

~~~~~~~~~~~~~~~~~~~~~
â—¦ statsmodels â€” a module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.
â—¦ scikit-learn â€” a module that provides simple and efficient tools for data mining and data analysis.
~~~~~~~~~~~~~~~~~~~~~

### â˜ºï¸ Linear regression w/ statsmodels

ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ë°ì´í„°ëŠ” advertising dataë¡œ, [kaggle](https://www.kaggle.com/datasets/ashydv/advertising-dataset)ì—ì„œ ë‹¤ìš´ ë°›ì„ ìˆ˜ ìˆë‹¤ [7].

(ìì„¸í•œ ë¶€ë¶„ì€ ğŸŒˆ[Code: Jupyter notebook](https://github.com/soyounson/ML_w_code/blob/main/01_Linear_Regression/09_MML_Linear_regression.ipynb)ë¥¼ ì°¸ê³  ë¶€íƒë“œë¦½ë‹ˆë‹¤.)


#### â˜» Dataset 

ë°ì´í„°ëŠ” ì•„ë˜ ê¸°ìˆ í•˜ëŠ” ë°”ì™€ ê°™ì´ ì´ 200ê°œì˜ rowì™€ 4ê°œì˜ columnìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 

```
(base) print(advert.shape)
(base) print(advert.describe())
(base) print(advert.head())
=========================================
total size :  (200, 4)
-----------------------------------------
               TV       Radio   Newspaper       Sales
count  200.000000  200.000000  200.000000  200.000000
mean   147.042500   23.264000   30.554000   15.130500
std     85.854236   14.846809   21.778621    5.283892
min      0.700000    0.000000    0.300000    1.600000
25%     74.375000    9.975000   12.750000   11.000000
50%    149.750000   22.900000   25.750000   16.000000
75%    218.825000   36.525000   45.100000   19.050000
max    296.400000   49.600000  114.000000   27.000000
=========================================
      TV  Radio  Newspaper  Sales
0  230.1   37.8       69.2   22.1
1   44.5   39.3       45.1   10.4
2   17.2   45.9       69.3   12.0
3  151.5   41.3       58.5   16.5
4  180.8   10.8       58.4   17.9
=========================================
```
#### â˜» statsmodelsâ€™ ols function

`statsmodels`ì„ ì´ìš©í•´ì„œ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤ [8].

> statsmodels.formula.api: A convenience interface for specifying models using formula strings and DataFrames. This API directly exposes the from_formula class method of models that support the formula API. Canonically imported using import statsmodels.formula.api as smf

ì—¬ê¸°ì„œëŠ” xëŠ” TV datasetì„ yëŠ” Salesë§Œ ê³ ë ¤í•˜ì˜€ë‹¤. 

```
(base) print(model.params)
=========================================
Intercept    6.974821
TV           0.055465
dtype: float64
=========================================
```

ì—¬ê¸°ì„œ Interceptê°€ ğ›½ì´ê³ , TVê°€ ğ›¼ì´ë‹¤. ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ sale = 0.05x + 6.97ì´ë‹¤.

ë°ì´í„° ë¶„í¬ë¥¼ ì‹œê°í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. black circle ì´ ì‹¤ì œ ground truth data p't ì´ê³ , red solid lineì´ predicted valueë¡œ ì´ë¤„ì§„ functionì´ë‹¤. 

<p align="center">
<img src="/images/Linear_regression_statsmodel.png" width="800">
</p>

ì´ë¥¼ í†µí•´ì„œ TV ê´‘ê³ ë¥¼ ë§ì´ í• ìˆ˜ë¡ ë§ì´ íŒ”ë¦°ë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤. 

### â˜ºï¸ Linear regression w/ scikit-learn

ì•ì—ì„œëŠ” 1ê°œì˜ ì¢…ì†ë³€ìˆ˜ (uni-variable)ì„ ì´ìš©í–ˆë‹¤ë©´, ì—¬ê¸°ì„œëŠ” ì—¬ëŸ¬ê°œì˜ ë…ë¦½ë³€ìˆ˜, x (multi-variable)ì„ ê³ ë ¤í•´ì„œ multi-Linear regressionì„ ë§Œë“¤ë„ë¡ í•œë‹¤. 

**univariable/multivariableê³¼ univariate/multivariateì— ëŒ€í•´ì„œ ê¶ê¸ˆí•˜ë‹¤ë©´ ê¸°ì¡´ ë¸”ë¡œê·¸ ê¸€ì¸ [Multivariate (ë‹¤ë³€ëŸ‰) vs Multivariable (ë‹¤ë³€ìˆ˜)ë€?](https://soyounson.github.io/variate_variable/) ì°¸ê³  ë°”ëë‹ˆë‹¤ [5].**

(ìì„¸í•œ ë¶€ë¶„ì€ ğŸŒˆ[Code: Jupyter notebook](https://github.com/soyounson/ML_w_code/blob/main/01_Linear_Regression/09_MML_Linear_regression.ipynb)ë¥¼ ì°¸ê³  ë¶€íƒë“œë¦½ë‹ˆë‹¤.)

Multi-variableì´ë¯€ë¡œ, ì‹ì€ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. 

<img src="https://latex.codecogs.com/svg.image?\hat{y}&space;=&space;\alpha_1{x_1}&space;&plus;&space;\alpha_2{x_2}&space;&plus;&space;\alpha_3{x_3}&plus;&space;\cdots&space;&plus;&space;\beta" title="\hat{y} = \alpha_1{x_1} + \alpha_2{x_2} + \alpha_3{x_3}+ \cdots + \beta" />

ë°ì´í„°ëŠ” ë™ì¼í•œ advertisingì´ê³ , multivariableë¡œ xëŠ” x1ì¸ TVì™€ x2ì¸ Radio, yëŠ” Salesì„ ê³ ë ¤í•˜ì˜€ë‹¤.

```
(base) print(f'alpha = {model.coef_}')
(base) print(f'beta = {model.intercept_}')
=========================================
alpha = [0.05444896 0.10717457]
beta = 4.63087946409777
=========================================
```

ì—¬ê¸°ì„œ êµ¬í•œ multiple linear regression modelì— ìš°ë¦¬ê°€ ê°–ê³  ìˆëŠ” TVì™€ Radio ë°ì´í„° (í˜¹ì€ ìƒˆë¡œìš´ ë°ì´í„°)ë¥¼ ê³ ë ¤í•´ì„œ salesì„ ì˜ˆì¸¡í• ìˆ˜ ìˆë‹¤. 

-----------------------------------------------------------------------

ë³¸ ë¸”ë¡œê·¸ ê¸€ì—ì„œëŠ” MMLì—ì„œ ë°°ìš´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ Linear regressionì˜ ê°œë…ì„ ì´ìš©í•˜ê³  Medium blogì— ìˆëŠ” Introduction to Linear Regression ì„ ì§ì ‘ ëª¨ì‚¬í•¨ìœ¼ë¡œì¨ ì´ë¡ ê³¼ ì‹¤ìŠµì„ ëª¨ë‘ í•´ë³´ì•˜ë‹¤. 

í›„ì—ëŠ” ì‹¤ì œ ë³µì¡í•˜ê³  ë‹¤ì–‘í•œ ë°ì´í„°ì— ì ìš©ì‹œì¼œë³¼ ì˜ˆì •ì´ë‹¤. 

-----------------------------------------------------------------------

### â˜» Reference
1. [Medium: Introduction to Linear Regression in Python](https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0)
2. [Oxford Languages]
3. ë„¤ì´ë²„ êµ­ì–´/ì˜ì–´ì‚¬ì „
4. [wikipedia: Regression analysis](https://en.wikipedia.org/wiki/Regression_analysis)
5. [blog: ABB](https://soyounson.github.io/)
6. [Book: Mathematics for Machine Learning](https://mml-book.github.io/book/mml-book.pdf)
7. [kaggle : Advertising Dataset](https://www.kaggle.com/datasets/ashydv/advertising-dataset)
8. [statsmodels, API Reference](https://www.statsmodels.org/stable/api.html)

### â˜» image sources
1. [Giphy](https://giphy.com/search/sesame-street)


ğŸŒº **Thanks for reading. Hope to see you again :o)**

-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="150">
</p>

ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ ì§„í–‰í•˜ëŠ” "í•¨ê»˜ ì½˜í…ì¸ ë¥¼ ì œì‘í•˜ëŠ” ì½˜í…ì¸  í¬ë¦¬ì—ì´í„° ëª¨ì„"ì¸ **COCRE(ì½”í¬ë¦¬)** ì˜ 2ê¸° íšŒì›ìœ¼ë¡œ ì œì‘í•œ ê¸€ì…ë‹ˆë‹¤

[ğŸ˜ ì½”í¬ë¦¬ê°€ ê¶ê¸ˆí•˜ë‹¤ë©´ í´ë¦­!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)




