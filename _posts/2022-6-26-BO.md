---
layout: post
title: Bayesian Optimization (BO) <img src="/images/B-icon-ver.png" width="30">
---

### Bayesian Optimization (베이지안 최적화) 란?

베이지안 최적화. 어디서부터 시작했는지는 모르겠지만, Gaussian Process Regression을 하다가 어느 순간부터 Bayesian의 세계로 빨려 들어가고 있는 것 같다.   

<p align="center">
<img src="/images/elmo_sliding.gif" width="350">
</p>

요즘은 기존의 나의 공부 방식과는 다소 다르게 의식의 흐름에 따라 공부하고 있는 것 같다. 공부 잘하는 학생들이 핵심을 공부하는 것에 반해 나는 핵심만을 찾아가지는 못하지만, 그래도..뭐 나름 흥미를 느끼고 있다. 

🐭 시골쥐의 AI 도시 탐방기라고 여기고 힘차게 시작해보자.

<p align="center">
<img src="/images/Ratatouille_remy.gif" width="350">
</p>


본 글을 작성하기 위해서 다양한 곳에서 정보를 얻었지만, 전반적으로 A tutorial on bayesian Optimization (2018) [1] 이라는 논문을 기반으로 contents를 작성하고자 한다.                      

-----------------------------------------------------------------------

우선, Bayesian optimization에 대하여 wikipedia를 검색해보면 [2]

> Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.

좀 더 자세한 것을 확인하면 [3], 

> Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. It works by building a probabilistic model of the objective function, called the surrogate function, that is then searched efficiently with an acquisition function before candidate samples are chosen for evaluation on the real objective function.

정리하자면 베이지안 최적화란 목적함수 (objective function, f)를 최대화 혹은 최소화하는 최적해를 찾는 기법으로 베이지안 정리를 기반으로 하고 있다. 또한, 이는 **objective function**과 **acquisition function**이라는 두 개의 큰 요소로 구성되어 있다 [1].

> BayesOpt consists of two main components : a Bayesian statistical model for modeling the objective function, and an acquisition function for deciding where to sample next. 

그런데 일부 글에서는 베이지안의 최적화는 **surrogate model**과 **acquisition model**로 구성되어 있다고 하는데, 둘 다 맞는 말이라고 생각한다. 
이는 **surrogate model**이란 literally **대리(대용) 모델** 로 목적함수인 **objective function**을 추정하는 ML모델이며, 여기서는 **Gaussian Process Regression (GPR)** 을 사용하고 있다. 
따라서 **objective function + surrogate model + acquisition model*** 삼총사를 기억하길 바란다.

이외에 베이지안 최적화에 대하여 좀 더 자세한 내용을 이해하기에 앞서 몇가지 키워드를 기억하고 가면 좋을 것 같다. 

~~~~~~~~~~~~~~~~~~~~~
◦ Global optimization
◦ Objective function
◦ Surrogate model 
◦ Acquisition function 
◦ Gaussian Process Regression (GPR) 
◦ Black-box function 
◦ Bayesian Theore
~~~~~~~~~~~~~~~~~~~~~

-----------------------------------------------------------------------

☾ Table of contents

☺︎ Optimization?
  - Global optimization vs Local optimization 
  - Hyperparameter Optimization (HPO) 
 
☺︎ Bayesian Optimization (in short, BO, BayesOpt)    
  - Objective function, f    
  - Surrogate model 
  - Acquisition model 

☻ Reference

-----------------------------------------------------------------------


### ☺︎ Optimization          

Bayesian Optimization을 찾다보면, 심심치않게 glbal optimization과 hyperparameter optimization (HPO)이라는 말을 맞딱뜨리게 된다. 

우선, first things first!!!         

최적화라는 것은 무엇일까? wikipedia의 설명은 다음과 같다 [4,5]

> 최적화(最適化, 영어: mathematical optimization 또는 mathematical programming)는 특정의 집합 위에서 정의된 실수값, 함수, 정수에 대해 그 값이 최대나 최소가 되는 상태를 해석하는 문제이다. 수리 계획 또는 수리 계획 문제라고도 한다. 물리학이나 컴퓨터에서의 최적화 문제는 생각하고 있는 함수를 모델로 한 시스템의 에너지를 나타낸 것으로 여김으로써 에너지 최소화 문제라고도 부른다.

> In mathematics, computer science and economics, an optimization problem is the problem of finding the best solution from all feasible solutions.

뒤에서도 알아둬야 하겠지만, 여기서 짚고 넘어갈 문제는 **최적화**라는 것은 가장 적절한 값 (혹은 someth)을 구하는 것이다. 
여기서 exact solution이 아닌, the best solutoin 구하한다는 표현이 다소 불편할수도 있다. 여기서 불편하다는 표현은 이해가 되지 않는 다는 것이다. 

> 조금 다른 얘기지만, 아시는 분이 미국에서 박사 과정을 할때, 담당 교수님께서 항상 "Do you feel comfortable?" 라고 물어보셨다던데, 그런 맥락으로 생각 할 수 있다. 이해하지 못하면 마음이 굉장히 불편하다 (부르르르르르).

<p align="center">
<img src="/images/cant_bear.gif" width="350">
</p>


Anyhow, 최적화라는 말이 너무 general 할수도 있어서 쉽게 와닿지 않는다. 그래서, 우리가 관심있는 ML과 연관지어 생각해보면, **모델이 데이터를 잘 설명할수 있도록 모델 변수들에 관한 목적 함수를 최적화한다** 라고 볼 수 있다. 따라서 여기서 나오는 말이 Hyperparamter optimization (HPO) 이라고 볼수 있을 것 같다. 

> Many algorithms in machine learning optimize an objective function with respect to a set of desired model parameters that control how well a model
explains the data: Finding good parameters can be phrased as an optimization problem. Examples include: (i) linear regression, where we look at curve-fitting problems and optimize linear weight parameters to maximize the likelihood; (ii) neural-network auto-encoders for dimensionality reduction and data compression, where the parameters are the weights and biases of each layer, and where we minimize a reconstruction error by repeated application of
the chain rule; and (iii) Gaussian mixture models for modeling data distributions, where we optimize the location and shape parameters of each mixture component to maximize the likelihood of the model [6].

HPO에 대한 설명은 아래서 좀 더 자세히 다루도록 하자. 


#### ☻ Global optimization vs Local optimization 

위에서 Bayesian optimizaiton에 대해서 **> Bayesian optimization is a sequential design strategy for global optimization of black-box functions [2]** 이라 했는데, 여기서 Global optimization이라는 것이 무엇인지 알아보고자 한다. 나아가 Global 과 Local 최적화 사이의 차이도 확인하고자 한다. 

우선, Global optimization이란, 

> Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set [7]

최적화에서 찾고자 하는 가장 적절한 **최대 혹은 최소** 를 찾는데, global 값을 찾아가는 것이다. Local optimization과 비교해서 확인해보면 

>  Local optimization involves finding the optimal solution for a specific region of the search space, or the global optima for problems with no local optima. Global optimization involves finding the optimal solution on problems that contain local optima. How and when to use local and global search algorithms and how to use both methods in concert [8].

자, Global 과 Local 의 사전적 의미부터 확인하면 다음과 같다[9].

~~~~~~~~~~~~~~~~~~~~~
◦ Global
  1. 세계적인 
  2. 전반[전체/포괄]적인
◦ Local 
  1. 지역의, 현지의
  2. 일부의
~~~~~~~~~~~~~~~~~~~~~
 
간단한 schematic으로 확인하면 아래와 같다 (아래 schematic은 제가 작성한 것 입니다.).
 
<p align="center">
<img src="/images/Global_vs_Local.jpg" width="600">
</p>

**전반적인 혹은 전체적인** 함수나 값들에서 최적화된 혹은 적절한 혹은 best 최대 혹은 최소를 찾는지 (Global optimization) 아니면 **지역에서 혹은 일부의** 함수나 값들에서 최적화된 혹은 적절한 혹은 best 최대 혹은 최소를 찾는지 (Local optimization)에 따라 나눠진다고 볼 수 있다. 

우선은 이정도의 개념만 갖고 가보도록 하자. 

#### ☻ Hyperparameter Optimization (HPO)




Linearity 가 존재하지 않는다 

아무래도 ML/DL에서 optimization을 사용하는 경우는 학습수행하기 사전에 설정하는 hyperparameter의 최적값 탐색하는 문제를 나타냄
여러가지 방법론이 존재함 
~~~~~~~~~~~~~~~~~~~~~
◦ Manual search grid 
◦ Grid search 
◦ Random search 
◦ Bayesian optimiation 
◦ Evolution algorithm
~~~~~~~~~~~~~~~~~~~~~


이중 우리는 Bayesian optimization 에 대해서 확인하도록 한다. 




### ☺︎ Bayesian Optimization  

Bayesian Theorem 적용해서 f를 best하게 optimization함
중요한 부분은 
◦ f 는 Bayesian Theorem을 적용함 : Bayesian이 probability 이용하므로 자연히 probability따르는 maximum갑승ㄹ 구하게 됨 
◦ surrogate model : Gaussian process가 들어있는 bayes model 사용. 
-> 여기서 질문은 surrogate model로 왜 GPR (Gaussian Process Regression)을 사용했는가? 

- 어느 입력값 x를 받는 미지의 목적 함수를 상정함 
- 함수 f를 최대로 만드는 최적해 찾음 
- 목적 함수 : 탐색 대상 함수 


#### ☻ Objective function, f

cost function으로 쓰이기도 하는데 무엇을 의미하는지 확인이 필요함 

논문에 나와있는 objective function이 되기 위한 조건으로는 
~~~~~~~~~~~~~~~~~~~~~
◦ Continuous 
◦ GPR
◦ Black-box
◦ concavity, linearity x 
◦ derivative free 
~~~~~~~~~~~~~~~~~~~~~

#### ☻ Surrogate model 
surrogate의 사전적 의미는 대체, 대용물임 
: Gaussian process가 들어있는 bayes model 사용. 
-> 여기서 질문은 surrogate model로 왜 GPR (Gaussian Process Regression)을 사용했는가? 
- 목적함수를 추정하는 ML 모델 (주로 GPR) 
- 미지의 목적함수, f의 형태에 대한 확률적인 추정하는 모델 

#### ☻ Acquisition model 
- 다음 테스트 시 고려한 데이터 추천하는데 활용하는 함수, 탐색할 입력값 후보 추천 



~~~~~~~~~~~~~~~~
 
independent (predictors or factors) while Multivariate is used for the analysis with more than 
1 outcome variables (eg, repeated measures) and multiple independent variables.
~~~~~~~~~~~~~~~~


-----------------------------------------------------------------------

이런 글을 적는 것은 내가 배운 것을 공유한다는 관점에서는 좋은데, 가끔은 내가 완벽하게 이해하지 못해서 잘못된 정보를 전달하는 것이 아닌가해서 굉장히 조심스럽고 또한 큰 책임감을 크게 느낀다. 

틀린 부분이 있다면 추후 계속적으로 수정하도록 할 예정이다. 

조금씩 그리고 꾸준히 노력하고자 한다. 


<p align="center">
<img src="/images/elmo_workingout.gif" width="350">
</p>




### ☻ Reference
1. [Paper : A Tutorial on Bayesian Optimization](https://arxiv.org/abs/1807.02811)
2. [wikipedia : Bayesian Optimization](https://en.wikipedia.org/wiki/Bayesian_optimization)
3. [Machine Learning Mastery : How to Implement Bayesian Optimization from Scratch in Python](https://machinelearningmastery.com/what-is-bayesian-optimization/)
4. [wikipedia : 수학적 최적화](https://ko.wikipedia.org/wiki/%EC%88%98%ED%95%99%EC%A0%81_%EC%B5%9C%EC%A0%81%ED%99%94#:~:text=%EC%B5%9C%EC%A0%81%ED%99%94(%E6%9C%80%E9%81%A9%E5%8C%96%2C%20%EC%98%81%EC%96%B4%3A,%EC%88%98%EB%A6%AC%20%EA%B3%84%ED%9A%8D%20%EB%AC%B8%EC%A0%9C%EB%9D%BC%EA%B3%A0%EB%8F%84%20%ED%95%9C%EB%8B%A4.)
5. [wikipedia : Optimization problem](https://en.wikipedia.org/wiki/Optimization_problem)
6. [Book (개인 소장): Mathematics for Machine Learning by Book by A. Aldo Faisal, Cheng Soon Ong, and Marc Peter Deisenroth](https://www.amazon.com/Mathematics-Machine-Learning-Peter-Deisenroth/dp/110845514X)  
7. [wikipedia : Global optimization](https://en.wikipedia.org/wiki/Global_optimization)
8. [Machine Learning Mastery : Local Optimization versus Global Optimization](https://machinelearningmastery.com/local-optimization-versus-global-optimization/#:~:text=Local%20optimization%20involves%20finding%20the,problems%20that%20contain%20local%20optima)
9. 네이버 국어/영어사전








### ☻ image sources
1. [Giphy](https://giphy.com/search/sesame-street)


-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="150">
</p>

모두의 연구소에서 진행하는 "함께 콘텐츠를 제작하는 콘텐츠 크리에이터 모임"인 **COCRE(코크리)** 의 2기 회원으로 제작한 글입니다

[🐘 코크리가 궁금하다면 클릭!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)

