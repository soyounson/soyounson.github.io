---
layout: post
title: Bayesian Optimization (BO) <img src="/images/B-icon-ver.png" width="30">
---

### Bayesian Optimization (ë² ì´ì§€ì•ˆ ìµœì í™”) ë€?

ë² ì´ì§€ì•ˆ ìµœì í™”. ì–´ë””ì„œë¶€í„° ì‹œì‘í–ˆëŠ”ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ, Gaussian Process Regressionì„ í•˜ë‹¤ê°€ ì–´ëŠ ìˆœê°„ë¶€í„° Bayesianì˜ ì„¸ê³„ë¡œ ë¹¨ë ¤ ë“¤ì–´ê°€ê³  ìˆëŠ” ê²ƒ ê°™ë‹¤.   

<p align="center">
<img src="/images/elmo_sliding.gif" width="350">
</p>

ìš”ì¦˜ì€ ê¸°ì¡´ì˜ ë‚˜ì˜ ê³µë¶€ ë°©ì‹ê³¼ëŠ” ë‹¤ì†Œ ë‹¤ë¥´ê²Œ ì˜ì‹ì˜ íë¦„ì— ë”°ë¼ ê³µë¶€í•˜ê³  ìˆëŠ” ê²ƒ ê°™ë‹¤. ê³µë¶€ ì˜í•˜ëŠ” í•™ìƒë“¤ì´ í•µì‹¬ì„ ê³µë¶€í•˜ëŠ” ê²ƒì— ë°˜í•´ ë‚˜ëŠ” í•µì‹¬ë§Œì„ ì°¾ì•„ê°€ì§€ëŠ” ëª»í•˜ì§€ë§Œ, ê·¸ë˜ë„..ë­ ë‚˜ë¦„ í¥ë¯¸ë¥¼ ëŠë¼ê³  ìˆë‹¤. 

ğŸ­ ì‹œê³¨ì¥ì˜ AI ë„ì‹œ íƒë°©ê¸°ë¼ê³  ì—¬ê¸°ê³  í˜ì°¨ê²Œ ì‹œì‘í•´ë³´ì.

<p align="center">
<img src="/images/Ratatouille_remy.gif" width="350">
</p>


ë³¸ ê¸€ì„ ì‘ì„±í•˜ê¸° ìœ„í•´ì„œ ë‹¤ì–‘í•œ ê³³ì—ì„œ ì •ë³´ë¥¼ ì–»ì—ˆì§€ë§Œ, ì „ë°˜ì ìœ¼ë¡œ A tutorial on bayesian Optimization (2018) [1] ì´ë¼ëŠ” ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ contentsë¥¼ ì‘ì„±í•˜ê³ ì í•œë‹¤.                      

-----------------------------------------------------------------------

ìš°ì„ , Bayesian optimizationì— ëŒ€í•˜ì—¬ wikipediaë¥¼ ê²€ìƒ‰í•´ë³´ë©´ [2]

> Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.

ì¢€ ë” ìì„¸í•œ ê²ƒì„ í™•ì¸í•˜ë©´ [3], 

> Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. It works by building a probabilistic model of the objective function, called the surrogate function, that is then searched efficiently with an acquisition function before candidate samples are chosen for evaluation on the real objective function.

ì •ë¦¬í•˜ìë©´ ë² ì´ì§€ì•ˆ ìµœì í™”ë€ ëª©ì í•¨ìˆ˜ (objective function, f)ë¥¼ ìµœëŒ€í™” í˜¹ì€ ìµœì†Œí™”í•˜ëŠ” ìµœì í•´ë¥¼ ì°¾ëŠ” ê¸°ë²•ìœ¼ë¡œ ë² ì´ì§€ì•ˆ ì •ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆë‹¤. ë˜í•œ, ì´ëŠ” **objective function**ê³¼ **acquisition function**ì´ë¼ëŠ” ë‘ ê°œì˜ í° ìš”ì†Œë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ [1].

> BayesOpt consists of two main components : a Bayesian statistical model for modeling the objective function, and an acquisition function for deciding where to sample next. 

ê·¸ëŸ°ë° ì¼ë¶€ ê¸€ì—ì„œëŠ” ë² ì´ì§€ì•ˆì˜ ìµœì í™”ëŠ” **surrogate model**ê³¼ **acquisition model**ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  í•˜ëŠ”ë°, ë‘˜ ë‹¤ ë§ëŠ” ë§ì´ë¼ê³  ìƒê°í•œë‹¤. 
ì´ëŠ” **surrogate model**ì´ë€ literally **ëŒ€ë¦¬(ëŒ€ìš©) ëª¨ë¸** ë¡œ ëª©ì í•¨ìˆ˜ì¸ **objective function**ì„ ì¶”ì •í•˜ëŠ” MLëª¨ë¸ì´ë©°, ì—¬ê¸°ì„œëŠ” **Gaussian Process Regression (GPR)** ì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤. 
ë”°ë¼ì„œ **objective function + surrogate model + acquisition model*** ì‚¼ì´ì‚¬ë¥¼ ê¸°ì–µí•˜ê¸¸ ë°”ë€ë‹¤.

ì´ì™¸ì— ë² ì´ì§€ì•ˆ ìµœì í™”ì— ëŒ€í•˜ì—¬ ì¢€ ë” ìì„¸í•œ ë‚´ìš©ì„ ì´í•´í•˜ê¸°ì— ì•ì„œ ëª‡ê°€ì§€ í‚¤ì›Œë“œë¥¼ ê¸°ì–µí•˜ê³  ê°€ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. 

~~~~~~~~~~~~~~~~~~~~~
â—¦ Global optimization
â—¦ Objective function
â—¦ Surrogate model 
â—¦ Acquisition function 
â—¦ Gaussian Process Regression (GPR) 
â—¦ Black-box function 
â—¦ Bayesian Theore
~~~~~~~~~~~~~~~~~~~~~

-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ Optimization?
  - Global optimization vs Local optimization 
  - Hyperparameter Optimization (HPO) 
 
â˜ºï¸ Bayesian Optimization (in short, BO, BayesOpt)    
  - Objective function, f    
  - Surrogate model 
  - Acquisition model 

â˜» Reference

-----------------------------------------------------------------------


### â˜ºï¸ Optimization          

Bayesian Optimizationì„ ì°¾ë‹¤ë³´ë©´, ì‹¬ì‹¬ì¹˜ì•Šê²Œ glbal optimizationê³¼ hyperparameter optimization (HPO)ì´ë¼ëŠ” ë§ì„ ë§ë”±ëœ¨ë¦¬ê²Œ ëœë‹¤. 

ìš°ì„ , first things first!!!         

ìµœì í™”ë¼ëŠ” ê²ƒì€ ë¬´ì—‡ì¼ê¹Œ? wikipediaì˜ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤ [4,5]

> ìµœì í™”(æœ€é©åŒ–, ì˜ì–´: mathematical optimization ë˜ëŠ” mathematical programming)ëŠ” íŠ¹ì •ì˜ ì§‘í•© ìœ„ì—ì„œ ì •ì˜ëœ ì‹¤ìˆ˜ê°’, í•¨ìˆ˜, ì •ìˆ˜ì— ëŒ€í•´ ê·¸ ê°’ì´ ìµœëŒ€ë‚˜ ìµœì†Œê°€ ë˜ëŠ” ìƒíƒœë¥¼ í•´ì„í•˜ëŠ” ë¬¸ì œì´ë‹¤. ìˆ˜ë¦¬ ê³„íš ë˜ëŠ” ìˆ˜ë¦¬ ê³„íš ë¬¸ì œë¼ê³ ë„ í•œë‹¤. ë¬¼ë¦¬í•™ì´ë‚˜ ì»´í“¨í„°ì—ì„œì˜ ìµœì í™” ë¬¸ì œëŠ” ìƒê°í•˜ê³  ìˆëŠ” í•¨ìˆ˜ë¥¼ ëª¨ë¸ë¡œ í•œ ì‹œìŠ¤í…œì˜ ì—ë„ˆì§€ë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒìœ¼ë¡œ ì—¬ê¹€ìœ¼ë¡œì¨ ì—ë„ˆì§€ ìµœì†Œí™” ë¬¸ì œë¼ê³ ë„ ë¶€ë¥¸ë‹¤.

> In mathematics, computer science and economics, an optimization problem is the problem of finding the best solution from all feasible solutions.

ë’¤ì—ì„œë„ ì•Œì•„ë‘¬ì•¼ í•˜ê² ì§€ë§Œ, ì—¬ê¸°ì„œ ì§šê³  ë„˜ì–´ê°ˆ ë¬¸ì œëŠ” **ìµœì í™”**ë¼ëŠ” ê²ƒì€ ê°€ì¥ ì ì ˆí•œ ê°’ (í˜¹ì€ someth)ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. 
ì—¬ê¸°ì„œ exact solutionì´ ì•„ë‹Œ, the best solutoin êµ¬í•˜í•œë‹¤ëŠ” í‘œí˜„ì´ ë‹¤ì†Œ ë¶ˆí¸í• ìˆ˜ë„ ìˆë‹¤. ì—¬ê¸°ì„œ ë¶ˆí¸í•˜ë‹¤ëŠ” í‘œí˜„ì€ ì´í•´ê°€ ë˜ì§€ ì•ŠëŠ” ë‹¤ëŠ” ê²ƒì´ë‹¤. 

> ì¡°ê¸ˆ ë‹¤ë¥¸ ì–˜ê¸°ì§€ë§Œ, ì•„ì‹œëŠ” ë¶„ì´ ë¯¸êµ­ì—ì„œ ë°•ì‚¬ ê³¼ì •ì„ í• ë•Œ, ë‹´ë‹¹ êµìˆ˜ë‹˜ê»˜ì„œ í•­ìƒ "Do you feel comfortable?" ë¼ê³  ë¬¼ì–´ë³´ì…¨ë‹¤ë˜ë°, ê·¸ëŸ° ë§¥ë½ìœ¼ë¡œ ìƒê° í•  ìˆ˜ ìˆë‹¤. ì´í•´í•˜ì§€ ëª»í•˜ë©´ ë§ˆìŒì´ êµ‰ì¥íˆ ë¶ˆí¸í•˜ë‹¤ (ë¶€ë¥´ë¥´ë¥´ë¥´ë¥´).

<p align="center">
<img src="/images/cant_bear.gif" width="350">
</p>


Anyhow, ìµœì í™”ë¼ëŠ” ë§ì´ ë„ˆë¬´ general í• ìˆ˜ë„ ìˆì–´ì„œ ì‰½ê²Œ ì™€ë‹¿ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë˜ì„œ, ìš°ë¦¬ê°€ ê´€ì‹¬ìˆëŠ” MLê³¼ ì—°ê´€ì§€ì–´ ìƒê°í•´ë³´ë©´, **ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í• ìˆ˜ ìˆë„ë¡ ëª¨ë¸ ë³€ìˆ˜ë“¤ì— ê´€í•œ ëª©ì  í•¨ìˆ˜ë¥¼ ìµœì í™”í•œë‹¤** ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œ ë‚˜ì˜¤ëŠ” ë§ì´ Hyperparamter optimization (HPO) ì´ë¼ê³  ë³¼ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤. 

> Many algorithms in machine learning optimize an objective function with respect to a set of desired model parameters that control how well a model
explains the data: Finding good parameters can be phrased as an optimization problem. Examples include: (i) linear regression, where we look at curve-fitting problems and optimize linear weight parameters to maximize the likelihood; (ii) neural-network auto-encoders for dimensionality reduction and data compression, where the parameters are the weights and biases of each layer, and where we minimize a reconstruction error by repeated application of
the chain rule; and (iii) Gaussian mixture models for modeling data distributions, where we optimize the location and shape parameters of each mixture component to maximize the likelihood of the model [6].

HPOì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ì„œ ì¢€ ë” ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ì. 


#### â˜» Global optimization vs Local optimization 

ìœ„ì—ì„œ Bayesian optimizaitonì— ëŒ€í•´ì„œ **> Bayesian optimization is a sequential design strategy for global optimization of black-box functions [2]** ì´ë¼ í–ˆëŠ”ë°, ì—¬ê¸°ì„œ Global optimizationì´ë¼ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³ ì í•œë‹¤. ë‚˜ì•„ê°€ Global ê³¼ Local ìµœì í™” ì‚¬ì´ì˜ ì°¨ì´ë„ í™•ì¸í•˜ê³ ì í•œë‹¤. 

ìš°ì„ , Global optimizationì´ë€, 

> Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set [7]

ìµœì í™”ì—ì„œ ì°¾ê³ ì í•˜ëŠ” ê°€ì¥ ì ì ˆí•œ **ìµœëŒ€ í˜¹ì€ ìµœì†Œ** ë¥¼ ì°¾ëŠ”ë°, global ê°’ì„ ì°¾ì•„ê°€ëŠ” ê²ƒì´ë‹¤. Local optimizationê³¼ ë¹„êµí•´ì„œ í™•ì¸í•´ë³´ë©´ 

>  Local optimization involves finding the optimal solution for a specific region of the search space, or the global optima for problems with no local optima. Global optimization involves finding the optimal solution on problems that contain local optima. How and when to use local and global search algorithms and how to use both methods in concert [8].

ì, Global ê³¼ Local ì˜ ì‚¬ì „ì  ì˜ë¯¸ë¶€í„° í™•ì¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤[9].

~~~~~~~~~~~~~~~~~~~~~
â—¦ Global
  1. ì„¸ê³„ì ì¸ 
  2. ì „ë°˜[ì „ì²´/í¬ê´„]ì ì¸
â—¦ Local 
  1. ì§€ì—­ì˜, í˜„ì§€ì˜
  2. ì¼ë¶€ì˜
~~~~~~~~~~~~~~~~~~~~~
 
ê°„ë‹¨í•œ schematicìœ¼ë¡œ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤ (ì•„ë˜ schematicì€ ì œê°€ ì‘ì„±í•œ ê²ƒ ì…ë‹ˆë‹¤.).
 
<p align="center">
<img src="/images/Global_vs_Local.jpg" width="600">
</p>

**ì „ë°˜ì ì¸ í˜¹ì€ ì „ì²´ì ì¸** í•¨ìˆ˜ë‚˜ ê°’ë“¤ì—ì„œ ìµœì í™”ëœ í˜¹ì€ ì ì ˆí•œ í˜¹ì€ best ìµœëŒ€ í˜¹ì€ ìµœì†Œë¥¼ ì°¾ëŠ”ì§€ (Global optimization) ì•„ë‹ˆë©´ **ì§€ì—­ì—ì„œ í˜¹ì€ ì¼ë¶€ì˜** í•¨ìˆ˜ë‚˜ ê°’ë“¤ì—ì„œ ìµœì í™”ëœ í˜¹ì€ ì ì ˆí•œ í˜¹ì€ best ìµœëŒ€ í˜¹ì€ ìµœì†Œë¥¼ ì°¾ëŠ”ì§€ (Local optimization)ì— ë”°ë¼ ë‚˜ëˆ ì§„ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. 

ìš°ì„ ì€ ì´ì •ë„ì˜ ê°œë…ë§Œ ê°–ê³  ê°€ë³´ë„ë¡ í•˜ì. 

#### â˜» Hyperparameter Optimization (HPO)




Linearity ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ 

ì•„ë¬´ë˜ë„ ML/DLì—ì„œ optimizationì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ëŠ” í•™ìŠµìˆ˜í–‰í•˜ê¸° ì‚¬ì „ì— ì„¤ì •í•˜ëŠ” hyperparameterì˜ ìµœì ê°’ íƒìƒ‰í•˜ëŠ” ë¬¸ì œë¥¼ ë‚˜íƒ€ëƒ„
ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ë¡ ì´ ì¡´ì¬í•¨ 
~~~~~~~~~~~~~~~~~~~~~
â—¦ Manual search grid 
â—¦ Grid search 
â—¦ Random search 
â—¦ Bayesian optimiation 
â—¦ Evolution algorithm
~~~~~~~~~~~~~~~~~~~~~


ì´ì¤‘ ìš°ë¦¬ëŠ” Bayesian optimization ì— ëŒ€í•´ì„œ í™•ì¸í•˜ë„ë¡ í•œë‹¤. 




### â˜ºï¸ Bayesian Optimization  

Bayesian Theorem ì ìš©í•´ì„œ fë¥¼ bestí•˜ê²Œ optimizationí•¨
ì¤‘ìš”í•œ ë¶€ë¶„ì€ 
â—¦ f ëŠ” Bayesian Theoremì„ ì ìš©í•¨ : Bayesianì´ probability ì´ìš©í•˜ë¯€ë¡œ ìì—°íˆ probabilityë”°ë¥´ëŠ” maximumê°‘ìŠ¹ã„¹ êµ¬í•˜ê²Œ ë¨ 
â—¦ surrogate model : Gaussian processê°€ ë“¤ì–´ìˆëŠ” bayes model ì‚¬ìš©. 
-> ì—¬ê¸°ì„œ ì§ˆë¬¸ì€ surrogate modelë¡œ ì™œ GPR (Gaussian Process Regression)ì„ ì‚¬ìš©í–ˆëŠ”ê°€? 

- ì–´ëŠ ì…ë ¥ê°’ xë¥¼ ë°›ëŠ” ë¯¸ì§€ì˜ ëª©ì  í•¨ìˆ˜ë¥¼ ìƒì •í•¨ 
- í•¨ìˆ˜ fë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” ìµœì í•´ ì°¾ìŒ 
- ëª©ì  í•¨ìˆ˜ : íƒìƒ‰ ëŒ€ìƒ í•¨ìˆ˜ 


#### â˜» Objective function, f

cost functionìœ¼ë¡œ ì“°ì´ê¸°ë„ í•˜ëŠ”ë° ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ í™•ì¸ì´ í•„ìš”í•¨ 

ë…¼ë¬¸ì— ë‚˜ì™€ìˆëŠ” objective functionì´ ë˜ê¸° ìœ„í•œ ì¡°ê±´ìœ¼ë¡œëŠ” 
~~~~~~~~~~~~~~~~~~~~~
â—¦ Continuous 
â—¦ GPR
â—¦ Black-box
â—¦ concavity, linearity x 
â—¦ derivative free 
~~~~~~~~~~~~~~~~~~~~~

#### â˜» Surrogate model 
surrogateì˜ ì‚¬ì „ì  ì˜ë¯¸ëŠ” ëŒ€ì²´, ëŒ€ìš©ë¬¼ì„ 
: Gaussian processê°€ ë“¤ì–´ìˆëŠ” bayes model ì‚¬ìš©. 
-> ì—¬ê¸°ì„œ ì§ˆë¬¸ì€ surrogate modelë¡œ ì™œ GPR (Gaussian Process Regression)ì„ ì‚¬ìš©í–ˆëŠ”ê°€? 
- ëª©ì í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ML ëª¨ë¸ (ì£¼ë¡œ GPR) 
- ë¯¸ì§€ì˜ ëª©ì í•¨ìˆ˜, fì˜ í˜•íƒœì— ëŒ€í•œ í™•ë¥ ì ì¸ ì¶”ì •í•˜ëŠ” ëª¨ë¸ 

#### â˜» Acquisition model 
- ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì‹œ ê³ ë ¤í•œ ë°ì´í„° ì¶”ì²œí•˜ëŠ”ë° í™œìš©í•˜ëŠ” í•¨ìˆ˜, íƒìƒ‰í•  ì…ë ¥ê°’ í›„ë³´ ì¶”ì²œ 



~~~~~~~~~~~~~~~~
 
independent (predictors or factors) while Multivariate is used for the analysis with more than 
1 outcome variables (eg, repeated measures) and multiple independent variables.
~~~~~~~~~~~~~~~~


-----------------------------------------------------------------------

ì´ëŸ° ê¸€ì„ ì ëŠ” ê²ƒì€ ë‚´ê°€ ë°°ìš´ ê²ƒì„ ê³µìœ í•œë‹¤ëŠ” ê´€ì ì—ì„œëŠ” ì¢‹ì€ë°, ê°€ë”ì€ ë‚´ê°€ ì™„ë²½í•˜ê²Œ ì´í•´í•˜ì§€ ëª»í•´ì„œ ì˜ëª»ëœ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì•„ë‹Œê°€í•´ì„œ êµ‰ì¥íˆ ì¡°ì‹¬ìŠ¤ëŸ½ê³  ë˜í•œ í° ì±…ì„ê°ì„ í¬ê²Œ ëŠë‚€ë‹¤. 

í‹€ë¦° ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì¶”í›„ ê³„ì†ì ìœ¼ë¡œ ìˆ˜ì •í•˜ë„ë¡ í•  ì˜ˆì •ì´ë‹¤. 

ì¡°ê¸ˆì”© ê·¸ë¦¬ê³  ê¾¸ì¤€íˆ ë…¸ë ¥í•˜ê³ ì í•œë‹¤. 


<p align="center">
<img src="/images/elmo_workingout.gif" width="350">
</p>




### â˜» Reference
1. [Paper : A Tutorial on Bayesian Optimization](https://arxiv.org/abs/1807.02811)
2. [wikipedia : Bayesian Optimization](https://en.wikipedia.org/wiki/Bayesian_optimization)
3. [Machine Learning Mastery : How to Implement Bayesian Optimization from Scratch in Python](https://machinelearningmastery.com/what-is-bayesian-optimization/)
4. [wikipedia : ìˆ˜í•™ì  ìµœì í™”](https://ko.wikipedia.org/wiki/%EC%88%98%ED%95%99%EC%A0%81_%EC%B5%9C%EC%A0%81%ED%99%94#:~:text=%EC%B5%9C%EC%A0%81%ED%99%94(%E6%9C%80%E9%81%A9%E5%8C%96%2C%20%EC%98%81%EC%96%B4%3A,%EC%88%98%EB%A6%AC%20%EA%B3%84%ED%9A%8D%20%EB%AC%B8%EC%A0%9C%EB%9D%BC%EA%B3%A0%EB%8F%84%20%ED%95%9C%EB%8B%A4.)
5. [wikipedia : Optimization problem](https://en.wikipedia.org/wiki/Optimization_problem)
6. [Book (ê°œì¸ ì†Œì¥): Mathematics for Machine Learning by Book by A. Aldo Faisal, Cheng Soon Ong, and Marc Peter Deisenroth](https://www.amazon.com/Mathematics-Machine-Learning-Peter-Deisenroth/dp/110845514X)  
7. [wikipedia : Global optimization](https://en.wikipedia.org/wiki/Global_optimization)
8. [Machine Learning Mastery : Local Optimization versus Global Optimization](https://machinelearningmastery.com/local-optimization-versus-global-optimization/#:~:text=Local%20optimization%20involves%20finding%20the,problems%20that%20contain%20local%20optima)
9. ë„¤ì´ë²„ êµ­ì–´/ì˜ì–´ì‚¬ì „








### â˜» image sources
1. [Giphy](https://giphy.com/search/sesame-street)


-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="150">
</p>

ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ ì§„í–‰í•˜ëŠ” "í•¨ê»˜ ì½˜í…ì¸ ë¥¼ ì œì‘í•˜ëŠ” ì½˜í…ì¸  í¬ë¦¬ì—ì´í„° ëª¨ì„"ì¸ **COCRE(ì½”í¬ë¦¬)** ì˜ 2ê¸° íšŒì›ìœ¼ë¡œ ì œì‘í•œ ê¸€ì…ë‹ˆë‹¤

[ğŸ˜ ì½”í¬ë¦¬ê°€ ê¶ê¸ˆí•˜ë‹¤ë©´ í´ë¦­!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)

