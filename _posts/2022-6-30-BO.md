---
layout: post
title: Bayesian Optimization (BO) <img src="/images/B-icon-ver.png" width="30">
---

### Bayesian Optimization (ë² ì´ì§€ì•ˆ ìµœì í™”) ë€?


ë² ì´ì§€ì•ˆ ìµœì í™”. 

ì–´ë””ì„œë¶€í„° ì‹œì‘í–ˆëŠ”ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ, Gaussian Process Regression (GPR) ì„ í•˜ë‹¤ê°€ ì–´ëŠ ìˆœê°„ë¶€í„° Bayesianì˜ ì„¸ê³„ë¡œ ë¹¨ë ¤ ë“¤ì–´ê°€ê³  ìˆëŠ” ê²ƒ ê°™ë‹¤.   


<p align="center">
<img src="/images/elmo_sliding.gif" width="350">
</p>


ìš”ì¦˜ì€ ì•„ë¬´ ìƒê°ì—†ì´ ì˜ì‹ì˜ íë¦„ì— ë”°ë¼ ê³µë¶€í•˜ê³  ìˆëŠ” ê²ƒ ê°™ì§€ë§Œ, ë‚˜ë¦„ í¥ë¯¸ë¥¼ ëŠë¼ê³  ìˆë‹¤.  
ë„ì‹œì— ì²˜ìŒ ì˜¬ë¼ì™€ ëª¨ë“  ê²ƒì´ ì‹ ê¸°í•œ ğŸ­ ì‹œê³¨ ì¥ì˜ AI ë„ì‹œ íƒë°©ê¸°ë¼ê³  ì—¬ê¸°ê³  í˜ì°¨ê²Œ ì‹œì‘í•´ë³´ì.

<p align="center">
<img src="/images/Ratatouille_remy.gif" width="350">
</p>



ë³¸ ê¸€ì€ A tutorial on bayesian Optimization (2018) [1] ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‚´ìš©ì„ ë”í•´ë‚˜ê°€ëŠ” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ë‹¤.             



-----------------------------------------------------------------------


ìš°ì„ , Bayesian optimizationì— ëŒ€í•˜ì—¬ wikipediaë¥¼ ê²€ìƒ‰í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤ [2].


> Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.


ì´í•´í•˜ê¸°ê°€ ì–´ë ¤ì›Œ, ì¢€ ë” ì‰½ê²Œ í’€ì´í•œ ë¶€ë¶„ì„ ì°¾ì•„ë³´ì•˜ë‹¤ [3]. 


> Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. It works by **building a probabilistic model of the objective function, called the surrogate function, that is then searched efficiently with an acquisition function before candidate samples are chosen for evaluation on the real objective function.**


ì •ë¦¬í•˜ìë©´ ë² ì´ì§€ì•ˆ ìµœì í™”ë€ ëª©ì í•¨ìˆ˜ (objective function, f)ë¥¼ ìµœëŒ€í™” í˜¹ì€ ìµœì†Œí™”í•˜ëŠ” ìµœì í•´ë¥¼ ì°¾ëŠ” ê¸°ë²•ìœ¼ë¡œ ë² ì´ì§€ì•ˆ ì •ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆë‹¤. ë˜í•œ, ì´ëŠ” **objective function**ê³¼ **acquisition function**ì´ë¼ëŠ” ë‘ ê°œì˜ í° ìš”ì†Œë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ [1].


> BayesOpt consists of two main components : a Bayesian statistical model for modeling the objective function, and an acquisition function for deciding where to sample next. 


ê·¸ëŸ°ë°, ì—¬ê¸°ì„œ í™•ì¸í•˜ê³  ê°€ì•¼ í•  ë¶€ë¶„ì´ ìˆë‹¤. ì¼ë¶€ ê¸€ì—ì„œëŠ” ë² ì´ì§€ì•ˆì˜ ìµœì í™”ëŠ” **surrogate model**ê³¼ **acquisition function**ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³ ë„ í•˜ëŠ”ë°, ì‚¬ì‹¤ ë‘˜ ë‹¤ ë§ëŠ” ë§ì´ë¼ê³  ìƒê°í•œë‹¤. **surrogate model**ì´ë€ literally **ëŒ€ë¦¬(ëŒ€ìš©) ëª¨ë¸** ë¡œ ëª©ì í•¨ìˆ˜ì¸ **objective function**ì„ ì¶”ì •í•˜ëŠ” MLëª¨ë¸ì´ë©°, **Gaussian Process Regression (GPR)** ì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤. ë”°ë¼ì„œ **objective function + surrogate model + acquisition function** ì´ ì‚¼ì´ì‚¬ê°€ ë² ì´ì§€ì•ˆ ìµœì í™”ì˜ ì£¼ìš” êµ¬ì„±ìš”ì†Œë“¤ì´ë¼ê³  ê¸°ì–µí•˜ê¸¸ ë°”ë€ë‹¤.

ì´ì™¸ì— ë² ì´ì§€ì•ˆ ìµœì í™”ì— ëŒ€í•˜ì—¬ ì¢€ ë” ìì„¸í•œ ë‚´ìš©ì„ ì´í•´í•˜ê¸°ì— ì•ì„œ ëª‡ê°€ì§€ í‚¤ì›Œë“œë¥¼ ê¸°ì–µí•˜ê³  ê°€ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. 

~~~~~~~~~~~~~~~~~~~~~
â—¦ Global optimization
â—¦ Objective function
â—¦ Surrogate model 
â—¦ Acquisition function 
â—¦ Gaussian Process Regression (GPR) 
â—¦ Black-box function 
â—¦ Bayesian Theore
~~~~~~~~~~~~~~~~~~~~~

-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ Optimization?
  - Global optimization vs Local optimization 
  - Hyperparameter Optimization (HPO) 
 
â˜ºï¸ Bayesian Optimization (in short, BO or BayesOpt)    
  - Objective function, f    
  - Surrogate model 
  - Acquisition model 

â˜» Reference

-----------------------------------------------------------------------


### â˜ºï¸ Optimization          

ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ì°¾ì•„ë³´ë©´, ì‹¬ì‹¬ì¹˜ì•Šê²Œ glbal optimizationê³¼ hyperparameter optimization (HPO)ì´ë¼ëŠ” ë§ì„ ë§ë”±ëœ¨ë¦¬ê²Œ ëœë‹¤. ë­ì§€? 

ìš°ì„ , first things firstìœ¼ë¡œ, ì¤‘ë³µë˜ì–´ ìˆëŠ” ë‹¨ì–´ì¸ optimizationë¶€í„° í™•ì¸í•´ë³´ì.          

ìµœì í™” (optimization)ëŠ” ë¬´ì—‡ì¼ê¹Œ? wikipediaì˜ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤ [4,5]


> ìµœì í™”(æœ€é©åŒ–, ì˜ì–´: mathematical optimization ë˜ëŠ” mathematical programming)ëŠ” íŠ¹ì •ì˜ ì§‘í•© ìœ„ì—ì„œ ì •ì˜ëœ ì‹¤ìˆ˜ê°’, í•¨ìˆ˜, ì •ìˆ˜ì— ëŒ€í•´ ê·¸ ê°’ì´ ìµœëŒ€ë‚˜ ìµœì†Œê°€ ë˜ëŠ” ìƒíƒœë¥¼ í•´ì„í•˜ëŠ” ë¬¸ì œì´ë‹¤. ìˆ˜ë¦¬ ê³„íš ë˜ëŠ” ìˆ˜ë¦¬ ê³„íš ë¬¸ì œë¼ê³ ë„ í•œë‹¤. ë¬¼ë¦¬í•™ì´ë‚˜ ì»´í“¨í„°ì—ì„œì˜ ìµœì í™” ë¬¸ì œëŠ” ìƒê°í•˜ê³  ìˆëŠ” í•¨ìˆ˜ë¥¼ ëª¨ë¸ë¡œ í•œ ì‹œìŠ¤í…œì˜ ì—ë„ˆì§€ë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒìœ¼ë¡œ ì—¬ê¹€ìœ¼ë¡œì¨ ì—ë„ˆì§€ ìµœì†Œí™” ë¬¸ì œë¼ê³ ë„ ë¶€ë¥¸ë‹¤.



> In mathematics, computer science and economics, an optimization problem is the problem of finding the best solution from all feasible solutions.


ì¦‰, **ìµœì í™”**ë¼ëŠ” ê²ƒì€ ê°€ì¥ ì ì ˆí•œ ê°’ (í˜¹ì€ someth)ì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ì˜ë¯¸í•˜ëŠ” ì ì ˆí•œ ê°’ì´ë€ the best solutoin êµ¬í•˜ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. Best solution? ì´ê²Œ ë¬´ìŠ¨ ë§ì´ì§€? ì´ëŸ° ì •í™•í•˜ì§€ ëª»í•œ í‘œí˜„ì€ êµ‰ì¥íˆ ë¶ˆí¸í•˜ë‹¤. ì¦‰, ì´í•´ê°€ ì˜ ì•ˆë˜ì„œ ë§ˆìŒì´ í¸ì¹˜ì•Šë‹¤. 


> ì¡°ê¸ˆ ë‹¤ë¥¸ ì–˜ê¸°ì§€ë§Œ, ì•„ì‹œëŠ” ë¶„ê»˜ì„œ ê°‘ìê¸° ML/DLì´ í¸í•˜ëƒê³  ë¬¼ì–´ë³´ì…¨ë‹¤. í¸í•˜ëŠ” ê²ƒì´ ë¬´ìŠ¨ ì˜ë¯¸ì¸ì§€ ì—¬ì­¤ë³´ë‹ˆ, ê·¸ ë¶„ì´ ë¯¸êµ­ì—ì„œ ê³µë¶€í•˜ì…¨ì„ë•Œ ë‹´ë‹¹ êµìˆ˜ë‹˜ê»˜ì„œ í•­ìƒ "Do you feel comfortable?" ë¼ê³  ë¬¼ì–´ë³´ì…¨ë‹¤í•œë‹¤. ë§ë‹¤! ê·¸ëŸ° ë§¥ë½ìœ¼ë¡œ ìƒê° í•  ìˆ˜ ìˆë‹¤. ì‚¬ì‹¤ ì´í•´í•˜ì§€ ëª»í•˜ë©´ ë§ˆìŒì´ êµ‰ì¥íˆ ë¶ˆí¸í•˜ë‹¤. ë…¼ë¬¸ì„ ì½ë‹¤ê°€ ì´í•´ê°€ ì•ˆë˜ëŠ”ë° ë’·ì¥ì„ ë„˜ê¸¸ë•ŒëŠ” ë­”ê°€ ì°ì°í•˜ê³  ë‹µë‹µí•˜ê³  ë¶ˆí¸í•œ ê°ì •ì´ ìƒê¸´ë‹¤ (ë¶€ë¥´ë¥´ë¥´ë¥´~). ì•—! ë‚˜ë§Œ ê·¸ëŸ´ìˆ˜ë„ ìˆë‹¤ ğŸ¤”


<p align="center">
<img src="/images/cant_bear.gif" width="350">
</p>


Anyhow, ìµœì í™”ë¼ëŠ” ë§ì´ ë„ˆë¬´ general í• ìˆ˜ë„ ìˆì–´ì„œ ì‰½ê²Œ ì™€ë‹¿ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë˜ì„œ, ìš°ë¦¬ê°€ ê´€ì‹¬ìˆëŠ” MLê³¼ ì—°ê´€ì§€ì–´ ìƒê°í•˜ë©´, **ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í• ìˆ˜ ìˆë„ë¡ ëª¨ë¸ ë³€ìˆ˜ë“¤ì— ê´€í•œ ëª©ì  í•¨ìˆ˜ë¥¼ ìµœì í™”í•œë‹¤** ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œ ë‚˜ì˜¤ëŠ” ë§ì´ ë§¤ê°œë³€ìˆ˜ë“¤ì˜ ìµœì í™”ì¸ Hyperparamter optimization (HPO) ì´ë¼ê³  ë³¼ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤. 


> Many algorithms in machine learning optimize an objective function with respect to a set of desired model parameters that control how well a model
explains the data: Finding good parameters can be phrased as an optimization problem. Examples include: (i) linear regression, where we look at curve-fitting problems and optimize linear weight parameters to maximize the likelihood; (ii) neural-network auto-encoders for dimensionality reduction and data compression, where the parameters are the weights and biases of each layer, and where we minimize a reconstruction error by repeated application of the chain rule; and (iii) Gaussian mixture models for modeling data distributions, where we optimize the location and shape parameters of each mixture component to maximize the likelihood of the model [6].


HPOì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ì„œ ì¢€ ë” ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ì. 



#### â˜» Global optimization vs Local optimization 

ìœ„ì—ì„œ Bayesian optimizaitonì— ëŒ€í•´ì„œ **Bayesian optimization is a sequential design strategy for global optimization of black-box functions [2]** ì´ë¼ í–ˆëŠ”ë°, ì—¬ê¸°ì„œ Global optimizationì´ë¼ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³ ì í•œë‹¤. ë‚˜ì•„ê°€ Global ê³¼ Local ìµœì í™” ì‚¬ì´ì˜ ì°¨ì´ë„ í™•ì¸í•˜ê³ ì í•œë‹¤. 

ìš°ì„ , Global optimizationì´ë€, 


> Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set [7]


ìµœì í™”ì—ì„œ ì°¾ê³ ì í•˜ëŠ” ê°€ì¥ ì ì ˆí•œ **ìµœëŒ€ í˜¹ì€ ìµœì†Œ** ë¥¼ ì°¾ëŠ”ë°, global ê°’ì„ ì°¾ì•„ê°€ëŠ” ê²ƒì´ë‹¤. Local optimizationê³¼ ë¹„êµí•´ì„œ í™•ì¸í•´ë³´ë©´ 


>  Local optimization involves finding the optimal solution for a specific region of the search space, or the global optima for problems with no local optima. Global optimization involves finding the optimal solution on problems that contain local optima. How and when to use local and global search algorithms and how to use both methods in concert [8].

ì, Global ê³¼ Local ì˜ ì‚¬ì „ì  ì˜ë¯¸ë¶€í„° í™•ì¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤ [9].

~~~~~~~~~~~~~~~~~~~~~
â—¦ Global
  1. ì„¸ê³„ì ì¸ 
  2. ì „ë°˜[ì „ì²´/í¬ê´„]ì ì¸
â—¦ Local 
  1. ì§€ì—­ì˜, í˜„ì§€ì˜
  2. ì¼ë¶€ì˜
~~~~~~~~~~~~~~~~~~~~~
 
ê°„ë‹¨í•œ schematicìœ¼ë¡œ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤ (ì•„ë˜ schematicì€ ì œê°€ ì‘ì„±í•œ ê²ƒ ì…ë‹ˆë‹¤.).
 
<p align="center">
<img src="/images/Global_vs_Local.jpg" width="600">
</p>

**ì „ë°˜ì ì¸ í˜¹ì€ ì „ì²´ì ì¸** í•¨ìˆ˜ë‚˜ ê°’ë“¤ì—ì„œ ìµœì í™”ëœ í˜¹ì€ ì ì ˆí•œ í˜¹ì€ best ìµœëŒ€ í˜¹ì€ ìµœì†Œë¥¼ ì°¾ëŠ”ì§€ (Global optimization) ì•„ë‹ˆë©´ **ì§€ì—­ì—ì„œ í˜¹ì€ ì¼ë¶€ì˜** í•¨ìˆ˜ë‚˜ ê°’ë“¤ì—ì„œ ìµœì í™”ëœ í˜¹ì€ ì ì ˆí•œ í˜¹ì€ best ìµœëŒ€ í˜¹ì€ ìµœì†Œë¥¼ ì°¾ëŠ”ì§€ (Local optimization)ì— ë”°ë¼ ë‚˜ëˆ ì§„ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. 

ìš°ì„ ì€ ì´ì •ë„ì˜ ê°œë…ë§Œ ê°–ê³  ê°€ë³´ë„ë¡ í•˜ì. 


#### â˜» Hyperparameter Optimization (HPO)

ML/DLì—ì„œ ì‚¬ìš©ë˜ëŠ” optimizationì€ **í•™ìŠµì„ ìˆ˜í–‰í•˜ê¸° ì‚¬ì „ì— ì„¤ì •í•˜ëŠ” hyperparameterì˜ ìµœì ê°’ íƒìƒ‰í•˜ëŠ” ë¬¸ì œ** ì— ì‚¬ìš©ëœë‹¤. 

ìš°ì„ , Hyperparameterì— ëŒ€í•´ ì•Œì•„ë³´ìë©´, ì´ëŠ” learning ì‹œì‘í•˜ê¸° ì „ì— ë¯¸ë¦¬ ì •í•˜ëŠ” ê°’ì´ë‹¤. ëª¨ë¸ë¡œ learningí•˜ëŠ” ê°’ì´ ì•„ë‹ˆë¼ trainingì „ ì‚¬ì „ì— ì„¤ì •í•´ì„œ ì£¼ì–´ì§€ëŠ” ê°’ìœ¼ë¡œ ìµœì ì˜ ëª¨ë¸ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ tuningì„ í•´ì¤„ í•„ìš”ê°€ ìˆë‹¤. ë”°ë¼ì„œ, Hyperparamter tuningì´ë¼ëŠ” í•˜ëŠ” ê²ƒì´ë‹¤.


> Hyperparameters refer to the parameters that the model cannot learn and need to be provided before training. Hyperparameter tuning basically refers to tweaking the parameters of the model, which is basically a lengthy process [10].


ì—¬ê¸°ì„œ, ì¢€ ë” deep diveí•˜ê²Œ í™•ì¸í•´ì„œ, paramterì•ì— Hyper-ë¼ëŠ” ë§ì„ ë¶™ì€ ì´ìœ ì— ëŒ€í•´ì„œ ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ìë©´, 


> Hyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. The prefix â€˜hyper_â€™ suggests that they are â€˜top-levelâ€™ parameters that control the learning process and the model parameters that result from it. As a machine learning engineer designing a model, you choose and set hyperparameter values that your learning algorithm will use before the training of the model even begins. In this light, hyperparameters are said to be external to the model because the model cannot change its values during learning/training [11].


> hyperparameters that the learning algorithm will use to learn the optimal parameters that correctly map the input features (independent variables) to the labels or targets (dependent variable) such that you achieve some form of intelligence[11].


ì¦‰, learning proecssì™€ ëª¨ë¸ì˜ ë³€ìˆ˜ë“¤ì„ ì œì–´í•˜ëŠ” top-levelì˜ ë³€ìˆ˜ë¼ì„œ hyper-ê°€ ë¶™ëŠ” ê²ƒì´ë‹¤. Hyper-ì˜ ì‚¬ì „ì  ì˜ë¯¸ë¥¼ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ê³ , top-levelì´ë¼ëŠ” ë§ê³¼ ìƒí†µí•œë‹¤ [12].

~~~~~~~~~~~~~~~~~~~~~
â—¦ hyper
  1. above, beyond, SUPER-
  2. excessively, excessive
~~~~~~~~~~~~~~~~~~~~~

ë”°ë¼ì„œ ìµœì ì˜ hyperparamterë¥¼ ì œê³µí•´ì£¼ëŠ” ê²ƒì´ ìµœì ì˜ modelì„ êµ¬í˜„í•˜ëŠ”ë° ì¤‘ìš”í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. 

ì´ëŸ¬í•œ HPOì—ëŠ” ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤ [10]. 

~~~~~~~~~~~~~~~~~~~~~
â—¦ Manual search grid 
â—¦ Grid search 
â—¦ Random search (Randomized Search)
â—¦ HyperOpt-Sklearn
â—¦ Bayesian optimiation 
â—¦ Evolution algorithm
~~~~~~~~~~~~~~~~~~~~~

ê°ê°ì˜ ë°©ë²•ë“¤ì— ëŒ€í•œ ì„¤ëª…ì€ out of our scopeì´ë¼ì„œ ë” ì´ìƒ ë‹¤ë£¨ì§€ ì•Šê³ , ë°”ë¡œ Bayesian optimizationìœ¼ë¡œ ë„˜ì–´ê°€ë„ë¡ í•œë‹¤. 


### â˜ºï¸ Bayesian Optimization  

ë² ì´ì§€ì•ˆ ìµœì í™”ë€ ë² ì´ì§€ì•ˆ ì •ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, **objective function (ëª©ì í•¨ìˆ˜)** ë¥¼ ì¶”ì •í•˜ëŠ” í™•ë¥  ê¸°ë°˜ì˜ **surrogate model**ê³¼ **acquisition function**ì´ë¼ëŠ” ë‘ ê°œì˜ í° ìš”ì†Œë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ [1].

> BayesOpt consists of two main components : a Bayesian statistical model for modeling the objective function, and an acquisition function for deciding where to sample next. 

ìµœì í™”í•˜ëŠ” ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ê³  [13].

> Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines the next query point.

Pseudo-codeë¡œ ë³¸ë‹¤ë©´ ì•„ë˜ì™€ ê°™ë‹¤ (ì•„ë˜ Pseudo-codeëŠ” ë…¼ë¬¸ [14]ì—ì„œ ê°–ê³ ì™€ì„œ ì¸ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.).

<p align="center">
<img src="/images/Pseudo_code_BO.png" width="450">
</p>
Fig. Pseudo-code of BO [14]

ì½”ë“œì—ì„œ 5ë²ˆì¸ statistical modelì€ surrogate model, ì¦‰ GPRì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. 
í”„ë¡œì„¸ìŠ¤ëŠ” surrogate modelì—ì„œ GPRì„ í†µí•´ Bayesian posterial probability distributionì„ êµ¬í•˜ê³  (mean, ğê³¼ variance, ğˆ^2), ê·¸ ê°’ì„ acquisition functionì— ì ìš©í•´ì„œ next query p'të¥¼ ì°¾ì•„ëƒ„ì— ë”°ë¼ data augmentationì„ í•˜ê³  ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ ìµœì ì˜ objective function, f ì„ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì´ Bayesian Optimizationì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. 

ì´ëŸ¬í•œ ê³¼ì •ì„ schematicì„ í†µí•´ ì¢€ ë” ì§ê´€ì ìœ¼ë¡œ ì´í•´í•´ë³´ì(ì•„ë˜ Schematic of BOëŠ” ë…¼ë¬¸[14]ì˜ Fig.1ì´ë©°, BOì˜ ì„¤ëª…ì„ ìœ„í•´ì„œ ê°–ê³ ì™€ì„œ ì¸ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.).

<p align="center">
<img src="/images/Schematic_BO.png" width="600">
</p>
Fig. Schematic of BO [14]


ì´í•´ë¥¼ ë•ê¸°ìœ„í•´ ì„¤ëª…ì„ ì¶”ê°€í•˜ë©´ (ì•„ë˜ schematicì€ ìœ„ì˜ ê·¸ë¦¼ ì¼ë¶€ì— ë‚´ìš©ì„ ì¶”ê°€í•´ì„œ ì‘ì„±í•œ ê²ƒ ì…ë‹ˆë‹¤.)


<p align="center">
<img src="/images/Schematic_BO_by_SS.jpg" width="750">
</p>
Fig. Schematic of BO w/ comments by SS


GPRì„ í†µí•´ mean, ğê³¼ variance, ğˆ^2ë¥¼ ê°–ëŠ” Bayesian posterial probability distributionì„ êµ¬í•˜ê²Œ ë˜ëŠ”ë°, ê·¸ë¦¼ì—ì„œ solid lineì€ mean ê°’ì„ ë‚˜íƒ€ë‚´ê³ , ë³´ë¼ìƒ‰ shaded regionì€ Bayesian credible intervalë¡œ 

![image](https://user-images.githubusercontent.com/40614421/176131253-5c3ed220-d9d7-4861-b3c5-bb76e78fd96c.png)

ì´ë‹¤. ì´ëŠ” frequentist statisticsì—ì„œ confidence interval ê°™ì€ ì—­í• ì„ í•˜ë©° posterial probabilityì— ë”°ë¼ í™•ë¥ ì´ 95%(ì´ëŠ” 2ğˆì— í•´ë‹¹í•¨)ì¸ f(x)ë¥¼ í¬í•¨í•œë‹¤ [15]. ì´ shaded regionì€ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” observed dataì—ì„œëŠ” 0ê°’ì„ ê°–ì§€ë§Œ, ìš°ë¦¬ê°€ ëª¨ë¥´ëŠ” ê°’ë“¤ì˜ ê²½ìš° ê°’ì„ ê°–ëŠ”ë‹¤. ê·¸ë¦¬ê³  observed dataì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ê·¸ ê°’ì´ ì»¤ì§„ë‹¤. 

ê·¸ë¦¬ê³  ê·¸ë¦¼ì˜ ì•„ë˜ green shaded regionì€ acquisition function ê°’ì„ ì˜ë¯¸í•˜ëŠ”ë°, ì´ ê²ƒì´ maximied valueì¸ ê²ƒì´ next query p'tê°€ ëœë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ â˜» Acquisition functionì—ì„œ ìì„¸íˆ ë‹¤ë£¨ë„ë¡ í•˜ì. 


#### â˜» Objective function, f

objective functionì€ ëª©ì í•¨ìˆ˜ë¡œ ë² ì´ì§€ì•ˆ ìµœì í™”ì—ì„œëŠ” ì´ í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì´ëŸ¬í•œ  objective functionì˜ íŠ¹ì§•ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤[1,13].  

~~~~~~~~~~~~~~~~~~~~~
â—¦ Continuous 
â—¦ Expensive to evaluate 
â—¦ Black-box (unknown struction) 
â—¦ derivative free : prevents the application of first- and  second-order methods 
â—¦ noise-free : be observed w/o noise
â—¦ nonlinear 
â—¦ non-convex function
â—¦ global optimum
~~~~~~~~~~~~~~~~~~~~~

ê·¸ë˜ì„œ ê°„ë‹¨í•˜ê²Œ í•œë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•˜ìë©´, 


> We summarize these problem characteristics by saying that BayesOpt is designed for black-box derivative-free global optimization [1]. 


ì—¬ê¸°ì„œ objectiveë¥¼ black-boxë¼ê³  í•˜ëŠ” ì´ìœ ëŠ” ì–´ë– í•œ í˜•íƒœì¸ì§€, derivativeë˜ëŠ”ì§€ ë“± ì•„ë¬´ê²ƒë„ ëª¨ë¥´ëŠ” black-boxì˜ ìƒíƒœë¡œ ì¡´ì¬í•˜ë©°, inputìœ¼ë¡œ xê°€ ë“¤ì–´ê°€ì„œ outputìœ¼ë¡œ f(x)ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ê²ƒë§Œ ì•Œê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤.(ì•„ë˜ schematicì€ ì œê°€ ì‘ì„±í•œ ê²ƒ ì…ë‹ˆë‹¤.).

<p align="center">
<img src="/images/black_box.jpg" width="600">
</p>

ì´ëŸ¬í•œ objective functionì„ êµ¬í•˜ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ questì´ë©°, ì´ë•Œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ì´ì œ ì‚´í´ë³¼ surrogate modelì´ë‹¤. 

#### â˜» Surrogate model 

surrogate  modelì€ blackbox functionì„ ìœ„í•œ statistic/probabilistic ëª¨ë¸ë§ìœ¼ë¡œ, ë¯¸ì§€ì˜ ëª©ì í•¨ìˆ˜ objective functionì„ í™•ë¥ ì ìœ¼ë¡œ ì¶”ì •í•˜ëŠ” ëª¨ë¸ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

surrogateì˜ ì‚¬ì „ì  ì˜ë¯¸ë¥¼ í™•ì¸í•´ë³´ë©´ **ëŒ€ë¦¬ì˜, ëŒ€ìš©ì˜**ë¼ëŠ” ì˜ë¯¸ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤ [16]. 

~~~~~~~~~~~~~~~~~~~~~
â—¦ surrogate
  : someone or something that replaces or is used instead of someone or something else; a substitute for another
~~~~~~~~~~~~~~~~~~~~~


> A surrogate model is an engineering method used when an outcome of interest cannot be easily measured or computed, so a model of the outcome is used instead. Most engineering design problems require experiments and/or simulations to evaluate design objective and constraint functions as a function of design variables [17]. 


MLê´€ì ì—ì„œ surrogate modelì„ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤ [18]. 


> ë³µì¡í•˜ê³  ì„¤ëª…í•˜ê¸° í˜ë“  ëª¨í˜•(ë¸”ë™ë°•ìŠ¤ ëª¨í˜•)ì„ ë‹¨ìˆœí•˜ê³  ì„¤ëª…í•˜ê¸° ì‰¬ìš´ ëª¨í˜•(ëŒ€ë¦¬ ëª¨í˜•)ìœ¼ë¡œ ëŒ€ì‹  ì„¤ëª…í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. ì„ í˜•íšŒê·€ ëª¨í˜•, ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨í˜•, íŠ¸ë¦¬ëª¨í˜• ë“±ì˜ ì„¤ëª… ê°€ëŠ¥í•œ ëª¨í˜•ì„ ëŒ€ë¦¬ ëª¨í˜•ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ëŒ€ë¦¬ ëª¨í˜•ì€ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ì— ë”°ë¼ í¬ê²Œ ë‘ ê°€ì§€ ë°©í–¥ìœ¼ë¡œ ë¶„ë¥˜í•´ ë³¼ ìˆ˜ ìˆë‹¤. train ë°ì´í„°ì˜ ì „ë¶€ ë˜ëŠ” ì¼ë¶€ë¥¼ ëŒ€ë¦¬ ëª¨í˜•ìœ¼ë¡œ ë‹¤ì‹œ í•™ìŠµí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì „ì—­ì  ëŒ€ë¦¬ ëª¨í˜•(global surrogate model)ì´ë¼ í•˜ê³ , íŠ¹ì • ë°ì´í„° í¬ì¸íŠ¸ ê·¼ë°©ì—ì„œ ìƒ˜í”Œë§ëœ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ëŒ€ë¦¬ ëª¨í˜•ì„ í•™ìŠµí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ êµ­ì†Œì  ëŒ€ë¦¬ ëª¨í˜•(local surrogate model)ì´ë¼ê³  í•œë‹¤.


ë”°ë¼ì„œ black-boxì¸ objective functionì„ ì„¤ëª…í•˜ê¸°ìœ„í•´ ê³ ë ¤í•œ ê²ƒì´ surrogate model (ëŒ€ë¦¬ëª¨í˜•)ì´ë¼ê³  ë³¼ ìˆ˜ ìˆê³ , ìš°ë¦¬ëŠ” Bayes Theoremì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆëŠ” GPRì„ ì‚¬ìš©í•˜ê²Œ ë¨ì— ë”°ë¼ ë³¸ ìµœì í™” ë°©ë²•ì´ **Bayesian Optimization**ì´ ë˜ëŠ” ê²ƒì´ë‹¤. 

ë­.. ì¡°ê¸ˆì€ ê·¸ëŸ­ì €ëŸ­ ì´í•´ê°€ ë˜ëŠ” ê°™ë‹¤. 

<p align="center">
<img src="/images/Ratatouille_remy_whatever.gif" width="350">
</p>


ì, ë‹¤ì‹œ ë…¼ë¬¸ìœ¼ë¡œ ëŒì•„ê°€ì„œ í™•ì¸í•˜ë©´ [1,19], kê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ ìˆê³ 

![image](https://user-images.githubusercontent.com/40614421/176208607-f67b2f69-c6af-466a-a409-e9f750434dc1.png)

ê·¸ì— ëŒ€ì‘í•˜ëŠ” í•¨ìˆ˜ê°’ë“¤ì´ vectorì˜ í˜•íƒœë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì¡´ì¬ í•  ë•Œ,  

![image](https://user-images.githubusercontent.com/40614421/176209053-f1f9fc12-dff0-481c-afd4-9abdd30addbe.png)

i ë²ˆì§¸ ë°ì´í„° í¬ì¸íŠ¸ì—ì„œ mean function (í‰ê·  í•¨ìˆ˜)ê³¼ covariance function (or kernel) (ë‘ ì…ë ¥ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë°˜ì˜) ì€ ![image](https://user-images.githubusercontent.com/40614421/176209785-549b9c88-45a8-426e-a6ba-40b482f2bca5.png)
ì´ë‹¤. 
ì—¬ê¸°ì„œ normal distiributionì˜ í˜•íƒœë¥¼ ê°–ëŠ” Prior distributionì€ ì•„ë˜ì™€ ê°™ë‹¤. 

![image](https://user-images.githubusercontent.com/40614421/176210207-ddaaf569-6d0d-4cf6-b4bc-4bdfc744e30d.png)

ì—¬ê¸°ì„œ 1:këŠ” 1ë²ˆì§¸ë¶€í„° kë²ˆì§¸ ê°’ì„ í¬í•¨í•˜ëŠ” ë²¡í„°ë¥¼ ì˜ë¯¸í•œë‹¤

![image](https://user-images.githubusercontent.com/40614421/176210814-d087466e-0b5b-476b-b7d5-75ca6532f7eb.png)

ì´ì œ, n+1ë²ˆì§¸ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ kë¼ê³  ë†“ê³ , ì´ë•Œ f(x)ì— ëŒ€í•œ conditional distribution (posterior probability)ë¥¼ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.  

![image](https://user-images.githubusercontent.com/40614421/176214434-ff1ad67d-29b5-4d20-baf7-81fe2fcb3d71.png)

![image](https://user-images.githubusercontent.com/40614421/176212752-a6811437-9bdc-4b74-a7fc-bc3b1d864bf6.png)

![image](https://user-images.githubusercontent.com/40614421/176214227-ddb27a83-188b-45c3-b807-17721aa5a132.png)

ì¦‰, Gaussian Process Regression (GPR)ì„ ì´ìš©í•´ì„œ meanê³¼ covarianceë¥¼ êµ¬í•˜ê³ , ì´ ê°’ë“¤ì„ aquisition functionì— ì—…ë°ì´íŠ¸ ì‹œì¼œì¤€ë‹¤. ì´ë•Œ, GPRì€ ê¸°ì¡´ì— ì¸¡ì •í•œ ë‹¤ë¥¸ê°’ë“¤ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ê³ , ë‘ ë°ì´í„° ê°’ì´ ê°€ê¹Œìš°ë©´ í° ê°’ì„ ë©€ë©´ ì‘ì€ ê°’ì„ ë°˜í™˜í•œë‹¤ [1,19].

#### â˜» Acquisition function

ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì‹œ ê³ ë ¤í•œ ë°ì´í„° ì¶”ì²œí•˜ëŠ”ë° í™œìš©í•˜ëŠ” í•¨ìˆ˜, íƒìƒ‰í•  ì…ë ¥ê°’ í›„ë³´ ì¶”ì²œí•˜ëŠ” í•¨ìˆ˜ë¡œ surrogate modelì—ì„œ ì¤€ meanê³¼ variance ê°’ì„ ì´ìš©í•˜ê³  ìˆë‹¤. ì´ë•Œ,  agquisitoin functionì˜ ì—­í• ì€ exploitionê³¼ explorationì˜ balanceë¥¼ ì˜ ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. 

(ëë‚ ë“¯ ëŠë‚˜ì§€ ì•ŠëŠ” BOì˜ ëŠª ğŸŠğŸ»â€â™€ï¸ ğŸŠğŸ» ...)


<p align="center">
<img src="/images/sesame_what.gif" width="350">
</p>



> They all trade-off exploration and exploitation so as to minimize the number of function queries [13]. 


ì–¸ê¸‰í•˜ì˜€ë“¯ì´ exploitation ê³¼ explorationì€ trade offê´€ê³„ë¡œ **exploitationì€ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ì˜ì—­** ê·¸ë¦¬ê³  **explorationì€ ìš°ë¦¬ê°€ ì˜ ëª¨ë¥´ëŠ” ì˜ì—­** ì„ ì˜ë¯¸í•œë‹¤ [20].


> Exploration ensures the algorithm to reach different promising regions of the search space, whereas exploitation ensures the searching of optimal solutions within the given region. The fine tuning of these components is required to achieve the optimal solution for a given problem. It is difficult to balance between these components due to stochastic nature of optimization problem. This fact motivates us to develop a novel metaheuristic algorithm for solving real-life engineering design problem. The performance of one optimizer to solve the set of problem does not guarantee to solve all optimization problems with different natures.

ì´ëŸ¬í•œ ì„¤ëª…ì€ ë„ˆë¬´ generalí•´ì„œ ìš°ë¦¬ê°€ ê³ ë ¤í•˜ëŠ” Acquisition functionì—ì„œ ì‚¬ìš©ë˜ëŠ” exploitationê³¼ explorationì— ëŒ€í•´ì„œ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ê³ , ì´ ë‘˜ì˜ balanceë¥¼ ì˜ ìœ ì§€í•˜ë©´ì„œ next query p'të¥¼ ì°¾ì•„ì£¼ëŠ” ê²ƒì´ acquisition functionì˜ ì—­í• ì´ë¼ í•  ìˆ˜ ìˆë‹¤ [21,22]. 

~~~~~~~~~~~~~~~~~~~~~
â—¦ Exploitation : posterial mean ì´ ë†’ì€ ì˜ì—­ 
â—¦ Exploration :  posterial varianceê°€ ë†’ì€ ì˜ì—­ 
~~~~~~~~~~~~~~~~~~~~~


> Choose the next x where the posterior mean is hight (exploitation) and the posterior variance is high (exploration). 


> Comparing two points x1 and x2: if their means are the same, then BO will pick the one that has larger Ïƒ2(x). This is called exploration.Comparing two points x1 and x2: if their variances are the same, then BO will pick the one that has larger Î¼(x). This is called exploitation [22].


ì´ëŸ¬í•œ Acquisitoin functionsìœ¼ë¡œëŠ” ì•„ë˜ì™€ ê°™ì´ ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆë‹¤ [1].

~~~~~~~~~~~~~~~~~~~~~
â—¦ Probability of Improvement (PI)
â—¦ Expected Improvement (EI)
â—¦ Bayesian expeced losses
â—¦ Upper confindence bouncds (UCB)
â—¦ Thompson smapling 
â—¦ GP Upper Con dence Bound (GP-UCB)
â—¦ Entropy search 
â—¦ Knowledge gradient 
~~~~~~~~~~~~~~~~~~~~~


ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ê³ ë ¤í•œ functionì€ EIì´ê³ , ì´ëŸ¬í•œ ë°°ê²½ìœ¼ë¡œëŠ” EIê°€ ì‚¬ìš©í•˜ê¸° í¸í•˜ê³  perform wellí•˜ê¸° ë•Œë¬¸ì´ë‹¤. ê° functionì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ out of scopeì´ë¼ì„œ ìƒëµí•˜ê¸°ë¡œ í•œë‹¤ [1,13]. 


**working here**





ì˜¤í˜¸! ì´ì œ Bo (BayesOpt)ì— ëŒ€í•´ì„œ ì¢€ ì´í•´ê°€ ë˜ëŠ” ê²ƒ ê°™ë‹¤. **Bayes theorem** ê³¼ **Optimization** ì˜ ë§Œë‚¨ì´ë€ í™˜ìƒì ì´êµ¬ë‚˜~~


<p align="center">
<img src="/images/Ratatouille_remy_happy.gif" width="350">
</p>


-----------------------------------------------------------------------

ê¸€ì„ ì‘ì„±í•œë‹¤ëŠ” ê²ƒì€ ë‚´ê°€ ë°°ìš´ ê²ƒì„ ê³µìœ í•œë‹¤ëŠ” ê´€ì ì—ì„œëŠ” ì¬ë°Œê³  ë¿Œë“¯í•¨ì„ ëŠë‚€ë‹¤. í•˜ì§€ë§Œ í•œí¸ìœ¼ë¡œëŠ” ë¬´ì²™ì´ë‚˜ ì¡°ì‹¬ìŠ¤ëŸ½ê³  í° ì±…ì„ê°ì„ ëŠë‚€ë‹¤.

ëª¨ë“  ì¼ì—ëŠ” ì±…ì„ê°ì´ ë”°ë¥´ê¸° ë§ˆë ¨ì´ë‹ˆ ìˆ˜ì •í•  ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì¶”í›„ì—ë„ ê³„ì† ì—…ë°ì´íŠ¸í•  ì˜ˆì •ì´ë‹¤. 

ê³µë¶€í•´ì•¼ í•  ê²ƒë„ ë§ê³ , ë°°ìš°ê³  ì‹¶ì€ ê²ƒë“¤ë„ ë§ê³ , ê°ˆ ê¸¸ë„ ì¢€ ë©€ì§€ë§Œ ê·¸ë˜ë„ ì¡°ê¸ˆì”© ê·¸ë¦¬ê³  ê¾¸ì¤€íˆ ë‚˜ì•„ê°€ë³´ì í•œë‹¤. 

<p align="center">
<img src="/images/elmo_workingout.gif" width="350">
</p>


### â˜» Reference
1. [Paper : A Tutorial on Bayesian Optimization](https://arxiv.org/abs/1807.02811)
2. [wikipedia : Bayesian Optimization](https://en.wikipedia.org/wiki/Bayesian_optimization)
3. [Machine Learning Mastery : How to Implement Bayesian Optimization from Scratch in Python](https://machinelearningmastery.com/what-is-bayesian-optimization/)
4. wikipedia : ìˆ˜í•™ì  ìµœì í™”
5. [wikipedia : Optimization problem](https://en.wikipedia.org/wiki/Optimization_problem)
6. [Book (ê°œì¸ ì†Œì¥): Mathematics for Machine Learning by Book by A. Aldo Faisal, Cheng Soon Ong, and Marc Peter Deisenroth](https://www.amazon.com/Mathematics-Machine-Learning-Peter-Deisenroth/dp/110845514X)  
7. [wikipedia : Global optimization](https://en.wikipedia.org/wiki/Global_optimization)
8. [Machine Learning Mastery : Local Optimization versus Global Optimization](https://machinelearningmastery.com/local-optimization-versus-global-optimization/#:~:text=Local%20optimization%20involves%20finding%20the,problems%20that%20contain%20local%20optima)
9. ë„¤ì´ë²„ êµ­ì–´/ì˜ì–´ì‚¬ì „
10. [7 Hyperparameter Optimization Techniques Every Data Scientist Should Know](https://towardsdatascience.com/7-hyperparameter-optimization-techniques-every-data-scientist-should-know-12cdebe713da)
11. [Parameters and Hyperparameters in Machine Learning and Deep Learning](https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac)
12. [Definition of hyper-](https://www.merriam-webster.com/dictionary/hyper)
13. [wikipedia : Bayesian Optimization](https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions)
14. [Paper :Taking the Human Out of the Loop: A Review of Bayesian Optimization (2016)](https://ieeexplore.ieee.org/document/7352306)
15. [wikipedia " 68-95-99.7 rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule#:~:text=In%20the%20empirical%20sciences%2C%20the,99.7%25%20probability%20as%20near%20certainty.)
16. [cambridge dictionary : surrogate](https://dictionary.cambridge.org/dictionary/english/surrogate)
17. [wikipedia : surrogate model](https://en.wikipedia.org/wiki/Surrogate_model#:~:text=A%20surrogate%20model%20is%20an,a%20function%20of%20design%20variables.)
18. [ëŒ€ë¦¬ëª¨í˜•](https://psystat.tistory.com/139)
19. [ë…¼ë¬¸ë¦¬ë·°:ë² ì´ì§€ì•ˆ ìµœì í™” (Bayesian Optimization)](https://gils-lab.tistory.com/61)
20. [What is Exploitation and Exploration in Optimization Algorithms?](https://www.researchgate.net/post/What_is_Exploitation_and_Exploration_in_Optimization_Algorithms#:~:text=Exploration%20ensures%20the%20algorithm%20to,solution%20for%20a%20given%20problem)
21. [Epxloration and Exploitation](https://wonwooddo.tistory.com/89)
22. [How does Bayesian Optimization balance exploration with exploitation?](https://stats.stackexchange.com/questions/509812/how-does-bayesian-optimization-balance-exploration-with-exploitation)

Code : [ë² ì´ì§€ì•ˆ ìµœì í™”](https://notebook.community/zzsza/TIL/python/bayesian-optimization)

### â˜» image sources
1. [Giphy](https://giphy.com/search/sesame-street)


ğŸŒº **Thanks for reading. Hope to see you again :o)**


-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="150">
</p>

ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ ì§„í–‰í•˜ëŠ” "í•¨ê»˜ ì½˜í…ì¸ ë¥¼ ì œì‘í•˜ëŠ” ì½˜í…ì¸  í¬ë¦¬ì—ì´í„° ëª¨ì„"ì¸ **COCRE(ì½”í¬ë¦¬)** ì˜ 2ê¸° íšŒì›ìœ¼ë¡œ ì œì‘í•œ ê¸€ì…ë‹ˆë‹¤

[ğŸ˜ ì½”í¬ë¦¬ê°€ ê¶ê¸ˆí•˜ë‹¤ë©´ í´ë¦­!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)




