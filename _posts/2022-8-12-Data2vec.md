---
layout: post
title: data2vec <img src="/images/B-icon-ver.png" width="30">
---

### [Review | ë…¼ë¬¸ë¦¬ë·°] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language

ì½”ë„ˆ ì†ì˜ ì½”ë„ˆ EPA (Exploratory Paper Analysis), ë…¼ë¬¸ ë¦¬ë·° ì…ë‹ˆë‹¤.

ğŸ“ƒ [paper, 2022 [1]](https://arxiv.org/abs/2202.03555)

ğŸ”  [code](https://github.com/facebookresearch/fairseq/tree/main/examples/data2vec)

-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ What the data2vec is. 

â˜ºï¸ Related work 
  - Self-supervised learning in CV, NLP, and speech
  - Multimodal pre-training

â˜ºï¸ Method
  - Model architecture
  - Masking
  - Trainning targets

â˜ºï¸ Experimental setup + Results 
  - Computer vision 
  - Speech processing 
  - Natural language processing (NLP)
  - Ablations

â˜ºï¸ Discussion 
  - Modality specific features extractors and masking
  - Structured and contextualized targets 
  - Representation collapse 

â˜ºï¸ Extra notion

â˜» Reference

-----------------------------------------------------------------------


### â˜ºï¸ What the data2vec is [1-4].

<p align="center">
<img src="/images/data2vec_data.gif" width="350">
</p>


Data2vecì€ ì‰½ê²Œ ë§í•´ì„œ speech, NLP ë˜ëŠ” computer visionì— ë“±ì— ë™ì¼í•œ learning methodë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. 

ê¸°ì¡´ ë°©ë²•ë“¤ì€ Modality-specific targets (words, visual tokens or units of human speech)ì„ ì˜ˆì¸¡í•˜ì˜€ì§€ë§Œ ì´ì™€ ë‹¤ë¥´ê²Œ, data2vecì€ contextualized latent representationsì„ ì˜ˆì¸¡í•œë‹¤. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ contextualized latent representationsëŠ” entire inputìœ¼ë¡œë¶€í„°ì˜ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. 

ë˜í•œ ì—¬ê¸°ì„œëŠ” distillation setup, ì¦‰ student-teacher networkë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆê³ , ë‘˜ ë‹¤ ê°™ì€ êµ¬ì¡°ì˜ standard transformaer architectureì„ ì‚¬ìš©í•œë‹¤. 

teacher modelì€ original dataë¥¼ ì…ë ¥í•˜ì—¬ representationì„ ìƒì„±í•˜ê³  ì´ëŠ” target ì´ ëœë‹¤. ê·¸ë¦¬ê³  student modelì€ original dataì— maskingì„ í•œ masked dataë¥¼ ì…ë ¥í•˜ì—¬ original inputì˜ representationì„ ì˜ˆì¸¡í•œë‹¤. ê·¸ í›„ teacher modelì€ ë‹¤ì‹œ student modelì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³µì‚¬í•˜ì—¬ ìµœì‹ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.


#### â˜» self-supervised learning [3]

supervised learningì˜ ê²½ìš°, labeled datasetì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì‹œí‚¨ë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ labeled dataì˜ ê²½ìš° ë¹„ì‹¸ê³ , ë˜í•œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ êµ¬í•˜ê¸°ê°€ ì–´ë µë‹¤. ë”°ë¼ì„œ ìì²´ì ìœ¼ë¡œ labelì„ ë§Œë“¤ì–´ì„œ í•™ìŠµì„ ì‹œí‚¤ìê³  ì œì•ˆëœ ë°©ë²•ì´ self-supervised learningì´ë‹¤. 

ì¢€ ë” detailsí•˜ê²Œ í™•ì¸í•˜ë©´ [5,6]

> Self-supervised learning (SSL) is a method of machine learning. It learns from unlabeled sample data. It can be regarded as an intermediate form between supervised and unsupervised learning. It is based on an artificial neural network. The neural network learns in two steps. First, the task is solved based on pseudo-labels which help to initialize the network weights. Second, the actual task is performed with supervised or unsupervised learning.

ì´ëŸ¬í•œ self-supervised learningì€ CV, NLP, Speechì— ì‚¬ìš©ëœ ê²½ìš°ë¥¼ í™•ì¸í•´ë³´ë©´, 

CVì˜ ê²½ìš°ëŠ” í¬ê²Œ **Rotation**ê³¼ **Context prediction**ì´ ìˆë‹¤ [3,7,8]. 

ìš°ì„ , rotation ì‹œí‚¨í›„ CNNì— ë„£ì–´ì„œ íšŒì „ì„ ë§ì¶”ëŠ” taskë¥¼ í‘¸ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ê²½ìš°, ë™ì¼í•œ ì´ë¯¸ì§€ê°€ ë“¤ì–´ì˜¤ëŠ”ë° ê°ë„ê°€ ë°”ë€ŒëŠ” ê²ƒì´ë‹¤. 

<p align="center">
<img src="/images/data2vec_selfsupervised_CV_rotation.png" width="600">
</p>

Fig [7]

Context predictionì˜ ê²½ìš°ëŠ” patch ìƒì„±í•˜ê³ , ê°€ìš´ë° ì´ë¯¸ì§€ë¥¼ ì•µì»¤ì²˜ëŸ¼ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ 8ê°œ ì´ë¯¸ì§€ì¤‘ ëœë¤í•˜ê²Œ ì´ë¯¸ì§€ë¥¼ ê°–ê³ ì™€ì„œ ë‘ ì´ë¯¸ì§€ë¥¼ CNNì— ë„£ì–´ì„œ ì•µì»¤ ì œì™¸í•˜ê³  ê°–ê³ ì˜¨ query ì´ë¯¸ì§€ê°€ ëª‡ë²ˆì§¸ ì´ë¯¸ì§€ì¸ì§€ ë§ì¶°ì£¼ëŠ” taskì´ë‹¤. 
ì—¬ê¸°ì„œ ë” ë‚˜ì•„ê°„ ê²ƒì´ ë‘ê°œì˜ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ì´ 9ê°œì˜ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì‚¬ìš©í•´ì„œ CNNì— ë„£ì€ í›„ ì›ë˜ ì´ë¯¸ì§€ì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒìœ¼ë¡œ jigsaw puzzleë¼ê³  ë¶ˆë¦¬ìš´ë‹¤. 

<p align="center">
<img src="/images/data2vec_Context Prediction_two.png" width="600">
</p>

Fig [8]


<p align="center">
<img src="/images/data2vec_selfsupervised_CV_jigsaw_puzzle.png" width="600">
</p>

Fig [7]


NLPì˜ ê²½ìš°ëŠ” BERT 

<p align="center">
<img src="/images/data2vec_bert.png" width="600">
</p>

Fig [9]

speechì˜ ê²½ìš°ëŠ” wave2vec 2.0ì´ ëŒ€í‘œì ì¸ ì˜ˆì´ë‹¤ [1].

<p align="center">
<img src="/images/data2vec_wave2vec2.png" width="700">
</p>

Fig [10]

ì´ë¥¼ í†µí•´ ê°ê°ì˜ modalityì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ë°©ë²•ë¡ ì„ ê°ê¸° ì ìš©í•´ì„œ self-supervised learningì„ ì§„í–‰í•´ ì™”ë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤. 

#### â˜» Multimodal pre-training 

Multimodalityë¼ëŠ” ê²ƒì˜ ì •ì˜ë¥¼ ë³´ë©´ [11]

> The term multimodality refers to the combination of multiple sensory and communicative modes, such as sight, sound, print, images, video, music, and so on, that produce meaning in any given message.

ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ë©´ [12]
(ì—¬ê¸°ì„œ multimodalityì™€ multimodalì€ ë™ì¼í•˜ë‹¤.) 

> ì—¬ëŸ¬ê°€ì§€ í˜•íƒœì™€ ì˜ë¯¸ë¡œ ì»´í“¨í„°ì™€ ëŒ€í™”í•˜ëŠ” í™˜ê²½ìœ¼ë¡œ ëª¨ë‹¬ (modality)ë€ ì¸í„°ë™ì…˜ ê³¼ì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì˜ì‚¬ì†Œí†µ ì±„ë„ì„ ê°€ë¥´í‚¨ë‹¤. ë©€í‹°ëª¨ë‹¬ì€ í…ìŠ¤íŠ¸, ìŒì„±, ì´ë¯¸ì§€, ì˜ìƒë“± ì„œë¡œ ë‹¤ë¥¸ ì–‘ì‹ì˜ ë°ì´í„°ë¥¼ ììœ ìì¬ë¡œ ì´í•´í•˜ê³  ë³€í™˜í•  ìˆ˜ ìˆì–´ ì‚¬ëŒì²˜ëŸ¼ ë°°ìš°ê³  ìƒê°í•˜ë©° ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤. 

ì´ëŸ¬í•œ ë©€í‹°ëª¨ë‹¬ì˜ ê²½ìš° cross-modal representationì„ ìƒì„±ì„ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤ [1].

> There has been a considerable body of research on learning representations of multiple modalities simultaneously often using paired data (Aytar et al., 2017; Radford et al., 2021b; Wang et al., 2021; Singh et al., 2021) with the aim to produce cross-modal representations which can perform well on multi-modal tasks and with modalities benefiting from each other through joint training (Alayrac et al., 2020; Akbari et al., 2021) with recent methods exploring few-shot learning (Tsimpoukelli et al., 2021). Our work does not perform multimodal training but aims to unifiy the learning objective for self-supervised learning in different modalities. We hope that this will en- able better multimodal representations in the future.


ë³¸ ì—°êµ¬ì—ì„œëŠ” Modalityê°€ ë‹¤ë¥´ë”ë¼ë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ self-supervised learningì„ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ì˜€ë‹¤. ì´ëŠ” modalityê°€ ë‹¬ë¼ì„œ ë‹¤ë¥¸ inputì´ ë“¤ì–´ì™€ë„ ë™ì¼í•œ í˜•íƒœì˜ contextualized representationsë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. 

### â˜ºï¸ Method 

<p align="center">
<img src="/images/data2vec_teacher_student.png" width="800">
</p>

data2vecì˜ ê²½ìš° a partial view of the inputê°€ ì£¼ì–´ì§€ë©´ ì „ì²´ full input dataì˜ model representationsì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµì‹œì¼œì§„ë‹¤. 

ì²˜ìŒì— a masked version of the training sample (model in student mode)ì„ encodeí•˜ê³  ê·¸ í›„ ë™ì¼ ëª¨ë¸ë¡œ unmasked version of the inputì„ encodeí•´ì„œ training targetsì„ ë§Œë“ ë‹¤. í•˜ì§€ë§Œ an exponentially moving average of the model weightsìœ¼ë¡œ paramerize í•œë‹¤ (model in teacher mode). 


#### â˜» Model architecture

Modality-specific encodingì„ ê°–ì€ standard Transformer architectureì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ì¦‰, ì…ë ¥ì˜ encodingë°©ë²•ì€ modality ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ëª¨ë¸ êµ¬ì¡°ë‚˜ í•™ìŠµ ë°©ë²•ìì²´ëŠ” í†µí•©ëœ í˜•íƒœë¥¼ ë„ê³  ìˆë‹¤.

#### â˜» Masking [13]

Input sampleì´ a sequence of tokensìœ¼ë¡œ ì„ë°°ë”© ëœ í›„ì—, a learned MASK embedding tokenìœ¼ë¡œ ë°”ê¿ˆìœ¼ë¡œì¨ maskingí•˜ê³  sequenceë¥¼ transformer ë„¤íŠ¸ì›Œí¬ì— ê³µê¸‰í•œë‹¤. ì´ë•Œ modalityì— ë”°ë¼ì„œ ë‹¤ë¥¸ maskingë°©ë²•ì„ ê³ ë ¤í•œë‹¤.

CVì˜ ê²½ìš°, BEiT masking ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ì´ëŠ” image patchesë¥¼ ì„ì˜ë¡œ maskingí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ”ë° ì¸ì ‘í•œ ë¸”ë¡ì„ maskingí•˜ëŠ” blockwiseë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.

Speechì˜ ê²½ìš°, wave2vec masking ë°©ë²•ì„ ì‚¬ìš©í•˜ê³ , ì´ë•Œ latent speech representationì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ cnn êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ê³  nê°œì˜ tokenì„ ì—°ì†ì ìœ¼ë¡œ maskingí•œë‹¤.

NLPì˜ ê²½ìš°, RoBERTa masking ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. BERTëŠ” í•œë²ˆë§Œ random maskingì„ í•˜ê³  ëª¨ë“  epochì—ì„œ ë™ì¼í•œ maskë¥¼ ë°˜ë³µí•˜ì§€ë§Œ RoBERTaì—ì„œëŠ” ë§¤ epochë§ˆë‹¤ ë‹¤ë¥¸ maskingì„ ìˆ˜í–‰í•˜ëŠ” dynamic masking ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ë˜í•œ BERTì™€ ë‹¬ë¦¬ ë‹¤ìŒ ë¬¸ì¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.


#### â˜» Trainning targets

An encoding of the masked sample ì„ ê¸°ë°˜ìœ¼ë¡œ original unmasked training sampleì˜ model representationsì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ë‹¤. 

#### âœ¤ Teacher parameterization

Teacher paramter ê²½ìš° BYOLì—ì„œ ì œì•ˆí•œ an exponentially moving average (EMA)ì„ í†µí•´ parameterizeí•˜ëŠ”ë°, ì´ë•Œ ë‘ê°œì˜ parametersì¸ student model weight, ğ·ì™€ teacher model weight, ğ™ ì„ ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¤ëŠ”ì§€ê°€ ì¤‘ìš”í•˜ë‹¤. 

<img src="https://latex.codecogs.com/svg.image?\triangle&space;\leftarrow&space;\tau&space;\triangle&space;&plus;&space;(1-\tau)\theta" title="\triangle \leftarrow \tau \triangle + (1-\tau)\theta" />

Ï„ = 1ì´ë©´ teacher model weightë§Œ ì‚¬ìš©í•˜ê³ , Ï„ = 0 ì´ë©´ student modelë§Œ ì‚¬ìš©í•œë‹¤. 

ë³¸ ì—°êµ¬ì—ì„œëŠ” Ï„ì„ í•™ìŠµì˜ ì²« në²ˆì˜ ì—…ë°ì´íŠ¸ ë™ì•ˆ Ï„0ì—ì„œ Ï„e ê¹Œì§€ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ë„ë¡ ìŠ¤ì¼€ì¤„ë§í•˜ê³ , ë‚¨ì€ í•™ìŠµ ì‹œê°„ë™ì•ˆì€ ì¼ì •í•œ ìƒìˆ˜ê°’ì„ ê°–ê²Œ í•˜ì˜€ë‹¤.
ì´ëŸ¬í•œ ì „ëµì€ ëª¨ë¸ì´ randomì¸ í•™ìŠµ ì´ˆê¸°ì—ëŠ” teacherê°€ ì¢€ ë” ìì£¼ ì—…ë°ì´íŠ¸ë˜ê³  ì–´ëŠì •ë„ í•™ìŠµì´ ëœ ì´í›„, ì¦‰ ì¢‹ì€ parameterì„ í•™ìŠµí•œ ì´í›„ì—ëŠ” ì ê²Œ ì—…ë°ì´íŠ¸ ë˜ë„ë¡ í•œ ê²ƒì´ë‹¤. 

#### âœ¤ Target [3]

ë³´ëŠ” ë°”ì™€ ê°™ì´ student modelì—ëŠ” masked inputì„ teacher modelì—ëŠ” original inputì„ ì‚¬ìš©í•œë‹¤. transformer ìì²´ì˜ outputì„ ë§ì¶”ê²Œ ë˜ëŠ” í˜•íƒœë¥¼ ê°–ê³  ìˆìœ¼ë©° teacher modelì˜ ì•„ì›ƒí’‹ì´ targetì´ ë˜ê³ , student modelì˜ masked inputëœ ê²ƒì˜ ì•„ì›ƒí’‹ì´ targetì„ ì˜ representationì„ ì˜ ë”°ë¼ê°€ë„ë¡ lossë¥¼ ì „íŒŒí•œë‹¤. 

<p align="center">
<img src="/images/data2vec_target01.png" width="700">
</p>

target ëª¨ë¸ (teacher model)ì˜ ê²½ìš°ëŠ” original inputì´ ê·¸ëŒ€ë¡œ ë“¤ì–´ê°€ë©° ì´ ëª¨ë¸ì„ transformer êµ¬ì¡°ë¥¼ ê°–ê³  ìˆìœ¼ë¯€ë¡œ ë³´ëŠ” ë°”ì™€ ê°™ì´ self-attention layersì„ ì´ë£¬ë‹¤. ê·¼ë°, ì´ë•Œ ìƒìœ„ top K layersì„ ì‚¬ìš©í•´ì„œ target, yë¥¼ ì‚°ì¶œí•œë‹¤. ì´ë•Œ, ê° layerì—ì„œ outputì„ normalizationì„ í•˜ê³  ì´ ê°’ë“¤ì˜ í‰ê· ì„ ì·¨í•œë‹¤. ì´ë¥¼ í†µí•´ ì„±ëŠ¥ì´ ë” ë†’ë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤. speechì˜ ê²½ìš° instance normì„ CVì™€ NLPì˜ ê²½ìš° layer normì„ ì‚¬ìš©í•˜ì˜€ë‹¤. 

<p align="center">
<img src="/images/data2vec_target02.png" width="700">
</p>


<img src="https://latex.codecogs.com/svg.image?y_{t}&space;=&space;\frac{1}{K}\sum_{l=L-K&plus;1}^{L}\hat{a}_t^l" title="y_{t} = \frac{1}{K}\sum_{l=L-K+1}^{L}\hat{a}_t^l" />


#### âœ¤ Objectives
Time step, tì—ì„œ teacher modelì—ì„œ ë‚˜ì˜¨ target,yt ê³¼ model predicton (ì¦‰, Studentì˜ ì¶œë ¥ê°’), ftì— ëŒ€í•´ì„œSmooth L1 lossë¡œ regressionì„ ìˆ˜í–‰í•œë‹¤.

<p align="center">
<img src="/images/data2vec_objectives.png" width="400">
</p>

ì´ losssì˜ ê²½ìš° outlierì— ëœ ë¯¼ê°í•˜ë‹¤ëŠ” ì´ì ì„ ê°–ê³  ìˆì§€ë§Œ, Î²ë¥¼ ì˜ íŠœë‹ í•´ì•¼í•œë‹¤.

### â˜ºï¸ Experimental setup + Results 

ì—¬ê¸°ì„œëŠ” ë‘ê°œì˜ ëª¨ë¸ data2vec Baseì™€ data2vec Largeë¥¼ ê³ ë ¤í•˜ì˜€ë‹¤. ë‘ ëª¨ë¸ì˜ Transformer blocks (L)ê³¼ hidden dimension(H)ëŠ” 12 x 768 í˜¹ì€ 24 x 1024 ì´ë‹¤. 

ê° ëª¨ë‹¬ì˜ hyperparamters, paramter tuningì€ ë…¼ë¬¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤. 

#### â˜» Computer vision 

ImageNet-1K training setìœ¼ë¡œ data2vecë¥¼ pretrainí•˜ê³  ë™ì¼í•œ ë²¤ì¹˜ë§ˆí¬ì˜ labeled dataì‚¬ìš©í•´ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê²°ê³¼ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì˜€ë‹¤. 
ì•„ë˜ í‘œì— ë‚˜ì˜¤ëŠ” ë°”ì™€ ê°™ì´ ë‹¤ë¥¸ ëª¨ë¸ì— ë¹„í•˜ì—¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. 

<p align="center">
<img src="/images/data2vec_computer_vision.png" width="300">
</p>



#### â˜» Speech processing 

Librispeech (LS-960)ì—ì„œ ì–»ì€ 960 hrs ì˜ speech audio dataë¡œ data2vecë¥¼ pre-trainí•˜ì˜€ë‹¤. ê·¸ë¦¬ê³  ë‹¤ë¥¸ ì–‘ì˜ labeled dataë¥¼ ì´ìš©í•´ì„œ automic speech recognitionì„ ìœ„í•´ì„œ ëª¨ë¸ì„ fine-tuneí•˜ì˜€ë‹¤. 

ìœ ëª…í•œ ì•Œê³ ë¦¬ì¦˜ì¸ wav2vec 2.0 (Baevski et al., 2020b)ì™€ HuBERT (Hsu et al., 2021)ì™€ ë¹„êµí•˜ì˜€ë‹¤. 

<p align="center">
<img src="/images/data2vec_speech_processing.png" width="600">
</p>


#### â˜» Natural anguage processing (NLP)

ë³¸ ì—°êµ¬ì—ì„œëŠ” BERTì™€ ê°™ì€ training setì„ ì ìš©í•˜ì˜€ë‹¤. 

> we adopt the same training setup as BERT (Devlin et al., 2019)by pre-training on the Books Corpus (Zhu et al., 2015) and
English Wikipedia data over 1M updates and a batch size of 256 sequences. We evaluate on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) which includes tasks for natural language inference (MNLI, QNLI, RTE), sentence similarity (MRPC, QQP and STS-B), grammaticality (CoLA), and sentiment analysis (SST-2). 

<p align="center">
<img src="/images/data2vec_NLP.png" width="600">
</p>


<!--- 

#### â˜» Ablations


<p align="center">
<img src="/images/data2vec_predicting_target.png" width="800">
</p>


<p align="center">
<img src="/images/data2vec_contextualized_target.png" width="300">
</p>


<p align="center">
<img src="/images/data2vec_effect_of_different_features.png" width="300">
</p>

-->

### â˜ºï¸ Discussion

ê¸°ì¡´ ì—°êµ¬ì™€ ë¹„êµí•˜ì˜€ì„ë•Œ ë³¸ ì—°êµ¬ì˜ íŠ¹ì§•ì„ ê¸°ìˆ í•˜ê³ ì í•œë‹¤. 

#### â˜» Modality specific features extractors and masking

- ë³¸ ì—°êµ¬ì˜ ê°€ì¥ í° ëª©ì ì€ ë‹¤ë¥¸ modalityì— ëŒ€í•´ì„œ single learning mechanismì„ ì„¤ê³„í•˜ëŠ” ê²ƒì´ë‹¤. 
- unified learning regimeì´ ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì—¬ì „íˆ íŠ¹ì • ë¶€ë¶„, modality-specific features extractors/masking strategiesì—ì„œëŠ” modality-specificí•˜ë‹¤. ì¦‰, ì—¬ì „íˆ modality dependentí•˜ê²Œ ì ìš©í•´ì¤˜ì•¼ í•˜ëŠ” ë¶€ë¶„ì´ ì¡´ì¬í•œë‹¤. 
- ì´ëŸ¬í•œ ì˜ˆë¡œ, speechì˜ inputì˜ ê²½ìš°ëŠ” very high resolution input (16kHz waveform)ì´ í•„ìš”í•œë° ë¹„í•´ NLPì˜ ê²½ìš°ëŠ” lower resolution (in the form of much shorter word sequences)ì´ë‹¤. ë”°ë¼ì„œ modalityì— ë”°ë¼ ë‹¤ë¥¸ inputì„ ê³ ë ¤í•´ì¤˜ì•¼ í•œë‹¤. 

#### â˜» Structured and contextualized targets 

- ë‹¤ë¥¸ masked prediction ì—°êµ¬ë“¤ê³¼ì˜ í° ì°¨ì´ì ì€ contextualized featureë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì— ìˆë‹¤. 
- NLPì˜ ê²½ìš°,data2vecì€ pre-defined target unitsì— ì˜ì¡´í•˜ì§€ ì•Šì€ ì²«ë²ˆì§¸ ì—°êµ¬ì´ë‹¤. 

> Most other work uses either words, sub-words (Radford et al., 2018; Devlin et al., 2019), characters (Tay et al., 2021) or even bytes (Xue et al., 2021). Aside, defining word boundaries is not straightforward for some Asian languages. Contextualized targets enable inte- grating features from the entire sequence into the training target which provides a richer self-supervised task.

#### â˜» Representation collapse 

- Self-supervised learningì²˜ëŸ¼ ìê¸° ìì‹ ì˜ targetsì„ ë°°ìš°ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ í”í•œ ì´ìŠˆì¤‘ í•˜ë‚˜ê°€ representation collapseì´ë‹¤. 
- ì´ëŠ” ëª¨ë¸ì´ all masked segmentsì— ëŒ€í•´ì„œ ë§¤ìš° ìœ ì‚¬í•œ representationì„ ìƒì„±í•œë‹¤. 
- ì´ëŸ¬í•œ collapseê°€ ë°œìƒí•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¡œëŠ” 
  - (1) the learning rate is too large or the learning rate warmup is too short which can often be solved by tuning the respective hyperparameters.
  - (2) Ï„ is too low which leads to student model collapse and is then propagated to the teacher. This can be addressed by tuning Ï„0, Ï„e and Ï„n. 
  - (3) adjacent targets are very correlated and where longer spans need to be masked, e.g., speech. We address this by promoting variance through normalizing tar- get representations over the sequence or batch (Grill et al., 2020). For models where targets are less correlated, such as vision and NLP, momentum tracking is sufficient.

ì´ëŸ¬í•œ collapseì˜ í•˜ë‚˜ë¡œ ê°€ì¥ ë§ì´ ê±°ë¡ ë˜ëŠ” ê²ƒì´ GANì˜ mode collapseê°€ ìˆë‹¤. 

Mode collapseë€ GANì„ í•™ìŠµì‹œí‚¬ë•Œ ìƒì„±ì(Generator)ê°€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ëª»í•˜ê³  ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë§Œ ê³„ì†ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤ [14]. 

> However, if a generator produces an especially plausible output, the generator may learn to produce only that output. In fact, the generator is always trying to find the one output that seems most plausible to the discriminator.
If the generator starts producing the same output (or a small set of outputs) over and over again, the discriminator's best strategy is to learn to always reject that output. But if the next generation of discriminator gets stuck in a local minimum and doesn't find the best strategy, then it's too easy for the next generator iteration to find the most plausible output for the current discriminator [15].


<p align="center">
<img src="/images/data2vec_GAN_mode_collapse.png" width="500">
</p>



ì—¬ëŸ¬ modalityì—ì„œ generalí•œ ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤ í–ˆì§€ë§Œ, data2vecì˜ ê²½ìš°ë„ input encoderì™€ maskingì´ modality specificí•˜ë‹¤ëŠ” í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. ì´ëŠ” ì¶”í›„ ì´ ê³¼ì •ë“¤ë„ ëª¨ë‘ modality independent í•˜ê²Œ ë°”ê¾¸ëŠ” ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°€ì•¼ í•œë‹¤ê³  ìƒê°í•œë‹¤. 


-----------------------------------------------------------------------

### â˜ºï¸ Extra notion

ì¶”ê°€ì ì¸ ê°œë…ì— ëŒ€í•´ì„œ ë‹¤ë£¨ê³ ì í•œë‹¤. 

#### â˜» Transformer model [17, 18]

íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ëŠ” 2017ë…„ êµ¬ê¸€ì´ ë°œí‘œí•œ ë…¼ë¬¸ì¸ "Attention is all you need"ì—ì„œ ë‚˜ì˜¨ ëª¨ë¸ë¡œ ê¸°ì¡´ì˜ seq2seqì˜ êµ¬ì¡°ì¸ ì¸ì½”ë”-ë””ì½”ë”ë¥¼ ë”°ë¥´ë©´ì„œë„, ë…¼ë¬¸ì˜ ì´ë¦„ì²˜ëŸ¼ ì–´í…ì…˜(Attention)ë§Œìœ¼ë¡œ êµ¬í˜„í•œ ëª¨ë¸ì´ë‹¤. 

<p align="center">
<img src="/images/data2vec_transformer01.png" width="300">
</p>

[18]

ì´ ëª¨ë¸ì€ RNNì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ì˜€ìŒì—ë„ ë²ˆì—­ ì„±ëŠ¥ì—ì„œë„ RNNë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ë”°ë¼ì„œ ìµœê·¼ì—ëŠ” ëŒ€í‘œì ì¸ ë”¥ ëŸ¬ë‹ ëª¨ë¸ë¡œ ì†ê¼½í˜”ë˜ í•©ì„±ê³±ê³¼ ìˆœí™˜ ì‹ ê²½ë§(CNNê³¼ RNN)ì„ ëŒ€ì²´í•˜ê³  ìˆë‹¤. `íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(foundation model)`ì´ë¼ê³ ë„ ë¶ˆë¦°ë‹¤. 


<p align="center">
<img src="/images/data2vec_transformer02.png" width="600">
</p>

[17]

#### â˜» Contextualized Word Representation [19]

Contextualized Word Embeddingì€ ë‹¨ì–´ë¥¼ ì €ì°¨ì› (ì¼ë°˜ì ìœ¼ë¡œ 100~500 ì°¨ì›) ê³µê°„ì—ì„œ í‘œí˜„í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ ê¸°ì¡´ì˜ ì „í†µì  Word Embeddingê³¼ëŠ” ë‹¬ë¦¬, ê°™ì€ ë‹¨ì–´ë¼ë„ ë¬¸ë§¥ì— ë”°ë¼ ê·¸ í‘œí˜„ ë°©ë²• (representation)ì´ ë°”ë€” ìˆ˜ ìˆëŠ” ê°œë…ì˜ Embeddingì´ë‹¤. NLPì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ELMo, Bert, OpenAI GPTë“±ë„ Contextual Word Embeddingì— ì†í•˜ëŠ” ê¸°ë²•ë“¤ì´ë‹¤. ë˜í•œ ì´ëŸ¬í•œ ì‘ì—…ì„ ìœ„í•´ ë§¤ìš° Deep í•œ ì‹ ê²½ë§ì„ ì‚¬ìš©í•œë‹¤.


<p align="center">
<img src="/images/data2vec_contextualized_word_representation.png" width="700">
</p>


[20]

-----------------------------------------------------------------------
### â˜» Reference
1. [Paper: data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555)
2. [Blog: Meta AI](https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/)
3. [DSBM: data2vec](http://dsba.korea.ac.kr/seminar/?mod=document&uid=1935)
4. [Blog: Data2Vec Review](https://baekyeongmin.github.io/paper-review/data2vec/)
5. [wikipedia: self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning)
6. [Self-supervised learning: The dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)
7. [Blog: Self-Supervised Representation Learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
8. [Unsupervised Visual Representation Learning by Context Prediction](http://graphics.cs.cmu.edu/projects/deepContext/)
9. [paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
10. [paper: wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/pdf/2006.11477.pdf)
11. [Multimodality and Language Learning](https://onlinelibrary.wiley.com/doi/10.1002/9781119472384.ch3#:~:text=The%20term%20multimodality%20refers%20to,meaning%20in%20any%20given%20message)
12. [Blog: ë©€í‹°ëª¨ë‹¬](https://blog.naver.com/PostView.naver?blogId=yckim002&logNo=222599463531&parentCategoryNo=&categoryNo=63&viewDate=&isShowPopularPosts=true&from=search)
13. [blog: ì´ˆê°„ë‹¨ ë…¼ë¬¸ë¦¬ë·°, data2vec](https://everyday-deeplearning.tistory.com/m/entry/data2vec-A-General-Framework-for-Self-supervised-Learning-in-Speech-Vision-and-Language)
14. [blog: GAN model collapse](https://developer-ping9.tistory.com/108#:~:text=GAN%20%2D%20Mode%20Collapse,-Studying%20Ping9_%202020&text=%EC%83%9D%EC%84%B1%EC%9E%90(Generator)%EA%B0%80%20%EB%8B%A4%EC%96%91%ED%95%9C%20%EC%9D%B4%EB%AF%B8%EC%A7%80,%EC%9D%B4%EB%A5%BC%20Mode%20Collapse%EB%9D%BC%20%EC%B9%AD%ED%95%9C%EB%8B%A4)
15. [blog: GAN, Common problems](https://developers.google.com/machine-learning/gan/problems)
16. [paper: UNROLLED GENERATIVE ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1611.02163.pdf)
17. [NVIDIA : What Is a Transformer Model?](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/#:~:text=A%20transformer%20model%20is%20a,25%2C%202022%20by%20Rick%20Merritt)
18. [ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/31379)
19. [blog: Contextualized Word Embedding](https://ws-choi.github.io/blog-kor/nlp/deeplearning/paperreview/Contextualized-Word-Embedding/#:~:text=Contextualized%20Word%20Representation&text=%EC%9D%B4%EB%93%A4%EC%9D%98%20%ED%8A%B9%EC%A7%95%EC%9D%80%20%EA%B0%99%EC%9D%80,%ED%91%9C%ED%98%84(representation)%ED%95%9C%EB%8B%A4%EB%8A%94%20%EA%B2%83%EC%9D%B4%EB%8B%A4)
20. [blog: 62 - Contextualized Word Representations](https://www.innerdoc.com/periodic-table-of-nlp-tasks/62-contextualized-word-representations/)

### â˜» image sources
1. [Giphy](https://giphy.com/search/sesame-street)


ğŸŒˆ **Thanks for reading. Hope to see you again :o)**

-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="100">
</p>

ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ ì§„í–‰í•˜ëŠ” "í•¨ê»˜ ì½˜í…ì¸ ë¥¼ ì œì‘í•˜ëŠ” ì½˜í…ì¸  í¬ë¦¬ì—ì´í„° ëª¨ì„"ì¸ **COCRE(ì½”í¬ë¦¬)** ì˜ 2ê¸° íšŒì›ìœ¼ë¡œ ì œì‘í•œ ê¸€ì…ë‹ˆë‹¤

[ğŸ˜ ì½”í¬ë¦¬ê°€ ê¶ê¸ˆí•˜ë‹¤ë©´ í´ë¦­!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)

