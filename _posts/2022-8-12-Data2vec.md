---
layout: post
title: Data2Vec <img src="/images/B-icon-ver.png" width="30">
---

### [Review | 논문리뷰] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language

코너 속의 코너 EPA (Exploratory Paper Analysis), 논문 리뷰 입니다.

📃 [paper, 2022 [1]](https://arxiv.org/abs/2202.03555)


-----------------------------------------------------------------------

☾ Table of contents

☺︎ What the data2vec is. 

☺︎ Related work 
  - Self-supervised learning
  - Self-supervised learning in CV
  - Self-supervised learning in NLP
  - Self-supervised learning in speech
  - Multimodal pre-training

☺︎ Method
  - Model architecture
  - Masking
  - Trainning targets

☺︎ Experimental setup + Results 
  - Computer vision 
  - Speech processing 
  - Natural language processing (NLP)
  - Ablations

☺︎ Discussion 
  - Modality specific features extractors and masking
  - Structured and contextualized targets 


☻ Reference

-----------------------------------------------------------------------




~~~~~~~~~~~~~~~~~~~~~

### ☺︎ What the data2vec is [1-4].

Data2vec은 self-supervised learning (자기지도학습)으로 multimodality (speech, image, text)에서 적용 가능한 framework으로 이는 기존에 supervised learning (지도학습)으로 특정 단일 modality에 집중해서 각 modality별로 학습 방법을 다르게 했던 것에 비해 좀 더 일반적인 방법으로 확장된 형태이다. 



~~~~~~~~~~~~~~~~~~~~~
** 수정 **
핵심 아이디어는 표준 Transformer 아키텍처를 사용하는 자체 증류 설정(selfdistillation setup)에서 입력의 마스크된 보기를 기반으로 전체 입력 데이터의 잠재된 표현을 예측하는 것입니다.

Knowledge distillation이란: 큰 모델에서 작은 모델을 학습하는 것 (Hinton et al., 2015)
Data2vec은 teacher-student라는 방법을 사용했습니다. teacher와 student는 동일한 구조의 transformer 모델입니다. 다만 teacher는 원본 데이터를 그대로 받아서 예측하고, student는 원본을 마스킹하여 예측합니다. 그리고 student가 teacher의 representation과 동일하게 예측하도록 학습합니다. teacher는 다시 student의 가중치를 복사하여 최신상태를 유지합니다.
~~~~~~~~~~~~~~~~~~~~~


#### ☻ self-supervised learning [3]

supervised learning의 경우, labeled dataset을 사용하여 학습을 시킨다. 하지만 이러한 labeled data의 경우 비싸고, 또한 대용량 데이터를 구하기가 어렵다. 따라서 자체적으로 label을 만들어서 학습을 시키자고 제안된 방법이 self-supervised learning이다. 

좀 더 details하게 확인하면 [5,6]

> Self-supervised learning (SSL) is a method of machine learning. It learns from unlabeled sample data. It can be regarded as an intermediate form between supervised and unsupervised learning. It is based on an artificial neural network.[1] The neural network learns in two steps. First, the task is solved based on pseudo-labels which help to initialize the network weights.[2][3] Second, the actual task is performed with supervised or unsupervised learning.

이러한 self-supervised learning이 어떤식으로 사용했는지 확인해보자. 

#### ☻ self-supervised learning in CV

Rotation
rotation 시킨후 CNN에 넣어서 맞추는 것. 이 
예시에서는 
4class classification이 




#### ☻ Multimodal pre-training 
Goal: Modality가 다르더라도 동일한 self-supervised learning을 할 수 있는 방법론을 제안 
⇒ predict : contextualized representations
- modality specific: data2vec에서는 feature 추출하는 부분까지만 각각 task별로 다르게 생성


### ☺︎ Method 


<p align="center">
<img src="/images/data2vec_teacher_student.png" width="800">
</p>


#### ☻ Model architecture
인코더는 Transformer 구조를 이용했다. 각 모달리티별로 인코더의 입력을 구성하는 방식은 조금씩 다르다.

이미지 피쳐의 경우 16x16패치 영역의 픽셀을 하나의 토큰으로 인코딩하는 ViT의 전략을 이용했다.
텍스트의 경우 sub word단위로(토크나이즈) 전처리하고, 이를 임베딩 벡터로 인코딩했다.
음성의 경우, multi-layer 1-D Convolution 연산을 이용해 16kHz waveform을 50Hz의 representation으로 매핑했다.
무슨 의미인지 잘 모르지만, 음성 데이터를 고정된 입력 사이즈로 매핑하는 방법인 것 같다. (정확한 내용은 논문 참고)

#### ☻ Masking
token 단위의 일부를 masking하고 sequence를 transformer 네트워크에 공급한다.

각 modality에 따라서 다른 masking 방법을 따른다. 

CV의 경우, 
BEiT masking 방법을 따른다. image patches를 임의로 masking하는 방법을 사용하는데 인접한 블록을 masking하는 blockwise방법을 사용한다.

Speech의 경우, 
wave2vec masking 방법을 따른다. latent speech representation을 추출하기 위해 cnn 구조를 사용하고 n개의 token을 연속적으로 masking한다.

NLP의 경우, 
RoBERTa masking 방법을 따른다.
BERT에서는 한번만 random masking을 하고 모든 epoch에서 동일한 mask를 반복하지만 RoBERTa에서는 매 epoch마다 다른 masking을 수행하는 dynamic masking 방법을 사용한다. 또한 BERT와 달리 다음 문장을 예측하는 방법은 사용하지 않는다.


#### ☻ Trainning targets

**Teacher parameterization**


<img src="https://latex.codecogs.com/svg.image?\triangle&space;\leftarrow&space;\tau&space;\triangle&space;&plus;&space;(1-\tau)\theta" title="\triangle \leftarrow \tau \triangle + (1-\tau)\theta" />

### ☺︎ Experimental setup + Results 


#### ☻ Computer vision 

<p align="center">
<img src="/images/data2vec_computer_vision.png" width="600">
</p>



#### ☻ Speech processing 

<p align="center">
<img src="/images/data2vec_speech_processing.png" width="800">
</p>




#### ☻ Natural anguage processing (NLP)

<p align="center">
<img src="/images/data2vec_NLP.png" width="800">
</p>


#### ☻ Ablations


<p align="center">
<img src="/images/data2vec_predicting_target.png" width="800">
</p>


<p align="center">
<img src="/images/data2vec_contextualized_target.png" width="800">
</p>


<p align="center">
<img src="/images/data2vec_effect_of_different_features.png" width="800">
</p>


### ☺︎ Discussion

#### ☻ Modality specific features extractors and masking
NLP에서 data2vec은 target units(word/sub-word/character/byte)를 미리 정의하지 않아도 되는 첫 모델이다.
#### ☻ Structured and contextualized targets 
Representation collapse: input에 상관 없이 모두 같은(비슷한) constant vector를 뱉는 현상
논문에서 collapse가 발생하는 시나리오를 찾았다.
the learning rate is too large or the learning rate warmup is too short which can often be solved by tuning the respective hyperparameters.
τ is too low which leads to student model collapse and is then propagated to the teacher.
we found collapse to be more likely for modalities where adjacent targets are very correlated and where longer spans need to be masked, e.g., speech.



하지만 학습 알고리즘은 통합적이지만 여전히 representation은 각 modality의 양식에 대해 개별적으로 학습한다는 
하지만 여러 modality에서 general한 방법을 사용한다 했지만, data2vec의 경우도 input encoder와 masking이 modality specific하다는 한계점이 존재한다. 이는 추후 이 과정들도 모두 modality independent 하게 바꾸는 방향으로 나아가야 한다고 생각한다. 


-----------------------------------------------------------------------

### ☻ Reference
1. [Paper : data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555)
2. [Blow : Meta AI](https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/)
3. [DSBM: data2vec](http://dsba.korea.ac.kr/seminar/?mod=document&uid=1935)
4. [blog : Data2Vec Review](https://baekyeongmin.github.io/paper-review/data2vec/)
5. [wikipedia: self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning)
6. [Self-supervised learning: The dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)
7. [blog : 초간단 논문리뷰, data2vec](https://everyday-deeplearning.tistory.com/m/entry/data2vec-A-General-Framework-for-Self-supervised-Learning-in-Speech-Vision-and-Language)



### ☻ image sources
1. [Giphy](https://giphy.com/search/sesame-street)


🌺 **Thanks for reading. Hope to see you again :o)**

-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="150">
</p>

모두의 연구소에서 진행하는 "함께 콘텐츠를 제작하는 콘텐츠 크리에이터 모임"인 **COCRE(코크리)** 의 2기 회원으로 제작한 글입니다

[🐘 코크리가 궁금하다면 클릭!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)




