---
layout: post
title: Data2Vec <img src="/images/B-icon-ver.png" width="30">
---

### [Review | ë…¼ë¬¸ë¦¬ë·°] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language

ì½”ë„ˆ ì†ì˜ ì½”ë„ˆ EPA (Exploratory Paper Analysis), ë…¼ë¬¸ ë¦¬ë·° ì…ë‹ˆë‹¤.

ğŸ“ƒ [paper, 2022 [1]](https://arxiv.org/abs/2202.03555)


-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ What the data2vec is. 

â˜ºï¸ Related work 
  - Self-supervised learning in CV, NLP, and speech
  - Multimodal pre-training

â˜ºï¸ Method
  - Model architecture
  - Masking
  - Trainning targets

â˜ºï¸ Experimental setup + Results 
  - Computer vision 
  - Speech processing 
  - Natural language processing (NLP)
  - Ablations

â˜ºï¸ Discussion 
  - Modality specific features extractors and masking
  - Structured and contextualized targets 


â˜» Reference

-----------------------------------------------------------------------


### â˜ºï¸ What the data2vec is [1-4].

Data2vecì€ self-supervised learning (ìê¸°ì§€ë„í•™ìŠµ)ìœ¼ë¡œ multimodality (speech, image, text)ì—ì„œ ì ìš© ê°€ëŠ¥í•œ frameworkìœ¼ë¡œ ì´ëŠ” ê¸°ì¡´ì— supervised learning (ì§€ë„í•™ìŠµ)ìœ¼ë¡œ íŠ¹ì • ë‹¨ì¼ modalityì— ì§‘ì¤‘í•´ì„œ ê° modalityë³„ë¡œ í•™ìŠµ ë°©ë²•ì„ ë‹¤ë¥´ê²Œ í–ˆë˜ ê²ƒì— ë¹„í•´ ì¢€ ë” ì¼ë°˜ì ì¸ ë°©ë²•ìœ¼ë¡œ í™•ì¥ëœ í˜•íƒœì´ë‹¤. 



~~~~~~~~~~~~~~~~~~~~~
** ìˆ˜ì • **
í•µì‹¬ ì•„ì´ë””ì–´ëŠ” í‘œì¤€ Transformer ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ìì²´ ì¦ë¥˜ ì„¤ì •(selfdistillation setup)ì—ì„œ ì…ë ¥ì˜ ë§ˆìŠ¤í¬ëœ ë³´ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ì…ë ¥ ë°ì´í„°ì˜ ì ì¬ëœ í‘œí˜„ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

Knowledge distillationì´ë€: í° ëª¨ë¸ì—ì„œ ì‘ì€ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒ (Hinton et al., 2015)
Data2vecì€ teacher-studentë¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. teacherì™€ studentëŠ” ë™ì¼í•œ êµ¬ì¡°ì˜ transformer ëª¨ë¸ì…ë‹ˆë‹¤. ë‹¤ë§Œ teacherëŠ” ì›ë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë°›ì•„ì„œ ì˜ˆì¸¡í•˜ê³ , studentëŠ” ì›ë³¸ì„ ë§ˆìŠ¤í‚¹í•˜ì—¬ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  studentê°€ teacherì˜ representationê³¼ ë™ì¼í•˜ê²Œ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤. teacherëŠ” ë‹¤ì‹œ studentì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³µì‚¬í•˜ì—¬ ìµœì‹ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
~~~~~~~~~~~~~~~~~~~~~


#### â˜» self-supervised learning [3]

supervised learningì˜ ê²½ìš°, labeled datasetì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì‹œí‚¨ë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ labeled dataì˜ ê²½ìš° ë¹„ì‹¸ê³ , ë˜í•œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ êµ¬í•˜ê¸°ê°€ ì–´ë µë‹¤. ë”°ë¼ì„œ ìì²´ì ìœ¼ë¡œ labelì„ ë§Œë“¤ì–´ì„œ í•™ìŠµì„ ì‹œí‚¤ìê³  ì œì•ˆëœ ë°©ë²•ì´ self-supervised learningì´ë‹¤. 

ì¢€ ë” detailsí•˜ê²Œ í™•ì¸í•˜ë©´ [5,6]

> Self-supervised learning (SSL) is a method of machine learning. It learns from unlabeled sample data. It can be regarded as an intermediate form between supervised and unsupervised learning. It is based on an artificial neural network.[1] The neural network learns in two steps. First, the task is solved based on pseudo-labels which help to initialize the network weights.[2][3] Second, the actual task is performed with supervised or unsupervised learning.

ì´ëŸ¬í•œ self-supervised learningì€ Computer vision, NLP, Speechì— ì‚¬ìš©ëœ ê²½ìš°ë¥¼ í™•ì¸í•´ë³´ë©´, 

CVì˜ ê²½ìš°ëŠ” í¬ê²Œ **Rotation**ê³¼ **Context prediction**ì´ ìˆë‹¤ [3,7]. 

ìš°ì„ , rotation ì‹œí‚¨í›„ CNNì— ë„£ì–´ì„œ íšŒì „ì„ ë§ì¶”ëŠ” taskë¥¼ í‘¸ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ê²½ìš°, ë™ì¼í•œ ì´ë¯¸ì§€ê°€ ë“¤ì–´ì˜¤ëŠ”ë° ê°ë„ê°€ ë°”ë€ŒëŠ” ê²ƒì´ë‹¤. 

<p align="center">
<img src="/images/data2vec_selfsupervised_CV_rotation.png" width="800">
</p>


Context predictionì˜ ê²½ìš°ëŠ” patch ìƒì„±í•˜ê³ , ê°€ìš´ë° ì´ë¯¸ì§€ë¥¼ ì•µì»¤ì²˜ëŸ¼ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ 8ê°œ ì´ë¯¸ì§€ì¤‘ ëœë¤í•˜ê²Œ ì´ë¯¸ì§€ë¥¼ ê°–ê³ ì™€ì„œ ë‘ ì´ë¯¸ì§€ë¥¼ CNNì— ë„£ì–´ì„œ ì•µì»¤ ì œì™¸í•˜ê³  ê°–ê³ ì˜¨ query ì´ë¯¸ì§€ê°€ ëª‡ë²ˆì§¸ ì´ë¯¸ì§€ì¸ì§€ ë§ì¶°ì£¼ëŠ” taskì´ë‹¤. 
ì—¬ê¸°ì„œ ë” ë‚˜ì•„ê°„ ê²ƒì´ ë‘ê°œì˜ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ì´ 9ê°œì˜ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì‚¬ìš©í•´ì„œ CNNì— ë„£ì€ í›„ ì›ë˜ ì´ë¯¸ì§€ì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒìœ¼ë¡œ jigsaw puzzleë¼ê³  ë¶ˆë¦¬ìš´ë‹¤. 

<p align="center">
<img src="/images/data2vec_selfsupervised_CV_jigsaw_puzzle.png" width="800">
</p>


NLPì˜ ê²½ìš°ëŠ” BERT ê·¸ë¦¬ê³  Speechì˜ ê²½ìš°ëŠ” wave2vec 2.0ì´ ëŒ€í‘œì ì¸ ì˜ˆì´ë‹¤ [1].

ì´ë¥¼ í†µí•´ ê°ê°ì˜ modalityì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ë°©ë²•ë¡ ì„ ê°ê¸° ì ìš©í•´ì„œ self-supervised learningì„ ì§„í–‰í•´ ì™”ë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤. 


#### â˜» Multimodal pre-training 

Multimodalityë¼ëŠ” ê²ƒì˜ ì •ì˜ë¥¼ ë³´ë©´ [8]

> The term multimodality refers to the combination of multiple sensory and communicative modes, such as sight, sound, print, images, video, music, and so on, that produce meaning in any given message.

ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ë©´ [9]
(ì—¬ê¸°ì„œ multimodalityì™€ multimodalì€ ë™ì¼í•˜ë‹¤.) 

> ì—¬ëŸ¬ê°€ì§€ í˜•íƒœì™€ ì˜ë¯¸ë¡œ ì»´í“¨í„°ì™€ ëŒ€í™”í•˜ëŠ” í™˜ê²½ìœ¼ë¡œ ëª¨ë‹¬ (modality)ë€ ì¸í„°ë™ì…˜ ê³¼ì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì˜ì‚¬ì†Œí†µ ì±„ë„ì„ ê°€ë¥´í‚¨ë‹¤. ë©€í‹°ëª¨ë‹¬ì€ í…ìŠ¤íŠ¸, ìŒì„±, ì´ë¯¸ì§€, ì˜ìƒë“± ì„œë¡œ ë‹¤ë¥¸ ì–‘ì‹ì˜ ë°ì´í„°ë¥¼ ììœ ìì¬ë¡œ ì´í•´í•˜ê³  ë³€í™˜í•  ìˆ˜ ìˆì–´ ì‚¬ëŒì²˜ëŸ¼ ë°°ìš°ê³  ìƒê°í•˜ë©° ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤. 

ì´ëŸ¬í•œ ë©€í‹°ëª¨ë‹¬ì˜ ê²½ìš° cross-modal representationì„ ìƒì„±ì„ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤ [1].

> There has been a considerable body of research on learning representations of multiple modalities simultaneously often using paired data (Aytar et al., 2017; Radford et al., 2021b; Wang et al., 2021; Singh et al., 2021) with the aim to produce cross-modal representations which can perform well on multi-modal tasks and with modalities benefiting from each other through joint training (Alayrac et al., 2020; Akbari et al., 2021) with recent methods exploring few-shot learning (Tsimpoukelli et al., 2021). Our work does not perform multimodal training but aims to unifiy the learning objective for self-supervised learning in different modalities. We hope that this will en- able better multimodal representations in the future.


ë³¸ ì—°êµ¬ì—ì„œëŠ” Modalityê°€ ë‹¤ë¥´ë”ë¼ë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ self-supervised learningì„ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ì˜€ë‹¤. ì´ëŠ” modalityê°€ ë‹¬ë¼ì„œ ë‹¤ë¥¸ inputì´ ë“¤ì–´ì™€ë„ contextualized representationsë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. 

### â˜ºï¸ Method 


<p align="center">
<img src="/images/data2vec_teacher_student.png" width="800">
</p>


#### â˜» Model architecture
ì¸ì½”ë”ëŠ” Transformer êµ¬ì¡°ë¥¼ ì´ìš©í–ˆë‹¤. ê° ëª¨ë‹¬ë¦¬í‹°ë³„ë¡œ ì¸ì½”ë”ì˜ ì…ë ¥ì„ êµ¬ì„±í•˜ëŠ” ë°©ì‹ì€ ì¡°ê¸ˆì”© ë‹¤ë¥´ë‹¤.

ì´ë¯¸ì§€ í”¼ì³ì˜ ê²½ìš° 16x16íŒ¨ì¹˜ ì˜ì—­ì˜ í”½ì…€ì„ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì¸ì½”ë”©í•˜ëŠ” ViTì˜ ì „ëµì„ ì´ìš©í–ˆë‹¤.
í…ìŠ¤íŠ¸ì˜ ê²½ìš° sub wordë‹¨ìœ„ë¡œ(í† í¬ë‚˜ì´ì¦ˆ) ì „ì²˜ë¦¬í•˜ê³ , ì´ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ì¸ì½”ë”©í–ˆë‹¤.
ìŒì„±ì˜ ê²½ìš°, multi-layer 1-D Convolution ì—°ì‚°ì„ ì´ìš©í•´ 16kHz waveformì„ 50Hzì˜ representationìœ¼ë¡œ ë§¤í•‘í–ˆë‹¤.
ë¬´ìŠ¨ ì˜ë¯¸ì¸ì§€ ì˜ ëª¨ë¥´ì§€ë§Œ, ìŒì„± ë°ì´í„°ë¥¼ ê³ ì •ëœ ì…ë ¥ ì‚¬ì´ì¦ˆë¡œ ë§¤í•‘í•˜ëŠ” ë°©ë²•ì¸ ê²ƒ ê°™ë‹¤. (ì •í™•í•œ ë‚´ìš©ì€ ë…¼ë¬¸ ì°¸ê³ )

#### â˜» Masking
token ë‹¨ìœ„ì˜ ì¼ë¶€ë¥¼ maskingí•˜ê³  sequenceë¥¼ transformer ë„¤íŠ¸ì›Œí¬ì— ê³µê¸‰í•œë‹¤.

ê° modalityì— ë”°ë¼ì„œ ë‹¤ë¥¸ masking ë°©ë²•ì„ ë”°ë¥¸ë‹¤. 

CVì˜ ê²½ìš°, 
BEiT masking ë°©ë²•ì„ ë”°ë¥¸ë‹¤. image patchesë¥¼ ì„ì˜ë¡œ maskingí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ”ë° ì¸ì ‘í•œ ë¸”ë¡ì„ maskingí•˜ëŠ” blockwiseë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.

Speechì˜ ê²½ìš°, 
wave2vec masking ë°©ë²•ì„ ë”°ë¥¸ë‹¤. latent speech representationì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ cnn êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ê³  nê°œì˜ tokenì„ ì—°ì†ì ìœ¼ë¡œ maskingí•œë‹¤.

NLPì˜ ê²½ìš°, 
RoBERTa masking ë°©ë²•ì„ ë”°ë¥¸ë‹¤.
BERTì—ì„œëŠ” í•œë²ˆë§Œ random maskingì„ í•˜ê³  ëª¨ë“  epochì—ì„œ ë™ì¼í•œ maskë¥¼ ë°˜ë³µí•˜ì§€ë§Œ RoBERTaì—ì„œëŠ” ë§¤ epochë§ˆë‹¤ ë‹¤ë¥¸ maskingì„ ìˆ˜í–‰í•˜ëŠ” dynamic masking ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ë˜í•œ BERTì™€ ë‹¬ë¦¬ ë‹¤ìŒ ë¬¸ì¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.


#### â˜» Trainning targets

**Teacher parameterization**


<img src="https://latex.codecogs.com/svg.image?\triangle&space;\leftarrow&space;\tau&space;\triangle&space;&plus;&space;(1-\tau)\theta" title="\triangle \leftarrow \tau \triangle + (1-\tau)\theta" />

### â˜ºï¸ Experimental setup + Results 


#### â˜» Computer vision 

<p align="center">
<img src="/images/data2vec_computer_vision.png" width="600">
</p>



#### â˜» Speech processing 

<p align="center">
<img src="/images/data2vec_speech_processing.png" width="800">
</p>




#### â˜» Natural anguage processing (NLP)

<p align="center">
<img src="/images/data2vec_NLP.png" width="800">
</p>


#### â˜» Ablations


<p align="center">
<img src="/images/data2vec_predicting_target.png" width="800">
</p>


<p align="center">
<img src="/images/data2vec_contextualized_target.png" width="800">
</p>


<p align="center">
<img src="/images/data2vec_effect_of_different_features.png" width="800">
</p>


### â˜ºï¸ Discussion

#### â˜» Modality specific features extractors and masking
NLPì—ì„œ data2vecì€ target units(word/sub-word/character/byte)ë¥¼ ë¯¸ë¦¬ ì •ì˜í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” ì²« ëª¨ë¸ì´ë‹¤.
#### â˜» Structured and contextualized targets 
Representation collapse: inputì— ìƒê´€ ì—†ì´ ëª¨ë‘ ê°™ì€(ë¹„ìŠ·í•œ) constant vectorë¥¼ ë±‰ëŠ” í˜„ìƒ
ë…¼ë¬¸ì—ì„œ collapseê°€ ë°œìƒí•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì°¾ì•˜ë‹¤.
the learning rate is too large or the learning rate warmup is too short which can often be solved by tuning the respective hyperparameters.
Ï„ is too low which leads to student model collapse and is then propagated to the teacher.
we found collapse to be more likely for modalities where adjacent targets are very correlated and where longer spans need to be masked, e.g., speech.



í•˜ì§€ë§Œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ í†µí•©ì ì´ì§€ë§Œ ì—¬ì „íˆ representationì€ ê° modalityì˜ ì–‘ì‹ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ í•™ìŠµí•œë‹¤ëŠ” 
í•˜ì§€ë§Œ ì—¬ëŸ¬ modalityì—ì„œ generalí•œ ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤ í–ˆì§€ë§Œ, data2vecì˜ ê²½ìš°ë„ input encoderì™€ maskingì´ modality specificí•˜ë‹¤ëŠ” í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. ì´ëŠ” ì¶”í›„ ì´ ê³¼ì •ë“¤ë„ ëª¨ë‘ modality independent í•˜ê²Œ ë°”ê¾¸ëŠ” ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°€ì•¼ í•œë‹¤ê³  ìƒê°í•œë‹¤. 


-----------------------------------------------------------------------

### â˜» Reference
1. [Paper : data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555)
2. [Blog : Meta AI](https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/)
3. [DSBM: data2vec](http://dsba.korea.ac.kr/seminar/?mod=document&uid=1935)
4. [Blog : Data2Vec Review](https://baekyeongmin.github.io/paper-review/data2vec/)
5. [wikipedia: self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning)
6. [Self-supervised learning: The dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)
7. [Blog: Self-Supervised Representation Learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
8. [Multimodality and Language Learning](https://onlinelibrary.wiley.com/doi/10.1002/9781119472384.ch3#:~:text=The%20term%20multimodality%20refers%20to,meaning%20in%20any%20given%20message)
9. [Blog : ë©€í‹°ëª¨ë‹¬](https://blog.naver.com/PostView.naver?blogId=yckim002&logNo=222599463531&parentCategoryNo=&categoryNo=63&viewDate=&isShowPopularPosts=true&from=search)
10. [blog : ì´ˆê°„ë‹¨ ë…¼ë¬¸ë¦¬ë·°, data2vec](https://everyday-deeplearning.tistory.com/m/entry/data2vec-A-General-Framework-for-Self-supervised-Learning-in-Speech-Vision-and-Language)



### â˜» image sources
1. [Giphy](https://giphy.com/search/sesame-street)


ğŸŒº **Thanks for reading. Hope to see you again :o)**

-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="150">
</p>

ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ ì§„í–‰í•˜ëŠ” "í•¨ê»˜ ì½˜í…ì¸ ë¥¼ ì œì‘í•˜ëŠ” ì½˜í…ì¸  í¬ë¦¬ì—ì´í„° ëª¨ì„"ì¸ **COCRE(ì½”í¬ë¦¬)** ì˜ 2ê¸° íšŒì›ìœ¼ë¡œ ì œì‘í•œ ê¸€ì…ë‹ˆë‹¤

[ğŸ˜ ì½”í¬ë¦¬ê°€ ê¶ê¸ˆí•˜ë‹¤ë©´ í´ë¦­!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)




