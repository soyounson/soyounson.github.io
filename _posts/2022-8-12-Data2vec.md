---
layout: post
title: data2vec <img src="/images/B-icon-ver.png" width="30">
---

### [Review | ë…¼ë¬¸ë¦¬ë·°] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language

ì½”ë„ˆ ì†ì˜ ì½”ë„ˆ EPA (Exploratory Paper Analysis), ë…¼ë¬¸ ë¦¬ë·° ì…ë‹ˆë‹¤.

ğŸ“ƒ [paper, 2022 [1]](https://arxiv.org/abs/2202.03555)


-----------------------------------------------------------------------

â˜¾ Table of contents

â˜ºï¸ What the data2vec is. 

â˜ºï¸ Related work 
  - Self-supervised learning in CV, NLP, and speech
  - Multimodal pre-training

â˜ºï¸ Method
  - Model architecture
  - Masking
  - Trainning targets

â˜ºï¸ Experimental setup + Results 
  - Computer vision 
  - Speech processing 
  - Natural language processing (NLP)
  - Ablations

â˜ºï¸ Discussion 
  - Modality specific features extractors and masking
  - Structured and contextualized targets 
  - Representation collapse 

â˜» Reference

-----------------------------------------------------------------------


### â˜ºï¸ What the data2vec is [1-4].

<p align="center">
<img src="/images/data2vec_data.gif" width="500">
</p>


Data2vecì€ ì‰½ê²Œ ë§í•´ì„œ speech, NLP ë˜ëŠ” computer visionì— ë“±ì— ë™ì¼í•œ learning methodë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. Core ideaë¡œëŠ” standard transformer architectureë¥¼ ì‚¬ìš©í•˜ëŠ” self- distillation setupì—ì„œ ì…ë ¥ì˜ ë§ˆìŠ¤í¬ëœ ë³´ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ì…ë ¥ ë°ì´í„°ì˜ ì ì¬ì  í‘œí˜„ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì€ Modality-specific targets (words, visual tokens or units of human speech)ì„ ì˜ˆì¸¡í•˜ì˜€ì§€ë§Œ ì´ì™€ ë‹¤ë¥´ê²Œ, data2vecì€ contextualized latent representationsì„ ì˜ˆì¸¡í•œë‹¤. ì´ëŸ¬í•œ contextualized latent representationsëŠ” entire inputìœ¼ë¡œë¶€í„°ì˜ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. 

ë˜í•œ data2vecì€ ê°™ì€ êµ¬ì¡°ì˜ Transformerë¥¼ ê°–ëŠ” teacher-studentë¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ teacher modelì€ original dataë¥¼ ì…ë ¥í•˜ì—¬ representationì„ ìƒì„±í•˜ê³  student modelì€ original dataì— maskingì„ í•œ masked dataë¥¼ ì…ë ¥í•˜ì—¬ original inputì˜ representationì„ ì˜ˆì¸¡í•œë‹¤. ê·¸ í›„ teacher modelì€ ë‹¤ì‹œ studentì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³µì‚¬í•˜ì—¬ ìµœì‹ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.


#### â˜» self-supervised learning [3]

supervised learningì˜ ê²½ìš°, labeled datasetì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì‹œí‚¨ë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ labeled dataì˜ ê²½ìš° ë¹„ì‹¸ê³ , ë˜í•œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ êµ¬í•˜ê¸°ê°€ ì–´ë µë‹¤. ë”°ë¼ì„œ ìì²´ì ìœ¼ë¡œ labelì„ ë§Œë“¤ì–´ì„œ í•™ìŠµì„ ì‹œí‚¤ìê³  ì œì•ˆëœ ë°©ë²•ì´ self-supervised learningì´ë‹¤. 

ì¢€ ë” detailsí•˜ê²Œ í™•ì¸í•˜ë©´ [5,6]

> Self-supervised learning (SSL) is a method of machine learning. It learns from unlabeled sample data. It can be regarded as an intermediate form between supervised and unsupervised learning. It is based on an artificial neural network.[1] The neural network learns in two steps. First, the task is solved based on pseudo-labels which help to initialize the network weights.[2][3] Second, the actual task is performed with supervised or unsupervised learning.

ì´ëŸ¬í•œ self-supervised learningì€ Computer vision, NLP, Speechì— ì‚¬ìš©ëœ ê²½ìš°ë¥¼ í™•ì¸í•´ë³´ë©´, 

CVì˜ ê²½ìš°ëŠ” í¬ê²Œ **Rotation**ê³¼ **Context prediction**ì´ ìˆë‹¤ [3,7]. 

ìš°ì„ , rotation ì‹œí‚¨í›„ CNNì— ë„£ì–´ì„œ íšŒì „ì„ ë§ì¶”ëŠ” taskë¥¼ í‘¸ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ê²½ìš°, ë™ì¼í•œ ì´ë¯¸ì§€ê°€ ë“¤ì–´ì˜¤ëŠ”ë° ê°ë„ê°€ ë°”ë€ŒëŠ” ê²ƒì´ë‹¤. 

<p align="center">
<img src="/images/data2vec_selfsupervised_CV_rotation.png" width="600">
</p>


Context predictionì˜ ê²½ìš°ëŠ” patch ìƒì„±í•˜ê³ , ê°€ìš´ë° ì´ë¯¸ì§€ë¥¼ ì•µì»¤ì²˜ëŸ¼ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ 8ê°œ ì´ë¯¸ì§€ì¤‘ ëœë¤í•˜ê²Œ ì´ë¯¸ì§€ë¥¼ ê°–ê³ ì™€ì„œ ë‘ ì´ë¯¸ì§€ë¥¼ CNNì— ë„£ì–´ì„œ ì•µì»¤ ì œì™¸í•˜ê³  ê°–ê³ ì˜¨ query ì´ë¯¸ì§€ê°€ ëª‡ë²ˆì§¸ ì´ë¯¸ì§€ì¸ì§€ ë§ì¶°ì£¼ëŠ” taskì´ë‹¤. 
ì—¬ê¸°ì„œ ë” ë‚˜ì•„ê°„ ê²ƒì´ ë‘ê°œì˜ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ì´ 9ê°œì˜ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì‚¬ìš©í•´ì„œ CNNì— ë„£ì€ í›„ ì›ë˜ ì´ë¯¸ì§€ì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒìœ¼ë¡œ jigsaw puzzleë¼ê³  ë¶ˆë¦¬ìš´ë‹¤. 

<p align="center">
<img src="/images/data2vec_selfsupervised_CV_jigsaw_puzzle.png" width="600">
</p>


NLPì˜ ê²½ìš°ëŠ” BERT 

<p align="center">
<img src="/images/data2vec_bert.png" width="600">
</p>


speechì˜ ê²½ìš°ëŠ” wave2vec 2.0ì´ ëŒ€í‘œì ì¸ ì˜ˆì´ë‹¤ [1].

<p align="center">
<img src="/images/data2vec_wave2vec2.png" width="700">
</p>


ì´ë¥¼ í†µí•´ ê°ê°ì˜ modalityì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ë°©ë²•ë¡ ì„ ê°ê¸° ì ìš©í•´ì„œ self-supervised learningì„ ì§„í–‰í•´ ì™”ë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤. 

#### â˜» Multimodal pre-training 

Multimodalityë¼ëŠ” ê²ƒì˜ ì •ì˜ë¥¼ ë³´ë©´ [10]

> The term multimodality refers to the combination of multiple sensory and communicative modes, such as sight, sound, print, images, video, music, and so on, that produce meaning in any given message.

ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ë©´ [11]
(ì—¬ê¸°ì„œ multimodalityì™€ multimodalì€ ë™ì¼í•˜ë‹¤.) 

> ì—¬ëŸ¬ê°€ì§€ í˜•íƒœì™€ ì˜ë¯¸ë¡œ ì»´í“¨í„°ì™€ ëŒ€í™”í•˜ëŠ” í™˜ê²½ìœ¼ë¡œ ëª¨ë‹¬ (modality)ë€ ì¸í„°ë™ì…˜ ê³¼ì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì˜ì‚¬ì†Œí†µ ì±„ë„ì„ ê°€ë¥´í‚¨ë‹¤. ë©€í‹°ëª¨ë‹¬ì€ í…ìŠ¤íŠ¸, ìŒì„±, ì´ë¯¸ì§€, ì˜ìƒë“± ì„œë¡œ ë‹¤ë¥¸ ì–‘ì‹ì˜ ë°ì´í„°ë¥¼ ììœ ìì¬ë¡œ ì´í•´í•˜ê³  ë³€í™˜í•  ìˆ˜ ìˆì–´ ì‚¬ëŒì²˜ëŸ¼ ë°°ìš°ê³  ìƒê°í•˜ë©° ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤. 

ì´ëŸ¬í•œ ë©€í‹°ëª¨ë‹¬ì˜ ê²½ìš° cross-modal representationì„ ìƒì„±ì„ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤ [1].

> There has been a considerable body of research on learning representations of multiple modalities simultaneously often using paired data (Aytar et al., 2017; Radford et al., 2021b; Wang et al., 2021; Singh et al., 2021) with the aim to produce cross-modal representations which can perform well on multi-modal tasks and with modalities benefiting from each other through joint training (Alayrac et al., 2020; Akbari et al., 2021) with recent methods exploring few-shot learning (Tsimpoukelli et al., 2021). Our work does not perform multimodal training but aims to unifiy the learning objective for self-supervised learning in different modalities. We hope that this will en- able better multimodal representations in the future.


ë³¸ ì—°êµ¬ì—ì„œëŠ” Modalityê°€ ë‹¤ë¥´ë”ë¼ë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ self-supervised learningì„ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ì˜€ë‹¤. ì´ëŠ” modalityê°€ ë‹¬ë¼ì„œ ë‹¤ë¥¸ inputì´ ë“¤ì–´ì™€ë„ contextualized representationsë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. 

### â˜ºï¸ Method 

<p align="center">
<img src="/images/data2vec_teacher_student.png" width="800">
</p>

data2vecì˜ ê²½ìš° a partial view of the inputê°€ ì£¼ì–´ì§€ë©´ ì „ì²´ full input dataì˜ model representationsì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµì‹œì¼œì§„ë‹¤. 

ì²˜ìŒì— a masked version of the training sample (model in student mode)ì„ encodeí•˜ê³  ê·¸ í›„ ë™ì¼ ëª¨ë¸ë¡œ unmasked version of the inputì„ encodeí•´ì„œ training targetsì„ ë§Œë“ ë‹¤. í•˜ì§€ë§Œ an exponentially moving average of the model weightsìœ¼ë¡œ paramerize í•œë‹¤ (model in teacher mode). 


#### â˜» Model architecture

Modality-specific encodingì„ ê°–ì€ standard Transformer architectureì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ì¦‰, ì…ë ¥ì˜ encodingë°©ë²•ì€ modality ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ëª¨ë¸ êµ¬ì¡°ë‚˜ í•™ìŠµ ë°©ë²•ìì²´ëŠ” í†µí•©ëœ í˜•íƒœë¥¼ ë„ê³  ìˆë‹¤.

#### â˜» Masking [12]

Input sampleì´ a sequence of tokensìœ¼ë¡œ ì„ë°°ë”© ëœ í›„ì—, a learned MASK embedding tokenìœ¼ë¡œ ë°”ê¿ˆìœ¼ë¡œì¨ maskingí•˜ê³  sequenceë¥¼ transformer ë„¤íŠ¸ì›Œí¬ì— ê³µê¸‰í•œë‹¤. ì´ë•Œ modalityì— ë”°ë¼ì„œ ë‹¤ë¥¸ maskingë°©ë²•ì„ ê³ ë ¤í•œë‹¤.

CVì˜ ê²½ìš°, BEiT masking ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ì´ëŠ” image patchesë¥¼ ì„ì˜ë¡œ maskingí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ”ë° ì¸ì ‘í•œ ë¸”ë¡ì„ maskingí•˜ëŠ” blockwiseë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.

Speechì˜ ê²½ìš°, wave2vec masking ë°©ë²•ì„ ì‚¬ìš©í•˜ê³ , ì´ë•Œ latent speech representationì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ cnn êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ê³  nê°œì˜ tokenì„ ì—°ì†ì ìœ¼ë¡œ maskingí•œë‹¤.

NLPì˜ ê²½ìš°, RoBERTa masking ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. BERTëŠ” í•œë²ˆë§Œ random maskingì„ í•˜ê³  ëª¨ë“  epochì—ì„œ ë™ì¼í•œ maskë¥¼ ë°˜ë³µí•˜ì§€ë§Œ RoBERTaì—ì„œëŠ” ë§¤ epochë§ˆë‹¤ ë‹¤ë¥¸ maskingì„ ìˆ˜í–‰í•˜ëŠ” dynamic masking ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ë˜í•œ BERTì™€ ë‹¬ë¦¬ ë‹¤ìŒ ë¬¸ì¥ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.



==============================================================================

#### â˜» Trainning targets

An encoding of the masked sample ì„ ê¸°ë°˜ìœ¼ë¡œ original unmasked training sampleì˜ model representationsì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ë‹¤. 

**Teacher parameterization**
Distillationì—ì„œëŠ” ì´ ë‘ ê°œì˜ parametersì„ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ì§€ê°€ ì¤‘ìš”í•˜ë‹¤. 
BYOLì—ì„œ ì œì•ˆí•œ exponentially moving averageë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. 
ì—¬ê¸°ì„œ ğ™ëŠ” teachre model wightì´ê³  ğ·ëŠ” student model weightì´ë‹¤. 
ìœ„ì—ì„œ ì ê¹ ì–¸ê¸‰í–ˆë“¯ì´, Teacher ëª¨ë“œëŠ” ëª¨ë¸ íŒŒë¼ë©”í„°ì˜ Exponentially Moving Average(EMA)ë¡œ êµ¬ì„±ëœë‹¤. Teacherì˜ ê°€ì¤‘ì¹˜ Î” ë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ Î”â†Ï„Î”+(1âˆ’Ï„)Î¸ ì™€ ê°™ë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” Ï„ë¥¼ constantë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³  í•™ìŠµ ì§„í–‰ì •ë„ì—ë”°ë¼ ìŠ¤ì¼€ì¥´ë§í–ˆë‹¤. í•™ìŠµì˜ ì²« në²ˆì˜ ì—…ë°ì´íŠ¸ ë™ì•ˆ Ï„0ì—ì„œ Ï„e ê¹Œì§€ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ê³ , ë‚¨ì€ í•™ìŠµë™ì•ˆì€ ìƒìˆ˜ë¡œ ìœ ì§€í•œë‹¤. ì´ëŠ” í•™ìŠµì˜ ì‹œì‘ë¶€ì—ì„œëŠ” íŒŒë¼ë©”í„°ê°€ ëœë¤ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ìˆê¸° ë•Œë¬¸ì—, Ï„ë¥¼ ì‘ê²Œí•˜ì—¬ ë³€í™”ëŸ‰ì„ ë” í¬ê²Œ ë°˜ì˜í•˜ê³ , íŒŒë¼ë©”í„°ê°€ ì–´ëŠì •ë„ í•™ìŠµë˜ê³  ë‚œ í›„ì—ëŠ” Ï„ë¥¼ í¬ê²Œí•˜ì—¬, ë³€í™”ëŸ‰ì„ ì ê²Œ ë°˜ì˜í•˜ê¸° ìœ„í•¨ì´ë‹¤.

ì‹¤í—˜ì—ì„œ í”¼ì³ ì¸ì½”ë”ì™€ positional encoderì˜ ê²½ìš° EMAë¥¼ í•˜ì§€ ì•Šê³ , teacherì™€ studentì—ì„œ ê³µìœ í•˜ëŠ” ê²ƒì´ ì¡°ê¸ˆ ë” íš¨ìœ¨ì ì´ê³ , ì •í™•í•˜ë‹¤ê³  í•œë‹¤. ì½”ë“œ êµ¬í˜„ì„ í™•ì¸í•´ë³´ë©´, ema_transformer_layers_only ì¸ìë¥¼ ì´ìš©í•´ trasnformer layer ì´ì™¸ì˜ ì—°ì‚°ë“¤(positional encoder, layer normalization ë“±)ì„ EMAì˜ ëŒ€ìƒì—ì„œ ì œì™¸ì‹œì¼œ ì£¼ëŠ” ë¡œì§ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.


<img src="https://latex.codecogs.com/svg.image?\triangle&space;\leftarrow&space;\tau&space;\triangle&space;&plus;&space;(1-\tau)\theta" title="\triangle \leftarrow \tau \triangle + (1-\tau)\theta" />

**Target**

<img src="https://latex.codecogs.com/svg.image?y_{t}&space;=&space;\frac{1}{K}\sum_{l=L-K&plus;1}^{L}\hat{a}_t^l" title="y_{t} = \frac{1}{K}\sum_{l=L-K+1}^{L}\hat{a}_t^l" />



==============================================================================

**Objectives**
Time step, tì—ì„œ íƒ€ê²Ÿ(yt)ê³¼ model predicton (ì¦‰, Studentì˜ ì¶œë ¥ê°’) (ft)ì— ëŒ€í•´ Smooth L1 lossë¡œ regressionì„ ìˆ˜í–‰í•œë‹¤.

<p align="center">
<img src="/images/data2vec_objectives.png" width="300">
</p>

ì´ losssì˜ ê²½ìš° outlierì— ëœ ë¯¼ê°í•˜ë‹¤ëŠ” ì´ì ì„ ê°–ê³  ìˆì§€ë§Œ, Î²ë¥¼ ì˜ íŠœë‹ í•´ì•¼í•œë‹¤.


### â˜ºï¸ Experimental setup + Results 

ì—¬ê¸°ì„œëŠ” ë‘ê°œì˜ ëª¨ë¸ data2vec Baseì™€ data2vec Largeë¥¼ ê³ ë ¤í•˜ì˜€ë‹¤. ë‘ ëª¨ë¸ì˜ Transformer blocks (L)ê³¼ hidden dimension(H)ëŠ” 12 x 768 í˜¹ì€ 24 x 1024 ì´ë‹¤. 

ê° ëª¨ë‹¬ì˜ hyperparamters, paramter tuningì€ ë…¼ë¬¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤. 

#### â˜» Computer vision 

ImageNet-1K training setìœ¼ë¡œ data2vecë¥¼ pretrainí•˜ê³  ë™ì¼í•œ ë²¤ì¹˜ë§ˆí¬ì˜ labeled dataì‚¬ìš©í•´ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê²°ê³¼ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì˜€ë‹¤. 
ì•„ë˜ í‘œì— ë‚˜ì˜¤ëŠ” ë°”ì™€ ê°™ì´ ë‹¤ë¥¸ ëª¨ë¸ì— ë¹„í•˜ì—¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. 

<p align="center">
<img src="/images/data2vec_computer_vision.png" width="300">
</p>



#### â˜» Speech processing 

Librispeech (LS-960)ì—ì„œ ì–»ì€ 960 hrs ì˜ speech audio dataë¡œ data2vecë¥¼ pre-trainí•˜ì˜€ë‹¤. ê·¸ë¦¬ê³  ë‹¤ë¥¸ ì–‘ì˜ labeled dataë¥¼ ì´ìš©í•´ì„œ automic speech recognitionì„ ìœ„í•´ì„œ ëª¨ë¸ì„ fine-tuneí•˜ì˜€ë‹¤. 

ìœ ëª…í•œ ì•Œê³ ë¦¬ì¦˜ì¸ wav2vec 2.0 (Baevski et al., 2020b)ì™€ HuBERT (Hsu et al., 2021)ì™€ ë¹„êµí•˜ì˜€ë‹¤. 

<p align="center">
<img src="/images/data2vec_speech_processing.png" width="600">
</p>


#### â˜» Natural anguage processing (NLP)

ë³¸ ì—°êµ¬ì—ì„œëŠ” BERTì™€ ê°™ì€ training setì„ ì ìš©í•˜ì˜€ë‹¤. 

> we adopt the same training setup as BERT (Devlin et al., 2019)by pre-training on the Books Corpus (Zhu et al., 2015) and
English Wikipedia data over 1M updates and a batch size of 256 sequences. We evaluate on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) which includes tasks for natural language inference (MNLI, QNLI, RTE), sentence similarity (MRPC, QQP and STS-B), grammaticality (CoLA), and sentiment analysis (SST-2). 

<p align="center">
<img src="/images/data2vec_NLP.png" width="800">
</p>


<!--- 

#### â˜» Ablations


<p align="center">
<img src="/images/data2vec_predicting_target.png" width="800">
</p>


<p align="center">
<img src="/images/data2vec_contextualized_target.png" width="300">
</p>


<p align="center">
<img src="/images/data2vec_effect_of_different_features.png" width="300">
</p>

-->

### â˜ºï¸ Discussion

ê¸°ì¡´ ì—°êµ¬ì™€ ë¹„êµí•˜ì˜€ì„ë•Œ ë³¸ ì—°êµ¬ì˜ íŠ¹ì§•ì„ ê¸°ìˆ í•˜ê³ ì í•œë‹¤. 

#### â˜» Modality specific features extractors and masking

- ë³¸ ì—°êµ¬ì˜ ê°€ì¥ í° ëª©ì ì€ ë‹¤ë¥¸ modalityì— ëŒ€í•´ì„œ single learning mechanismì„ ì„¤ê³„í•˜ëŠ” ê²ƒì´ë‹¤. 
- unified learning regimeì´ ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì—¬ì „íˆ íŠ¹ì • ë¶€ë¶„, modality-specific features extractors/masking strategiesì—ì„œëŠ” modality-specificí•˜ë‹¤. ì¦‰, ì—¬ì „íˆ modality dependentí•˜ê²Œ ì ìš©í•´ì¤˜ì•¼ í•˜ëŠ” ë¶€ë¶„ì´ ì¡´ì¬í•œë‹¤. 
- ì´ëŸ¬í•œ ì˜ˆë¡œ, speechì˜ inputì˜ ê²½ìš°ëŠ” very high resolution input (16kHz waveform)ì´ í•„ìš”í•œë° ë¹„í•´ NLPì˜ ê²½ìš°ëŠ” lower resolution (in the form of much shorter word sequences)ì´ë‹¤. ë”°ë¼ì„œ modalityì— ë”°ë¼ ë‹¤ë¥¸ inputì„ ê³ ë ¤í•´ì¤˜ì•¼ í•œë‹¤. 

#### â˜» Structured and contextualized targets 

- ë‹¤ë¥¸ masked prediction ì—°êµ¬ë“¤ê³¼ì˜ í° ì°¨ì´ì ì€ contextualized featureë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì— ìˆë‹¤. 
- NLPì˜ ê²½ìš°,data2vecì€ pre-defined target unitsì— ì˜ì¡´í•˜ì§€ ì•Šì€ ì²«ë²ˆì§¸ ì—°êµ¬ì´ë‹¤. 

> Most other work uses either words, sub-words (Radford et al., 2018; Devlin et al., 2019), characters (Tay et al., 2021) or even bytes (Xue et al., 2021). Aside, defining word boundaries is not straightforward for some Asian languages. Contextualized targets enable inte- grating features from the entire sequence into the training target which provides a richer self-supervised task.

#### â˜» Representation collapse 

- Self-supervised learningì²˜ëŸ¼ ìê¸° ìì‹ ì˜ targetsì„ ë°°ìš°ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ í”í•œ ì´ìŠˆì¤‘ í•˜ë‚˜ê°€ representation collapseì´ë‹¤. 
- ì´ëŠ” ëª¨ë¸ì´ all masked segmentsì— ëŒ€í•´ì„œ ë§¤ìš° ìœ ì‚¬í•œ representationì„ ìƒì„±í•œë‹¤. 
- ì´ëŸ¬í•œ collapseê°€ ë°œìƒí•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¡œëŠ” 
  - (1) the learning rate is too large or the learning rate warmup is too short which can often be solved by tuning the respective hyperparameters.
  - (2) Ï„ is too low which leads to student model collapse and is then prop- agated to the teacher. This can be addressed by tuning Ï„0, Ï„e and Ï„n. 
  - (3) adjacent targets are very correlated and where longer spans need to be masked, e.g., speech. We address this by promoting variance through normalizing tar- get representations over the sequence or batch (Grill et al., 2020). For models where targets are less correlated, such as vision and NLP, momentum tracking is sufficient.

ì´ëŸ¬í•œ collapseì˜ í•˜ë‚˜ë¡œ ê°€ì¥ ë§ì´ ê±°ë¡ ë˜ëŠ” ê²ƒì´ GANì˜ mode collapseê°€ ìˆë‹¤. 

Mode collapseë€ GANì„ í•™ìŠµì‹œí‚¬ë•Œ ìƒì„±ì(Generator)ê°€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ëª»í•˜ê³  ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë§Œ ê³„ì†ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤ [13]. 

> However, if a generator produces an especially plausible output, the generator may learn to produce only that output. In fact, the generator is always trying to find the one output that seems most plausible to the discriminator.
If the generator starts producing the same output (or a small set of outputs) over and over again, the discriminator's best strategy is to learn to always reject that output. But if the next generation of discriminator gets stuck in a local minimum and doesn't find the best strategy, then it's too easy for the next generator iteration to find the most plausible output for the current discriminator [14].


<p align="center">
<img src="/images/data2vec_GAN_mode_collapse.png" width="500">
</p>


í•˜ì§€ë§Œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ í†µí•©ì ì´ì§€ë§Œ ì—¬ì „íˆ representationì€ ê° modalityì˜ ì–‘ì‹ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ í•™ìŠµí•œë‹¤ëŠ” 
í•˜ì§€ë§Œ ì—¬ëŸ¬ modalityì—ì„œ generalí•œ ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤ í–ˆì§€ë§Œ, data2vecì˜ ê²½ìš°ë„ input encoderì™€ maskingì´ modality specificí•˜ë‹¤ëŠ” í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. ì´ëŠ” ì¶”í›„ ì´ ê³¼ì •ë“¤ë„ ëª¨ë‘ modality independent í•˜ê²Œ ë°”ê¾¸ëŠ” ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°€ì•¼ í•œë‹¤ê³  ìƒê°í•œë‹¤. 


-----------------------------------------------------------------------

### â˜» Reference
1. [Paper: data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555)
2. [Blog: Meta AI](https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/)
3. [DSBM: data2vec](http://dsba.korea.ac.kr/seminar/?mod=document&uid=1935)
4. [Blog: Data2Vec Review](https://baekyeongmin.github.io/paper-review/data2vec/)
5. [wikipedia: self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning)
6. [Self-supervised learning: The dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)
7. [Blog: Self-Supervised Representation Learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
8. [paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
9. [paper: wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/pdf/2006.11477.pdf)
10. [Multimodality and Language Learning](https://onlinelibrary.wiley.com/doi/10.1002/9781119472384.ch3#:~:text=The%20term%20multimodality%20refers%20to,meaning%20in%20any%20given%20message)
11. [Blog: ë©€í‹°ëª¨ë‹¬](https://blog.naver.com/PostView.naver?blogId=yckim002&logNo=222599463531&parentCategoryNo=&categoryNo=63&viewDate=&isShowPopularPosts=true&from=search)
12. [blog: ì´ˆê°„ë‹¨ ë…¼ë¬¸ë¦¬ë·°, data2vec](https://everyday-deeplearning.tistory.com/m/entry/data2vec-A-General-Framework-for-Self-supervised-Learning-in-Speech-Vision-and-Language)
13. [blog: GAN model collapse](https://developer-ping9.tistory.com/108#:~:text=GAN%20%2D%20Mode%20Collapse,-Studying%20Ping9_%202020&text=%EC%83%9D%EC%84%B1%EC%9E%90(Generator)%EA%B0%80%20%EB%8B%A4%EC%96%91%ED%95%9C%20%EC%9D%B4%EB%AF%B8%EC%A7%80,%EC%9D%B4%EB%A5%BC%20Mode%20Collapse%EB%9D%BC%20%EC%B9%AD%ED%95%9C%EB%8B%A4)
14. [blog: GAN, Common problems](https://developers.google.com/machine-learning/gan/problems)
15. [paper: UNROLLED GENERATIVE ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1611.02163.pdf)



### â˜» image sources
1. [Giphy](https://giphy.com/search/sesame-street)


ğŸŒˆ **Thanks for reading. Hope to see you again :o)**

-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="100">
</p>

ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ ì§„í–‰í•˜ëŠ” "í•¨ê»˜ ì½˜í…ì¸ ë¥¼ ì œì‘í•˜ëŠ” ì½˜í…ì¸  í¬ë¦¬ì—ì´í„° ëª¨ì„"ì¸ **COCRE(ì½”í¬ë¦¬)** ì˜ 2ê¸° íšŒì›ìœ¼ë¡œ ì œì‘í•œ ê¸€ì…ë‹ˆë‹¤

[ğŸ˜ ì½”í¬ë¦¬ê°€ ê¶ê¸ˆí•˜ë‹¤ë©´ í´ë¦­!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)

