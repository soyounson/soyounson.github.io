---
layout: post
title: Data2Vec <img src="/images/B-icon-ver.png" width="30">
---

### [Review | 논문리뷰] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language

코너 속의 코너 EPA (Exploratory Paper Analysis), 논문 리뷰 입니다.

📃 [paper, 2022 [1]](https://arxiv.org/abs/2202.03555)


-----------------------------------------------------------------------

☾ Table of contents

☺︎ What the data2vec is. 

☺︎ Related work 
  - Self-supervised learning in CV, NLP, and speech
  - Multimodal pre-training

☺︎ Method
  - Model architecture
  - Masking
  - Trainning targets

☺︎ Experimental setup + Results 
  - Computer vision 
  - Speech processing 
  - Natural language processing (NLP)
  - Ablations

☺︎ Discussion 
  - Modality specific features extractors and masking
  - Structured and contextualized targets 


☻ Reference

-----------------------------------------------------------------------


### ☺︎ What the data2vec is [1-4].

Data2vec은 self-supervised learning (자기지도학습)으로 multimodality (speech, image, text)에서 적용 가능한 framework으로 이는 기존에 supervised learning (지도학습)으로 특정 단일 modality에 집중해서 각 modality별로 학습 방법을 다르게 했던 것에 비해 좀 더 일반적인 방법으로 확장된 형태이다. 



~~~~~~~~~~~~~~~~~~~~~
** 수정 **
핵심 아이디어는 표준 Transformer 아키텍처를 사용하는 자체 증류 설정(selfdistillation setup)에서 입력의 마스크된 보기를 기반으로 전체 입력 데이터의 잠재된 표현을 예측하는 것입니다.

Knowledge distillation이란: 큰 모델에서 작은 모델을 학습하는 것 (Hinton et al., 2015)
Data2vec은 teacher-student라는 방법을 사용했습니다. teacher와 student는 동일한 구조의 transformer 모델입니다. 다만 teacher는 원본 데이터를 그대로 받아서 예측하고, student는 원본을 마스킹하여 예측합니다. 그리고 student가 teacher의 representation과 동일하게 예측하도록 학습합니다. teacher는 다시 student의 가중치를 복사하여 최신상태를 유지합니다.
~~~~~~~~~~~~~~~~~~~~~


#### ☻ self-supervised learning [3]

supervised learning의 경우, labeled dataset을 사용하여 학습을 시킨다. 하지만 이러한 labeled data의 경우 비싸고, 또한 대용량 데이터를 구하기가 어렵다. 따라서 자체적으로 label을 만들어서 학습을 시키자고 제안된 방법이 self-supervised learning이다. 

좀 더 details하게 확인하면 [5,6]

> Self-supervised learning (SSL) is a method of machine learning. It learns from unlabeled sample data. It can be regarded as an intermediate form between supervised and unsupervised learning. It is based on an artificial neural network.[1] The neural network learns in two steps. First, the task is solved based on pseudo-labels which help to initialize the network weights.[2][3] Second, the actual task is performed with supervised or unsupervised learning.

이러한 self-supervised learning은 Computer vision, NLP, Speech에 사용된 경우를 확인해보면, 

CV의 경우는 크게 **Rotation**과 **Context prediction**이 있다 [3,7]. 

우선, rotation 시킨후 CNN에 넣어서 회전을 맞추는 task를 푸는 것이다. 이런 경우, 동일한 이미지가 들어오는데 각도가 바뀌는 것이다. 

<p align="center">
<img src="/images/data2vec_selfsupervised_CV_rotation.png" width="600">
</p>


Context prediction의 경우는 patch 생성하고, 가운데 이미지를 앵커처럼 사용하고 나머지 8개 이미지중 랜덤하게 이미지를 갖고와서 두 이미지를 CNN에 넣어서 앵커 제외하고 갖고온 query 이미지가 몇번째 이미지인지 맞춰주는 task이다. 
여기서 더 나아간 것이 두개의 이미지가 아닌 총 9개의 이미지를 모두 사용해서 CNN에 넣은 후 원래 이미지의 위치를 찾아내는 것으로 jigsaw puzzle라고 불리운다. 

<p align="center">
<img src="/images/data2vec_selfsupervised_CV_jigsaw_puzzle.png" width="600">
</p>


NLP의 경우는 BERT 

<p align="center">
<img src="/images/data2vec_bert.png" width="600">
</p>


speech의 경우는 wave2vec 2.0이 대표적인 예이다 [1].

<p align="center">
<img src="/images/data2vec_wave2vec2.png" width="700">
</p>


이를 통해 각각의 modality에서 서로 다른 방법론을 각기 적용해서 self-supervised learning을 진행해 왔다는 것을 확인 할 수 있었다. 

#### ☻ Multimodal pre-training 

Multimodality라는 것의 정의를 보면 [10]

> The term multimodality refers to the combination of multiple sensory and communicative modes, such as sight, sound, print, images, video, music, and so on, that produce meaning in any given message.

좀 더 자세히 살펴보면 [11]
(여기서 multimodality와 multimodal은 동일하다.) 

> 여러가지 형태와 의미로 컴퓨터와 대화하는 환경으로 모달 (modality)란 인터랙션 과정에서 사용되는 의사소통 채널을 가르킨다. 멀티모달은 텍스트, 음성, 이미지, 영상등 서로 다른 양식의 데이터를 자유자재로 이해하고 변환할 수 있어 사람처럼 배우고 생각하며 추론할 수 있다. 

이러한 멀티모달의 경우 cross-modal representation을 생성을 목적으로 사용될 수 있다 [1].

> There has been a considerable body of research on learning representations of multiple modalities simultaneously often using paired data (Aytar et al., 2017; Radford et al., 2021b; Wang et al., 2021; Singh et al., 2021) with the aim to produce cross-modal representations which can perform well on multi-modal tasks and with modalities benefiting from each other through joint training (Alayrac et al., 2020; Akbari et al., 2021) with recent methods exploring few-shot learning (Tsimpoukelli et al., 2021). Our work does not perform multimodal training but aims to unifiy the learning objective for self-supervised learning in different modalities. We hope that this will en- able better multimodal representations in the future.


본 연구에서는 Modality가 다르더라도 동일한 방식으로 self-supervised learning을 할 수 있는 방법론을 제안하였다. 이는 modality가 달라서 다른 input이 들어와도 contextualized representations를 예측할 수 있다. 

### ☺︎ Method 

<p align="center">
<img src="/images/data2vec_teacher_student.png" width="800">
</p>


#### ☻ Model architecture
인코더는 Transformer 구조를 이용했다. 각 모달리티별로 인코더의 입력을 구성하는 방식은 조금씩 다르지만 모델 구조나 학습 방법자체는 통합된 형태를 띄고 있다.

==============================================================================
#### ☻ Masking

각 모달리티의 입력이 토큰의 시퀀스(Transformer 입력의 형태)로 임배딩 되었을 때, Student의 입력으로 이용하기 위해 마스킹을 수행한다. 마스킹은 시퀀스(입력 단위)의 일부를 학습가능한 MASK 토큰 임베딩으로 대체한다.


token 단위의 일부를 masking하고 sequence를 transformer 네트워크에 공급한다.

각 modality에 따라서 다른 masking 방법을 따른다 [1,12]. 

CV의 경우, 
BEiT masking 방법을 따른다. image patches를 임의로 masking하는 방법을 사용하는데 인접한 블록을 masking하는 blockwise방법을 사용한다.

Speech의 경우, 
wave2vec masking 방법을 따른다. latent speech representation을 추출하기 위해 cnn 구조를 사용하고 n개의 token을 연속적으로 masking한다.

NLP의 경우, 
RoBERTa masking 방법을 따른다. BERT에서는 한번만 random masking을 하고 모든 epoch에서 동일한 mask를 반복하지만 RoBERTa에서는 매 epoch마다 다른 masking을 수행하는 dynamic masking 방법을 사용한다. 또한 BERT와 달리 다음 문장을 예측하는 방법은 사용하지 않는다.


#### ☻ Trainning targets

**Teacher parameterization**
Distillation에서는 이 두 개의 parameters을 어떻게 학습하는지가 중요하다. 
BYOL에서 제안한 exponentially moving average방법을 사용한다. 
여기서 𝝙는 teachre model wight이고 𝝷는 student model weight이다. 

<img src="https://latex.codecogs.com/svg.image?\triangle&space;\leftarrow&space;\tau&space;\triangle&space;&plus;&space;(1-\tau)\theta" title="\triangle \leftarrow \tau \triangle + (1-\tau)\theta" />

**Target**

<img src="https://latex.codecogs.com/svg.image?y_{t}&space;=&space;\frac{1}{K}\sum_{l=L-K&plus;1}^{L}\hat{a}_t^l" title="y_{t} = \frac{1}{K}\sum_{l=L-K+1}^{L}\hat{a}_t^l" />



==============================================================================


### ☺︎ Experimental setup + Results 

여기서는 두개의 모델 data2vec Base와 data2vec Large를 고려하였다. 두 모델의 Transformer blocks (L)과 hidden dimension(H)는 12 x 768 혹은 24 x 1024 이다. 

각 모달의 hyperparamters, paramter tuning은 논문에서 확인 가능하다. 

#### ☻ Computer vision 

ImageNet-1K training set으로 data2vec를 pretrain하고 동일한 벤치마크의 labeled data사용해서 이미지 분류를 위한 결과 모델을 미세 조정하였다. 
아래 표에 나오는 바와 같이 다른 모델에 비하여 더 좋은 성능을 보여준다. 

<p align="center">
<img src="/images/data2vec_computer_vision.png" width="300">
</p>


==============================================================================

#### ☻ Speech processing 



For speech processing, we pre-train data2vec on the 960 hours of speech audio data from Librispeech (LS-960). This dataset contains relatively clean speech audio from read audiobooks in English and is a standard benchmark in the speech community. To get a sense of performance in dif- ferent resource settings, we fine-tune models for automatic speech recognition using different amounts of labeled data, ranging from just 10 minutes to 960 hours. We also com- pare to other work from the literature, including wav2vec 2.0 (Baevski et al., 2020b) and HuBERT (Hsu et al., 2021), two popular algorithms for speech representation learning relying on discrete units of speech.


<p align="center">
<img src="/images/data2vec_speech_processing.png" width="600">
</p>



#### ☻ Natural anguage processing (NLP)


we adopt the same training setup as BERT (Devlin et al., 2019)by pre-training on the Books Corpus (Zhu et al., 2015) and
English Wikipedia data over 1M updates and a batch size
of 256 sequences. We evaluate on the General Language
Understanding Evaluation (GLUE) benchmark (Wang et al.,
2018) which includes tasks for natural language inference
(MNLI, QNLI, RTE), sentence similarity (MRPC, QQP and
STS-B

<p align="center">
<img src="/images/data2vec_NLP.png" width="800">
</p>


#### ☻ Ablations


<p align="center">
<img src="/images/data2vec_predicting_target.png" width="800">
</p>


<p align="center">
<img src="/images/data2vec_contextualized_target.png" width="300">
</p>


<p align="center">
<img src="/images/data2vec_effect_of_different_features.png" width="300">
</p>

==============================================================================

### ☺︎ Discussion
#### ☻ Modality specific features extractors and masking
NLP에서 data2vec은 target units(word/sub-word/character/byte)를 미리 정의하지 않아도 되는 첫 모델이다.
- 본 연구의 목적은 단일 학습 메커니즘을 디자인 하는 것임
- 그럼에도 불구하고 modality마다 다른 feature extractor와 masking 방식을 사용하고 있음 - 저자들은 input의 특성이 매우 다르기 때문에 충분히 의미가 있다고 주장함


#### ☻ Structured and contextualized targets 

- 본 연구가 선행 연구와 다른 핵심은 contextualized feature를 학습하는 것에 있음
- 저자들에 따르면 NLP에서는 pre-defined target unit에 의존하지 않는 최초의 연구임
- Vision에서는 DINO와 BYOL이 있지만 이들은 transformation-invariant representation에 집중함

#### Representation collapse 

- Self-supervised learning의 문제 중 하나는 representation collapse임 
- 이는 매우 비슷한 representation을 생성하는 것을 의미함
- Mode collapse problem in GANs[8]
- 본 연구에서는 아래의 상황을 제외하면 representation collapse가 쉽게 발생하지 않음
- 1. learning rate가 너무 크거나 warmup을 충분히 하지 않았을 경우
- 2. EMA decay rate가 너무 낮은 경우
- 3. 인접 target끼리 상관관계가 높은 경우(speech)

Representation collapse: input에 상관 없이 모두 같은(비슷한) constant vector를 뱉는 현상
논문에서 collapse가 발생하는 시나리오를 찾았다.
the learning rate is too large or the learning rate warmup is too short which can often be solved by tuning the respective hyperparameters.
τ is too low which leads to student model collapse and is then propagated to the teacher.
we found collapse to be more likely for modalities where adjacent targets are very correlated and where longer spans need to be masked, e.g., speech.





하지만 학습 알고리즘은 통합적이지만 여전히 representation은 각 modality의 양식에 대해 개별적으로 학습한다는 
하지만 여러 modality에서 general한 방법을 사용한다 했지만, data2vec의 경우도 input encoder와 masking이 modality specific하다는 한계점이 존재한다. 이는 추후 이 과정들도 모두 modality independent 하게 바꾸는 방향으로 나아가야 한다고 생각한다. 


-----------------------------------------------------------------------

### ☻ Reference
1. [Paper: data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555)
2. [Blog: Meta AI](https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/)
3. [DSBM: data2vec](http://dsba.korea.ac.kr/seminar/?mod=document&uid=1935)
4. [Blog: Data2Vec Review](https://baekyeongmin.github.io/paper-review/data2vec/)
5. [wikipedia: self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning)
6. [Self-supervised learning: The dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)
7. [Blog: Self-Supervised Representation Learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
8. [paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
9. [paper: wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/pdf/2006.11477.pdf)
10. [Multimodality and Language Learning](https://onlinelibrary.wiley.com/doi/10.1002/9781119472384.ch3#:~:text=The%20term%20multimodality%20refers%20to,meaning%20in%20any%20given%20message)
11. [Blog : 멀티모달](https://blog.naver.com/PostView.naver?blogId=yckim002&logNo=222599463531&parentCategoryNo=&categoryNo=63&viewDate=&isShowPopularPosts=true&from=search)
12. [blog : 초간단 논문리뷰, data2vec](https://everyday-deeplearning.tistory.com/m/entry/data2vec-A-General-Framework-for-Self-supervised-Learning-in-Speech-Vision-and-Language)



### ☻ image sources
1. [Giphy](https://giphy.com/search/sesame-street)


🌺 **Thanks for reading. Hope to see you again :o)**

-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="150">
</p>

모두의 연구소에서 진행하는 "함께 콘텐츠를 제작하는 콘텐츠 크리에이터 모임"인 **COCRE(코크리)** 의 2기 회원으로 제작한 글입니다

[🐘 코크리가 궁금하다면 클릭!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)




