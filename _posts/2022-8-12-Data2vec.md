---
layout: post
title: Data2Vec <img src="/images/B-icon-ver.png" width="30">
---

### [Review | 논문리뷰] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language

코너 속의 코너 EPA (Exploratory Paper Analysis), 논문 리뷰 입니다.

📃 [paper, 2022 [1]](https://arxiv.org/abs/2202.03555)


-----------------------------------------------------------------------

☾ Table of contents

☺︎ What the data2vec is. 

☺︎ Related work 
  - Self-supervised learning in CV, NLP, and speech
  - Multimodal pre-training

☺︎ Method
  - Model architecture
  - Masking
  - Trainning targets

☺︎ Experimental setup + Results 
  - Computer vision 
  - Speech processing 
  - Natural language processing (NLP)
  - Ablations

☺︎ Discussion 
  - Modality specific features extractors and masking
  - Structured and contextualized targets 


☻ Reference

-----------------------------------------------------------------------


### ☺︎ What the data2vec is [1-4].

Data2vec은 self-supervised learning (자기지도학습)으로 multimodality (speech, image, text)에서 적용 가능한 framework으로 이는 기존에 supervised learning (지도학습)으로 특정 단일 modality에 집중해서 각 modality별로 학습 방법을 다르게 했던 것에 비해 좀 더 일반적인 방법으로 확장된 형태이다. 



~~~~~~~~~~~~~~~~~~~~~
** 수정 **
핵심 아이디어는 표준 Transformer 아키텍처를 사용하는 자체 증류 설정(selfdistillation setup)에서 입력의 마스크된 보기를 기반으로 전체 입력 데이터의 잠재된 표현을 예측하는 것입니다.

Knowledge distillation이란: 큰 모델에서 작은 모델을 학습하는 것 (Hinton et al., 2015)
Data2vec은 teacher-student라는 방법을 사용했습니다. teacher와 student는 동일한 구조의 transformer 모델입니다. 다만 teacher는 원본 데이터를 그대로 받아서 예측하고, student는 원본을 마스킹하여 예측합니다. 그리고 student가 teacher의 representation과 동일하게 예측하도록 학습합니다. teacher는 다시 student의 가중치를 복사하여 최신상태를 유지합니다.
~~~~~~~~~~~~~~~~~~~~~


#### ☻ self-supervised learning [3]

supervised learning의 경우, labeled dataset을 사용하여 학습을 시킨다. 하지만 이러한 labeled data의 경우 비싸고, 또한 대용량 데이터를 구하기가 어렵다. 따라서 자체적으로 label을 만들어서 학습을 시키자고 제안된 방법이 self-supervised learning이다. 

좀 더 details하게 확인하면 [5,6]

> Self-supervised learning (SSL) is a method of machine learning. It learns from unlabeled sample data. It can be regarded as an intermediate form between supervised and unsupervised learning. It is based on an artificial neural network.[1] The neural network learns in two steps. First, the task is solved based on pseudo-labels which help to initialize the network weights.[2][3] Second, the actual task is performed with supervised or unsupervised learning.

이러한 self-supervised learning은 Computer vision, NLP, Speech에 사용된 경우를 확인해보면, 

CV의 경우는 크게 **Rotation**과 **Context prediction**이 있다 [3,7]. 

우선, rotation 시킨후 CNN에 넣어서 회전을 맞추는 task를 푸는 것이다. 이런 경우, 동일한 이미지가 들어오는데 각도가 바뀌는 것이다. 

<p align="center">
<img src="/images/data2vec_selfsupervised_CV_rotation.png" width="800">
</p>


Context prediction의 경우는 patch 생성하고, 가운데 이미지를 앵커처럼 사용하고 나머지 8개 이미지중 랜덤하게 이미지를 갖고와서 두 이미지를 CNN에 넣어서 앵커 제외하고 갖고온 query 이미지가 몇번째 이미지인지 맞춰주는 task이다. 
여기서 더 나아간 것이 두개의 이미지가 아닌 총 9개의 이미지를 모두 사용해서 CNN에 넣은 후 원래 이미지의 위치를 찾아내는 것으로 jigsaw puzzle라고 불리운다. 

<p align="center">
<img src="/images/data2vec_selfsupervised_CV_jigsaw_puzzle.png" width="800">
</p>


NLP의 경우는 BERT 그리고 Speech의 경우는 wave2vec 2.0이 대표적인 예이다 [1].

이를 통해 각각의 modality에서 서로 다른 방법론을 각기 적용해서 self-supervised learning을 진행해 왔다는 것을 확인 할 수 있었다. 


#### ☻ Multimodal pre-training 

Multimodality라는 것의 정의를 보면 [8]

> The term multimodality refers to the combination of multiple sensory and communicative modes, such as sight, sound, print, images, video, music, and so on, that produce meaning in any given message.

좀 더 자세히 살펴보면 [9]
(여기서 multimodality와 multimodal은 동일하다.) 

> 여러가지 형태와 의미로 컴퓨터와 대화하는 환경으로 모달 (modality)란 인터랙션 과정에서 사용되는 의사소통 채널을 가르킨다. 멀티모달은 텍스트, 음성, 이미지, 영상등 서로 다른 양식의 데이터를 자유자재로 이해하고 변환할 수 있어 사람처럼 배우고 생각하며 추론할 수 있다. 

이러한 멀티모달의 경우 cross-modal representation을 생성을 목적으로 사용될 수 있다 [1].

> There has been a considerable body of research on learning representations of multiple modalities simultaneously often using paired data (Aytar et al., 2017; Radford et al., 2021b; Wang et al., 2021; Singh et al., 2021) with the aim to produce cross-modal representations which can perform well on multi-modal tasks and with modalities benefiting from each other through joint training (Alayrac et al., 2020; Akbari et al., 2021) with recent methods exploring few-shot learning (Tsimpoukelli et al., 2021). Our work does not perform multimodal training but aims to unifiy the learning objective for self-supervised learning in different modalities. We hope that this will en- able better multimodal representations in the future.


본 연구에서는 Modality가 다르더라도 동일한 방식으로 self-supervised learning을 할 수 있는 방법론을 제안하였다. 이는 modality가 달라서 다른 input이 들어와도 contextualized representations를 예측할 수 있다. 

### ☺︎ Method 


<p align="center">
<img src="/images/data2vec_teacher_student.png" width="800">
</p>


#### ☻ Model architecture
인코더는 Transformer 구조를 이용했다. 각 모달리티별로 인코더의 입력을 구성하는 방식은 조금씩 다르다.

이미지 피쳐의 경우 16x16패치 영역의 픽셀을 하나의 토큰으로 인코딩하는 ViT의 전략을 이용했다.
텍스트의 경우 sub word단위로(토크나이즈) 전처리하고, 이를 임베딩 벡터로 인코딩했다.
음성의 경우, multi-layer 1-D Convolution 연산을 이용해 16kHz waveform을 50Hz의 representation으로 매핑했다.
무슨 의미인지 잘 모르지만, 음성 데이터를 고정된 입력 사이즈로 매핑하는 방법인 것 같다. (정확한 내용은 논문 참고)

#### ☻ Masking
token 단위의 일부를 masking하고 sequence를 transformer 네트워크에 공급한다.

각 modality에 따라서 다른 masking 방법을 따른다. 

CV의 경우, 
BEiT masking 방법을 따른다. image patches를 임의로 masking하는 방법을 사용하는데 인접한 블록을 masking하는 blockwise방법을 사용한다.

Speech의 경우, 
wave2vec masking 방법을 따른다. latent speech representation을 추출하기 위해 cnn 구조를 사용하고 n개의 token을 연속적으로 masking한다.

NLP의 경우, 
RoBERTa masking 방법을 따른다.
BERT에서는 한번만 random masking을 하고 모든 epoch에서 동일한 mask를 반복하지만 RoBERTa에서는 매 epoch마다 다른 masking을 수행하는 dynamic masking 방법을 사용한다. 또한 BERT와 달리 다음 문장을 예측하는 방법은 사용하지 않는다.


#### ☻ Trainning targets

**Teacher parameterization**


<img src="https://latex.codecogs.com/svg.image?\triangle&space;\leftarrow&space;\tau&space;\triangle&space;&plus;&space;(1-\tau)\theta" title="\triangle \leftarrow \tau \triangle + (1-\tau)\theta" />

### ☺︎ Experimental setup + Results 


#### ☻ Computer vision 

<p align="center">
<img src="/images/data2vec_computer_vision.png" width="600">
</p>



#### ☻ Speech processing 

<p align="center">
<img src="/images/data2vec_speech_processing.png" width="800">
</p>




#### ☻ Natural anguage processing (NLP)

<p align="center">
<img src="/images/data2vec_NLP.png" width="800">
</p>


#### ☻ Ablations


<p align="center">
<img src="/images/data2vec_predicting_target.png" width="800">
</p>


<p align="center">
<img src="/images/data2vec_contextualized_target.png" width="800">
</p>


<p align="center">
<img src="/images/data2vec_effect_of_different_features.png" width="800">
</p>


### ☺︎ Discussion

#### ☻ Modality specific features extractors and masking
NLP에서 data2vec은 target units(word/sub-word/character/byte)를 미리 정의하지 않아도 되는 첫 모델이다.
#### ☻ Structured and contextualized targets 
Representation collapse: input에 상관 없이 모두 같은(비슷한) constant vector를 뱉는 현상
논문에서 collapse가 발생하는 시나리오를 찾았다.
the learning rate is too large or the learning rate warmup is too short which can often be solved by tuning the respective hyperparameters.
τ is too low which leads to student model collapse and is then propagated to the teacher.
we found collapse to be more likely for modalities where adjacent targets are very correlated and where longer spans need to be masked, e.g., speech.



하지만 학습 알고리즘은 통합적이지만 여전히 representation은 각 modality의 양식에 대해 개별적으로 학습한다는 
하지만 여러 modality에서 general한 방법을 사용한다 했지만, data2vec의 경우도 input encoder와 masking이 modality specific하다는 한계점이 존재한다. 이는 추후 이 과정들도 모두 modality independent 하게 바꾸는 방향으로 나아가야 한다고 생각한다. 


-----------------------------------------------------------------------

### ☻ Reference
1. [Paper : data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555)
2. [Blog : Meta AI](https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/)
3. [DSBM: data2vec](http://dsba.korea.ac.kr/seminar/?mod=document&uid=1935)
4. [Blog : Data2Vec Review](https://baekyeongmin.github.io/paper-review/data2vec/)
5. [wikipedia: self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning)
6. [Self-supervised learning: The dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)
7. [Blog: Self-Supervised Representation Learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
8. [Multimodality and Language Learning](https://onlinelibrary.wiley.com/doi/10.1002/9781119472384.ch3#:~:text=The%20term%20multimodality%20refers%20to,meaning%20in%20any%20given%20message)
9. [Blog : 멀티모달](https://blog.naver.com/PostView.naver?blogId=yckim002&logNo=222599463531&parentCategoryNo=&categoryNo=63&viewDate=&isShowPopularPosts=true&from=search)
10. [blog : 초간단 논문리뷰, data2vec](https://everyday-deeplearning.tistory.com/m/entry/data2vec-A-General-Framework-for-Self-supervised-Learning-in-Speech-Vision-and-Language)



### ☻ image sources
1. [Giphy](https://giphy.com/search/sesame-street)


🌺 **Thanks for reading. Hope to see you again :o)**

-----------------------------------


<p align="center">
<img src="/images/B-icon-ver.png" width="150">
</p>

모두의 연구소에서 진행하는 "함께 콘텐츠를 제작하는 콘텐츠 크리에이터 모임"인 **COCRE(코크리)** 의 2기 회원으로 제작한 글입니다

[🐘 코크리가 궁금하다면 클릭!](https://medium.com/modulabs/cocre-%EC%BD%94%ED%81%AC%EB%A6%AC-%EB%A5%BC-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4-c3a4e9519e85)




